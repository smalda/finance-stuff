{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 12 Seminar — Derivatives Pricing with Neural Networks (Hands-On)\n",
    "\n",
    "**Exercises:**\n",
    "1. Vectorized Black-Scholes in PyTorch, autograd Greeks (25 min)\n",
    "2. Train NN to learn BS pricing from synthetic data (25 min)\n",
    "3. Compare neural Greeks to analytical Greeks (20 min)\n",
    "4. Discussion: Where does BS fail? Can NNs do better? (20 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.stats import norm\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 1: Vectorized Black-Scholes in PyTorch + Autograd Greeks (25 min)\n",
    "\n",
    "Implement BS in PyTorch so we can differentiate through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: BS in NumPy\n",
    "def bs_call_numpy(S, K, T, sigma, r):\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "    return S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n",
    "\n",
    "def bs_delta_numpy(S, K, T, sigma, r):\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n",
    "    return norm.cdf(d1)\n",
    "\n",
    "def bs_gamma_numpy(S, K, T, sigma, r):\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n",
    "    return norm.pdf(d1) / (S * sigma * np.sqrt(T))\n",
    "\n",
    "def bs_vega_numpy(S, K, T, sigma, r):\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n",
    "    return S * norm.pdf(d1) * np.sqrt(T)\n",
    "\n",
    "def bs_theta_numpy(S, K, T, sigma, r):\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "    theta = (-S * norm.pdf(d1) * sigma / (2 * np.sqrt(T))\n",
    "             - r * K * np.exp(-r * T) * norm.cdf(d2))\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch BS implementation\n",
    "def normal_cdf(x):\n",
    "    return 0.5 * (1 + torch.erf(x / np.sqrt(2)))\n",
    "\n",
    "def bs_call_torch(S, K, T, sigma, r):\n",
    "    \"\"\"Fully differentiable Black-Scholes call price.\"\"\"\n",
    "    d1 = (torch.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * torch.sqrt(T))\n",
    "    d2 = d1 - sigma * torch.sqrt(T)\n",
    "    return S * normal_cdf(d1) - K * torch.exp(-r * T) * normal_cdf(d2)\n",
    "\n",
    "# Verify: PyTorch vs NumPy\n",
    "S_np, K_np, T_np, sigma_np, r_np = 100.0, 100.0, 1.0, 0.2, 0.05\n",
    "price_np = bs_call_numpy(S_np, K_np, T_np, sigma_np, r_np)\n",
    "price_pt = bs_call_torch(\n",
    "    torch.tensor(S_np), torch.tensor(K_np),\n",
    "    torch.tensor(T_np), torch.tensor(sigma_np), torch.tensor(r_np)\n",
    ").item()\n",
    "\n",
    "print(f\"NumPy:   {price_np:.8f}\")\n",
    "print(f\"PyTorch: {price_pt:.8f}\")\n",
    "print(f\"Match:   {abs(price_np - price_pt) < 1e-6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized autograd Greeks\n",
    "def compute_all_greeks(S_vals, K_val, T_val, sigma_val, r_val):\n",
    "    \"\"\"Compute all Greeks for a vector of spot prices using autograd.\"\"\"\n",
    "    S = torch.tensor(S_vals, dtype=torch.float64, requires_grad=True)\n",
    "    K = torch.tensor(K_val, dtype=torch.float64)\n",
    "    T = torch.tensor(T_val, dtype=torch.float64, requires_grad=True)\n",
    "    sigma = torch.tensor(sigma_val, dtype=torch.float64, requires_grad=True)\n",
    "    r = torch.tensor(r_val, dtype=torch.float64)\n",
    "    \n",
    "    price = bs_call_torch(S, K, T, sigma, r)\n",
    "    total = price.sum()\n",
    "    \n",
    "    # First derivatives\n",
    "    grads = torch.autograd.grad(total, [S, T, sigma], create_graph=True)\n",
    "    delta = grads[0]  # dC/dS\n",
    "    theta = grads[1]  # dC/dT (note: convention is -dC/dT for theta)\n",
    "    vega = grads[2]   # dC/dsigma\n",
    "    \n",
    "    # Gamma = d2C/dS2\n",
    "    gamma = torch.autograd.grad(delta.sum(), S)[0]\n",
    "    \n",
    "    return {\n",
    "        'price': price.detach().numpy(),\n",
    "        'delta': delta.detach().numpy(),\n",
    "        'gamma': gamma.detach().numpy(),\n",
    "        'theta': theta.detach().numpy(),\n",
    "        'vega': vega.detach().numpy(),\n",
    "    }\n",
    "\n",
    "# Compute for a range of spot prices\n",
    "S_range = np.linspace(70, 130, 200)\n",
    "greeks = compute_all_greeks(S_range, 100.0, 1.0, 0.2, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all Greeks\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 9))\n",
    "\n",
    "greeks_list = [\n",
    "    ('Price', greeks['price'], bs_call_numpy(S_range, 100, 1, 0.2, 0.05)),\n",
    "    ('Delta', greeks['delta'], bs_delta_numpy(S_range, 100, 1, 0.2, 0.05)),\n",
    "    ('Gamma', greeks['gamma'], bs_gamma_numpy(S_range, 100, 1, 0.2, 0.05)),\n",
    "    ('Vega', greeks['vega'], bs_vega_numpy(S_range, 100, 1, 0.2, 0.05)),\n",
    "    ('Theta', greeks['theta'], bs_theta_numpy(S_range, 100, 1, 0.2, 0.05)),\n",
    "]\n",
    "\n",
    "for idx, (name, autograd_vals, analytical_vals) in enumerate(greeks_list):\n",
    "    ax = axes.flat[idx]\n",
    "    ax.plot(S_range, analytical_vals, label='Analytical', linewidth=2)\n",
    "    ax.plot(S_range, autograd_vals, '--', label='Autograd', linewidth=2)\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel('Spot Price S')\n",
    "    ax.legend()\n",
    "\n",
    "# Error plot in the last panel\n",
    "ax = axes.flat[5]\n",
    "for name, autograd_vals, analytical_vals in greeks_list[1:]:\n",
    "    error = np.abs(autograd_vals - analytical_vals)\n",
    "    ax.plot(S_range, error, label=name)\n",
    "ax.set_title('Absolute Errors')\n",
    "ax.set_xlabel('Spot Price S')\n",
    "ax.set_ylabel('|Autograd - Analytical|')\n",
    "ax.legend()\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 2: Train NN to Learn BS Pricing from Synthetic Data (25 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "N = 200_000\n",
    "\n",
    "S_data = np.random.uniform(50, 150, N)\n",
    "K_data = np.random.uniform(50, 150, N)\n",
    "T_data = np.random.uniform(0.05, 3.0, N)\n",
    "sigma_data = np.random.uniform(0.05, 0.6, N)\n",
    "r_data = np.random.uniform(0.0, 0.1, N)\n",
    "\n",
    "C_data = bs_call_numpy(S_data, K_data, T_data, sigma_data, r_data)\n",
    "\n",
    "# Features: log-moneyness, T, sigma, r\n",
    "log_m = np.log(S_data / K_data)\n",
    "X = np.column_stack([log_m, T_data, sigma_data, r_data]).astype(np.float32)\n",
    "y = (C_data / K_data).astype(np.float32)  # normalize by strike\n",
    "\n",
    "# Split\n",
    "n_train = int(0.85 * N)\n",
    "n_val = int(0.05 * N)\n",
    "X_tr = torch.tensor(X[:n_train]).to(device)\n",
    "y_tr = torch.tensor(y[:n_train]).to(device)\n",
    "X_val = torch.tensor(X[n_train:n_train+n_val]).to(device)\n",
    "y_val = torch.tensor(y[n_train:n_train+n_val]).to(device)\n",
    "X_test = torch.tensor(X[n_train+n_val:]).to(device)\n",
    "y_test = torch.tensor(y[n_train+n_val:]).to(device)\n",
    "\n",
    "print(f\"Train: {n_train:,} | Val: {n_val:,} | Test: {N - n_train - n_val:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptionPricerNN(nn.Module):\n",
    "    def __init__(self, hidden=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(4, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Softplus(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "model = OptionPricerNN(hidden=128).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "batch_size = 4096\n",
    "n_epochs = 60\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    perm = torch.randperm(n_train, device=device)\n",
    "    epoch_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for i in range(0, n_train, batch_size):\n",
    "        idx = perm[i:i+batch_size]\n",
    "        pred = model(X_tr[idx])\n",
    "        loss = nn.MSELoss()(pred, y_tr[idx])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    train_losses.append(epoch_loss / n_batches)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = nn.MSELoss()(model(X_val), y_val).item()\n",
    "    val_losses.append(val_loss)\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d} | Train: {train_losses[-1]:.2e} | Val: {val_loss:.2e}\")\n",
    "\n",
    "plt.plot(train_losses, label='Train')\n",
    "plt.plot(val_losses, label='Validation')\n",
    "plt.title('NN Pricer Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_pred = model(X_test).cpu().numpy()\n",
    "    test_true = y_test.cpu().numpy()\n",
    "\n",
    "errors = test_pred - test_true\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "axes[0].scatter(test_true[:3000], test_pred[:3000], s=1, alpha=0.3)\n",
    "axes[0].plot([0, test_true.max()], [0, test_true.max()], 'r--')\n",
    "axes[0].set_title('NN vs True (normalized price)')\n",
    "axes[0].set_xlabel('True C/K')\n",
    "axes[0].set_ylabel('Predicted C/K')\n",
    "\n",
    "axes[1].hist(errors, bins=100, edgecolor='black', linewidth=0.3)\n",
    "axes[1].set_title(f'Error Distribution (std={errors.std():.2e})')\n",
    "axes[1].set_xlabel('Prediction Error')\n",
    "\n",
    "axes[2].scatter(test_true[:3000], np.abs(errors[:3000]), s=1, alpha=0.3)\n",
    "axes[2].set_title('Absolute Error vs True Price')\n",
    "axes[2].set_xlabel('True C/K')\n",
    "axes[2].set_ylabel('|Error|')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Test MAE: {np.abs(errors).mean():.6f}\")\n",
    "print(f\"Test RMSE: {np.sqrt((errors**2).mean()):.6f}\")\n",
    "print(f\"Test R2: {1 - (errors**2).sum() / ((test_true - test_true.mean())**2).sum():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 3: Compare Neural Greeks to Analytical Greeks (20 min)\n",
    "\n",
    "Use autograd on the trained NN pricer and compare to the BS analytical formulas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_greeks(model, S_vals, K, T, sigma, r):\n",
    "    \"\"\"Compute Delta, Gamma, Vega from the NN pricer.\"\"\"\n",
    "    S_t = torch.tensor(S_vals, dtype=torch.float32, requires_grad=True).to(device)\n",
    "    sigma_t = torch.tensor(np.full_like(S_vals, sigma), dtype=torch.float32,\n",
    "                           requires_grad=True).to(device)\n",
    "    \n",
    "    log_m = torch.log(S_t / K)\n",
    "    T_t = torch.full_like(S_t, T)\n",
    "    r_t = torch.full_like(S_t, r)\n",
    "    \n",
    "    x = torch.stack([log_m, T_t, sigma_t, r_t], dim=1)\n",
    "    price_normalized = model(x)\n",
    "    price = price_normalized * K\n",
    "    \n",
    "    # Delta\n",
    "    delta = torch.autograd.grad(price.sum(), S_t, create_graph=True)[0]\n",
    "    # Gamma\n",
    "    gamma = torch.autograd.grad(delta.sum(), S_t)[0]\n",
    "    # Vega\n",
    "    vega = torch.autograd.grad(price.sum(), sigma_t, retain_graph=False)[0]\n",
    "    \n",
    "    return {\n",
    "        'price': price.detach().cpu().numpy(),\n",
    "        'delta': delta.detach().cpu().numpy(),\n",
    "        'gamma': gamma.detach().cpu().numpy(),\n",
    "        'vega': vega.detach().cpu().numpy(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and compare\n",
    "S_plot = np.linspace(70, 130, 300).astype(np.float32)\n",
    "K_val, T_val, sig_val, r_val = 100.0, 1.0, 0.2, 0.05\n",
    "\n",
    "nn_g = nn_greeks(model, S_plot, K_val, T_val, sig_val, r_val)\n",
    "\n",
    "analytical = {\n",
    "    'price': bs_call_numpy(S_plot, K_val, T_val, sig_val, r_val),\n",
    "    'delta': bs_delta_numpy(S_plot, K_val, T_val, sig_val, r_val),\n",
    "    'gamma': bs_gamma_numpy(S_plot, K_val, T_val, sig_val, r_val),\n",
    "    'vega': bs_vega_numpy(S_plot, K_val, T_val, sig_val, r_val),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 9))\n",
    "\n",
    "for idx, greek in enumerate(['price', 'delta', 'gamma', 'vega']):\n",
    "    ax = axes.flat[idx]\n",
    "    ax.plot(S_plot, analytical[greek], label='Analytical BS', linewidth=2)\n",
    "    ax.plot(S_plot, nn_g[greek], '--', label='Neural Network', linewidth=2)\n",
    "    ax.set_title(greek.capitalize())\n",
    "    ax.set_xlabel('Spot Price S')\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle('Neural Greeks vs Analytical Greeks', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis for Greeks\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "for idx, greek in enumerate(['delta', 'gamma', 'vega']):\n",
    "    error = nn_g[greek] - analytical[greek]\n",
    "    axes[idx].plot(S_plot, error)\n",
    "    axes[idx].axhline(0, color='gray', linestyle='--', linewidth=0.5)\n",
    "    axes[idx].set_title(f'{greek.capitalize()} Error (NN - BS)')\n",
    "    axes[idx].set_xlabel('Spot Price S')\n",
    "    mae = np.abs(error).mean()\n",
    "    axes[idx].text(0.05, 0.95, f'MAE: {mae:.4e}', transform=axes[idx].transAxes,\n",
    "                   verticalalignment='top', fontsize=10,\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "- **Delta**: NN approximation is very close. Errors are largest near ATM where delta changes fastest.\n",
    "- **Gamma**: More noise — second derivatives amplify approximation errors. This is inherent to finite-width NNs.\n",
    "- **Vega**: Good match overall. Errors are small relative to the magnitude of vega.\n",
    "\n",
    "**Key insight**: Neural Greeks are smooth but not exact. For risk management, the smoothness is actually a feature — analytical Greeks can be discontinuous for exotic options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 4: Discussion — Where Does BS Fail? Can NNs Do Better? (20 min)\n",
    "\n",
    "### Where Black-Scholes fails\n",
    "\n",
    "**1. Volatility is not constant**\n",
    "- Implied volatility varies by strike (smile) and maturity (term structure)\n",
    "- BS gives ONE price for a given vol — but market quotes different IVs for different strikes\n",
    "\n",
    "**2. Returns are not normal**\n",
    "- Real returns have fat tails (kurtosis > 3) and negative skew\n",
    "- BS underprices OTM puts (crash protection)\n",
    "\n",
    "**3. No jumps**\n",
    "- BS assumes continuous paths\n",
    "- Real stocks can gap (earnings, macro events)\n",
    "\n",
    "**4. Exotic options**\n",
    "- BS only prices vanillas. For barriers, Asians, lookbacks — no closed form\n",
    "- Need Monte Carlo or PDE methods (slow)\n",
    "\n",
    "### Where NNs can do better\n",
    "\n",
    "| Application | Why NNs Help |\n",
    "|-------------|---------------|\n",
    "| **Stochastic vol models** (Heston, SABR) | No closed form — NN replaces Monte Carlo |\n",
    "| **Exotic options** | NN trained on MC samples, then used for fast pricing |\n",
    "| **Model-free pricing** | Train on market data directly, no model assumptions |\n",
    "| **Real-time Greeks** | Autograd on NN is faster than finite differences |\n",
    "| **Calibration** | NN as fast approximation for inner loop of calibration |\n",
    "\n",
    "### Where NNs struggle\n",
    "\n",
    "| Challenge | Issue |\n",
    "|-----------|-------|\n",
    "| **Extrapolation** | NN accuracy degrades outside training range |\n",
    "| **No-arbitrage** | Must be explicitly enforced |\n",
    "| **Interpretability** | Hard to explain prices to traders |\n",
    "| **Data scarcity** | Illiquid options have few market prices |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the extrapolation problem\n",
    "# Our training data had S in [50, 150]. What happens outside?\n",
    "\n",
    "S_extrap = np.linspace(20, 200, 400).astype(np.float32)\n",
    "bs_prices_extrap = bs_call_numpy(S_extrap, 100, 1.0, 0.2, 0.05)\n",
    "\n",
    "log_m_ext = np.log(S_extrap / 100)\n",
    "X_ext = np.column_stack([\n",
    "    log_m_ext,\n",
    "    np.full_like(S_extrap, 1.0),\n",
    "    np.full_like(S_extrap, 0.2),\n",
    "    np.full_like(S_extrap, 0.05),\n",
    "]).astype(np.float32)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    nn_prices_extrap = model(torch.tensor(X_ext).to(device)).cpu().numpy() * 100\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(S_extrap, bs_prices_extrap, label='Black-Scholes')\n",
    "axes[0].plot(S_extrap, nn_prices_extrap, '--', label='NN')\n",
    "axes[0].axvspan(50, 150, alpha=0.1, color='green', label='Training range')\n",
    "axes[0].set_title('NN Extrapolation Test')\n",
    "axes[0].set_xlabel('Spot Price S')\n",
    "axes[0].set_ylabel('Call Price')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(S_extrap, np.abs(bs_prices_extrap - nn_prices_extrap))\n",
    "axes[1].axvspan(50, 150, alpha=0.1, color='green', label='Training range')\n",
    "axes[1].set_title('Absolute Error')\n",
    "axes[1].set_xlabel('Spot Price S')\n",
    "axes[1].set_ylabel('|BS - NN|')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "in_range = (S_extrap >= 50) & (S_extrap <= 150)\n",
    "print(f\"MAE in training range:  ${np.abs(bs_prices_extrap[in_range] - nn_prices_extrap[in_range]).mean():.4f}\")\n",
    "print(f\"MAE outside range:      ${np.abs(bs_prices_extrap[~in_range] - nn_prices_extrap[~in_range]).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion questions for students\n",
    "\n",
    "1. How would you ensure the NN pricer satisfies no-arbitrage constraints?\n",
    "2. If you train on BS data, you get a BS-equivalent pricer. What's the point? (Answer: speed + autograd Greeks. The real value is training on complex models like Heston.)\n",
    "3. How would you handle the extrapolation problem in production?\n",
    "4. Could you use the NN pricer for calibration? (Yes — it's the inner loop, so speed matters enormously.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "1. BS in PyTorch is straightforward and gives us autograd Greeks that match analytical formulas\n",
    "2. A simple 4-layer NN can learn BS pricing with very high accuracy (R2 > 0.9999)\n",
    "3. Neural Greeks are smooth and accurate for Delta and Vega; Gamma (2nd derivative) is noisier\n",
    "4. The extrapolation problem is real — always validate outside the training range\n",
    "5. The real value of NN pricers is for models without closed-form solutions (Heston, SABR, exotics)"
   ]
  }
 ]
}