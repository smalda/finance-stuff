{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 12 — Derivatives Pricing with Neural Networks\n",
    "\n",
    "**Key ideas:**\n",
    "- Neural networks can learn the pricing function from data — replacing or accelerating Monte Carlo\n",
    "- PyTorch autograd gives us Greeks (sensitivities) for free\n",
    "- Deep Hedging learns end-to-end hedging strategies\n",
    "- NNs can learn pricing for models where no closed-form solution exists\n",
    "\n",
    "**Outline:**\n",
    "1. Options crash course: calls/puts, Black-Scholes, Greeks\n",
    "2. Why neural networks for pricing\n",
    "3. NN pricing architecture and no-arbitrage constraints\n",
    "4. Deep Hedging (Buehler et al. 2019)\n",
    "5. Implied volatility surface learning\n",
    "6. Demo: Black-Scholes in PyTorch, autograd Greeks, train NN pricer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.stats import norm\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Options Crash Course\n",
    "\n",
    "### What is an option?\n",
    "- **Call option**: right (not obligation) to BUY the asset at strike $K$ at expiry $T$\n",
    "- **Put option**: right to SELL at strike $K$ at expiry $T$\n",
    "\n",
    "Payoff at expiry:\n",
    "$$\\text{Call payoff} = \\max(S_T - K, 0), \\qquad \\text{Put payoff} = \\max(K - S_T, 0)$$\n",
    "\n",
    "### Black-Scholes formula\n",
    "\n",
    "Under the Black-Scholes model (geometric Brownian motion, constant volatility):\n",
    "\n",
    "$$C(S, K, T, \\sigma, r) = S \\cdot N(d_1) - K e^{-rT} \\cdot N(d_2)$$\n",
    "\n",
    "where:\n",
    "$$d_1 = \\frac{\\ln(S/K) + (r + \\sigma^2/2)T}{\\sigma\\sqrt{T}}, \\qquad d_2 = d_1 - \\sigma\\sqrt{T}$$\n",
    "\n",
    "### The Greeks\n",
    "\n",
    "| Greek | Symbol | Definition | Interpretation |\n",
    "|-------|--------|------------|----------------|\n",
    "| Delta | $\\Delta$ | $\\frac{\\partial C}{\\partial S}$ | Price sensitivity to spot |\n",
    "| Gamma | $\\Gamma$ | $\\frac{\\partial^2 C}{\\partial S^2}$ | Delta's sensitivity to spot |\n",
    "| Theta | $\\Theta$ | $\\frac{\\partial C}{\\partial T}$ | Time decay |\n",
    "| Vega | $\\mathcal{V}$ | $\\frac{\\partial C}{\\partial \\sigma}$ | Sensitivity to volatility |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Black-Scholes in NumPy (reference implementation)\n",
    "def bs_call_numpy(S, K, T, sigma, r):\n",
    "    \"\"\"Black-Scholes European call price.\"\"\"\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "    return S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n",
    "\n",
    "def bs_delta_numpy(S, K, T, sigma, r):\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n",
    "    return norm.cdf(d1)\n",
    "\n",
    "def bs_gamma_numpy(S, K, T, sigma, r):\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n",
    "    return norm.pdf(d1) / (S * sigma * np.sqrt(T))\n",
    "\n",
    "def bs_vega_numpy(S, K, T, sigma, r):\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n",
    "    return S * norm.pdf(d1) * np.sqrt(T)\n",
    "\n",
    "# Example\n",
    "S, K, T, sigma, r = 100, 100, 1.0, 0.2, 0.05\n",
    "price = bs_call_numpy(S, K, T, sigma, r)\n",
    "print(f\"BS Call Price: ${price:.4f}\")\n",
    "print(f\"Delta: {bs_delta_numpy(S, K, T, sigma, r):.4f}\")\n",
    "print(f\"Gamma: {bs_gamma_numpy(S, K, T, sigma, r):.6f}\")\n",
    "print(f\"Vega:  {bs_vega_numpy(S, K, T, sigma, r):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize option price surface\n",
    "S_range = np.linspace(60, 140, 100)\n",
    "T_range = np.array([0.1, 0.25, 0.5, 1.0, 2.0])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Price vs spot for different maturities\n",
    "for t in T_range:\n",
    "    prices = bs_call_numpy(S_range, 100, t, 0.2, 0.05)\n",
    "    axes[0].plot(S_range, prices, label=f'T={t:.2f}')\n",
    "axes[0].plot(S_range, np.maximum(S_range - 100, 0), 'k--', alpha=0.3, label='Payoff')\n",
    "axes[0].set_title('Call Price vs Spot')\n",
    "axes[0].set_xlabel('Spot Price S')\n",
    "axes[0].set_ylabel('Call Price')\n",
    "axes[0].legend()\n",
    "\n",
    "# Delta vs spot\n",
    "for t in T_range:\n",
    "    deltas = bs_delta_numpy(S_range, 100, t, 0.2, 0.05)\n",
    "    axes[1].plot(S_range, deltas, label=f'T={t:.2f}')\n",
    "axes[1].set_title('Delta vs Spot')\n",
    "axes[1].set_xlabel('Spot Price S')\n",
    "axes[1].set_ylabel('Delta')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Why Neural Networks for Pricing?\n",
    "\n",
    "### Problem: Black-Scholes is wrong\n",
    "BS assumes constant volatility — in reality:\n",
    "- Volatility changes over time (stochastic vol)\n",
    "- Implied vol depends on strike and maturity (vol smile/skew)\n",
    "- Exotic options have no closed-form solution\n",
    "\n",
    "### Traditional alternatives\n",
    "- **Monte Carlo**: accurate but slow (10-60 seconds per price)\n",
    "- **Finite difference PDE**: fast for 1-2 factors, impossible for 5+\n",
    "- **Analytical approximations**: fast but inaccurate\n",
    "\n",
    "### Neural network approach\n",
    "\n",
    "Train a neural network to approximate the pricing function:\n",
    "\n",
    "$$f_\\theta(S, K, T, \\sigma, r) \\approx C(S, K, T, \\sigma, r)$$\n",
    "\n",
    "**Advantages:**\n",
    "1. **Speed**: Forward pass takes microseconds (1000x faster than Monte Carlo)\n",
    "2. **Autograd Greeks**: PyTorch gives us $\\partial C / \\partial S$, $\\partial^2 C / \\partial S^2$, etc. for free\n",
    "3. **Flexibility**: Same architecture works for any model (BS, Heston, SABR, ...)\n",
    "4. **Data-driven**: Can learn from market prices directly, no model assumption needed\n",
    "\n",
    "**Workflow:**\n",
    "1. Generate training data: $(S, K, T, \\sigma, r) \\to C$ using Monte Carlo or analytical formula\n",
    "2. Train neural network on this data\n",
    "3. Use the trained network for fast pricing and Greek computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. NN Pricing Architecture\n",
    "\n",
    "### Basic architecture\n",
    "```\n",
    "Input (S, K, T, sigma, r)  -->  [FC 128] --> ReLU --> [FC 128] --> ReLU --> [FC 64] --> ReLU --> [FC 1] --> Softplus --> Price\n",
    "```\n",
    "\n",
    "### Key design choices\n",
    "\n",
    "**Input normalization:** Use log-moneyness $\\ln(S/K)$ instead of raw $S$ and $K$ — reduces the input space and helps generalization.\n",
    "\n",
    "**Output activation:** Use `softplus` ($\\log(1 + e^x)$) to ensure positive prices.\n",
    "\n",
    "### No-arbitrage constraints\n",
    "\n",
    "A valid call price must satisfy:\n",
    "- $C \\geq 0$ (non-negativity) — enforced by softplus\n",
    "- $C \\leq S$ (bounded above by spot)\n",
    "- $0 \\leq \\Delta \\leq 1$ (monotonicity)\n",
    "- $\\Gamma \\geq 0$ (convexity)\n",
    "\n",
    "These can be enforced by:\n",
    "1. **Soft constraints**: Add penalty terms to loss\n",
    "2. **Hard constraints**: Use architectures that are inherently monotone/convex (ICNN)\n",
    "3. **Training data**: Ensure training data satisfies these properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Deep Hedging (Buehler et al. 2019)\n",
    "\n",
    "Instead of pricing then hedging, **learn the hedging strategy end-to-end**.\n",
    "\n",
    "### Setup\n",
    "- Sell an option at time 0, receive premium $C_0$\n",
    "- At each time step $t$, choose hedge ratio $\\delta_t = f_\\theta(S_t, t, \\ldots)$\n",
    "- Hold $\\delta_t$ shares of the underlying\n",
    "- At expiry: P&L = premium + hedging gains - option payoff\n",
    "\n",
    "### Loss function\n",
    "$$\\mathcal{L}(\\theta) = \\rho\\left( -C_0 - \\sum_{t=0}^{T-1} \\delta_t (S_{t+1} - S_t) + \\text{payoff}(S_T) \\right)$$\n",
    "\n",
    "where $\\rho$ is a risk measure (e.g., CVaR, variance).\n",
    "\n",
    "### Why this is powerful\n",
    "- Works with ANY underlying dynamics (no model needed)\n",
    "- Handles transaction costs naturally\n",
    "- Handles discrete hedging (realistic!)\n",
    "- The \"price\" is the minimum premium $C_0$ that makes the hedge P&L acceptable\n",
    "\n",
    "This approach is gaining traction at sell-side banks for exotic options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Implied Volatility Surface Learning\n",
    "\n",
    "### What is the implied vol surface?\n",
    "Given market prices of options at different $(K, T)$, invert BS to find $\\sigma_{\\text{imp}}(K, T)$.\n",
    "\n",
    "The IV surface captures:\n",
    "- **Smile/skew** in strike dimension (OTM puts are expensive)\n",
    "- **Term structure** in maturity dimension\n",
    "\n",
    "### NN approach\n",
    "Train $f_\\theta(\\text{moneyness}, T) \\to \\sigma_{\\text{imp}}$ to:\n",
    "- Interpolate between observed strikes/maturities\n",
    "- Extrapolate to unobserved regions\n",
    "- Ensure smoothness and no-arbitrage (no butterfly arbitrage)\n",
    "\n",
    "This replaces traditional parametric models (SVI, SSVI) with a more flexible learned surface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Demo: Black-Scholes in PyTorch + Autograd Greeks + NN Pricer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Black-Scholes in PyTorch (differentiable)\n",
    "def normal_cdf(x):\n",
    "    \"\"\"Standard normal CDF using PyTorch.\"\"\"\n",
    "    return 0.5 * (1 + torch.erf(x / np.sqrt(2)))\n",
    "\n",
    "def bs_call_torch(S, K, T, sigma, r):\n",
    "    \"\"\"Black-Scholes call price in PyTorch (fully differentiable).\"\"\"\n",
    "    d1 = (torch.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * torch.sqrt(T))\n",
    "    d2 = d1 - sigma * torch.sqrt(T)\n",
    "    return S * normal_cdf(d1) - K * torch.exp(-r * T) * normal_cdf(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Greeks using PyTorch autograd\n",
    "S = torch.tensor(100.0, requires_grad=True)\n",
    "K = torch.tensor(100.0)\n",
    "T = torch.tensor(1.0, requires_grad=True)\n",
    "sigma = torch.tensor(0.2, requires_grad=True)\n",
    "r = torch.tensor(0.05)\n",
    "\n",
    "# Forward pass\n",
    "price = bs_call_torch(S, K, T, sigma, r)\n",
    "\n",
    "# Compute first-order Greeks\n",
    "price.backward(retain_graph=True)\n",
    "\n",
    "delta_autograd = S.grad.item()\n",
    "theta_autograd = T.grad.item()  # Note: this is dC/dT, not -dC/dT\n",
    "vega_autograd = sigma.grad.item()\n",
    "\n",
    "# Compute Gamma (second derivative)\n",
    "S2 = torch.tensor(100.0, requires_grad=True)\n",
    "price2 = bs_call_torch(S2, K, T.detach(), sigma.detach(), r)\n",
    "delta_fn = torch.autograd.grad(price2, S2, create_graph=True)[0]\n",
    "gamma_autograd = torch.autograd.grad(delta_fn, S2)[0].item()\n",
    "\n",
    "print(f\"{'Greek':<10} {'Autograd':>12} {'Analytical':>12} {'Error':>12}\")\n",
    "print('-' * 48)\n",
    "print(f\"{'Price':<10} {price.item():>12.4f} {bs_call_numpy(100, 100, 1, 0.2, 0.05):>12.4f}\")\n",
    "print(f\"{'Delta':<10} {delta_autograd:>12.4f} {bs_delta_numpy(100, 100, 1, 0.2, 0.05):>12.4f} {abs(delta_autograd - bs_delta_numpy(100, 100, 1, 0.2, 0.05)):>12.6f}\")\n",
    "print(f\"{'Gamma':<10} {gamma_autograd:>12.6f} {bs_gamma_numpy(100, 100, 1, 0.2, 0.05):>12.6f} {abs(gamma_autograd - bs_gamma_numpy(100, 100, 1, 0.2, 0.05)):>12.8f}\")\n",
    "print(f\"{'Vega':<10} {vega_autograd:>12.4f} {bs_vega_numpy(100, 100, 1, 0.2, 0.05):>12.4f} {abs(vega_autograd - bs_vega_numpy(100, 100, 1, 0.2, 0.05)):>12.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd matches analytical Greeks to machine precision. This is the core insight: any differentiable pricing function gives us Greeks for free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data for NN pricer\n",
    "N = 100_000\n",
    "\n",
    "# Sample inputs uniformly from realistic ranges\n",
    "S_train = np.random.uniform(50, 150, N)\n",
    "K_train = np.random.uniform(50, 150, N)\n",
    "T_train = np.random.uniform(0.05, 3.0, N)\n",
    "sigma_train = np.random.uniform(0.05, 0.6, N)\n",
    "r_train = np.random.uniform(0.0, 0.1, N)\n",
    "\n",
    "# Compute BS prices (labels)\n",
    "C_train = bs_call_numpy(S_train, K_train, T_train, sigma_train, r_train)\n",
    "\n",
    "# Use log-moneyness as feature\n",
    "log_m = np.log(S_train / K_train)\n",
    "\n",
    "# Stack features: [log(S/K), T, sigma, r]\n",
    "X = np.column_stack([log_m, T_train, sigma_train, r_train]).astype(np.float32)\n",
    "# Normalize price by strike\n",
    "y = (C_train / K_train).astype(np.float32)\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"y range: [{y.min():.4f}, {y.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define NN pricer\n",
    "class OptionPricer(nn.Module):\n",
    "    def __init__(self, input_dim=4, hidden=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Softplus(),  # ensures positive output\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "model = OptionPricer().to(device)\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the NN pricer\n",
    "X_tensor = torch.tensor(X).to(device)\n",
    "y_tensor = torch.tensor(y).to(device)\n",
    "\n",
    "# Train/val split\n",
    "n_train = int(0.9 * N)\n",
    "X_tr, X_val = X_tensor[:n_train], X_tensor[n_train:]\n",
    "y_tr, y_val = y_tensor[:n_train], y_tensor[n_train:]\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "\n",
    "batch_size = 4096\n",
    "n_epochs = 50\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    perm = torch.randperm(n_train)\n",
    "    epoch_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for i in range(0, n_train, batch_size):\n",
    "        idx = perm[i:i+batch_size]\n",
    "        pred = model(X_tr[idx])\n",
    "        loss = nn.MSELoss()(pred, y_tr[idx])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    train_losses.append(epoch_loss / n_batches)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = model(X_val)\n",
    "        val_loss = nn.MSELoss()(val_pred, y_val).item()\n",
    "    val_losses.append(val_loss)\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d} | Train MSE: {train_losses[-1]:.6f} | Val MSE: {val_loss:.6f}\")\n",
    "\n",
    "plt.plot(train_losses, label='Train')\n",
    "plt.plot(val_losses, label='Validation')\n",
    "plt.title('NN Pricer Training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare NN vs BS prices\n",
    "model.eval()\n",
    "\n",
    "# Test on new data\n",
    "S_test = np.linspace(60, 140, 200)\n",
    "K_test = 100.0\n",
    "T_test = 1.0\n",
    "sigma_test = 0.2\n",
    "r_test = 0.05\n",
    "\n",
    "# BS prices\n",
    "bs_prices = bs_call_numpy(S_test, K_test, T_test, sigma_test, r_test)\n",
    "\n",
    "# NN prices\n",
    "log_m_test = np.log(S_test / K_test)\n",
    "X_test_nn = np.column_stack([\n",
    "    log_m_test,\n",
    "    np.full_like(S_test, T_test),\n",
    "    np.full_like(S_test, sigma_test),\n",
    "    np.full_like(S_test, r_test),\n",
    "]).astype(np.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    nn_prices = model(torch.tensor(X_test_nn).to(device)).cpu().numpy() * K_test\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(S_test, bs_prices, label='Black-Scholes', linewidth=2)\n",
    "axes[0].plot(S_test, nn_prices, '--', label='Neural Network', linewidth=2)\n",
    "axes[0].set_title('Call Price: BS vs NN')\n",
    "axes[0].set_xlabel('Spot Price S')\n",
    "axes[0].set_ylabel('Call Price')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(S_test, bs_prices - nn_prices)\n",
    "axes[1].set_title('Pricing Error (BS - NN)')\n",
    "axes[1].set_xlabel('Spot Price S')\n",
    "axes[1].set_ylabel('Error ($)')\n",
    "axes[1].axhline(0, color='gray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean absolute error: ${np.abs(bs_prices - nn_prices).mean():.4f}\")\n",
    "print(f\"Max absolute error:  ${np.abs(bs_prices - nn_prices).max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Greeks via autograd\n",
    "def nn_greeks(model, S_vals, K, T, sigma, r):\n",
    "    \"\"\"Compute Greeks from the NN pricer using autograd.\"\"\"\n",
    "    S_t = torch.tensor(S_vals, dtype=torch.float32, requires_grad=True).to(device)\n",
    "    log_m = torch.log(S_t / K)\n",
    "    T_t = torch.full_like(S_t, T)\n",
    "    sigma_t = torch.full_like(S_t, sigma)\n",
    "    r_t = torch.full_like(S_t, r)\n",
    "    \n",
    "    x = torch.stack([log_m, T_t, sigma_t, r_t], dim=1)\n",
    "    price = model(x) * K  # un-normalize\n",
    "    \n",
    "    # Delta = dC/dS\n",
    "    delta = torch.autograd.grad(price.sum(), S_t, create_graph=True)[0]\n",
    "    # Gamma = d2C/dS2\n",
    "    gamma = torch.autograd.grad(delta.sum(), S_t)[0]\n",
    "    \n",
    "    return (\n",
    "        price.detach().cpu().numpy(),\n",
    "        delta.detach().cpu().numpy(),\n",
    "        gamma.detach().cpu().numpy(),\n",
    "    )\n",
    "\n",
    "nn_price, nn_delta, nn_gamma = nn_greeks(model, S_test, 100.0, 1.0, 0.2, 0.05)\n",
    "\n",
    "# Compare\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "axes[0].plot(S_test, bs_prices, label='BS')\n",
    "axes[0].plot(S_test, nn_price, '--', label='NN')\n",
    "axes[0].set_title('Price')\n",
    "axes[0].legend()\n",
    "\n",
    "bs_deltas = bs_delta_numpy(S_test, 100, 1.0, 0.2, 0.05)\n",
    "axes[1].plot(S_test, bs_deltas, label='BS')\n",
    "axes[1].plot(S_test, nn_delta, '--', label='NN')\n",
    "axes[1].set_title('Delta')\n",
    "axes[1].legend()\n",
    "\n",
    "bs_gammas = bs_gamma_numpy(S_test, 100, 1.0, 0.2, 0.05)\n",
    "axes[2].plot(S_test, bs_gammas, label='BS')\n",
    "axes[2].plot(S_test, nn_gamma, '--', label='NN')\n",
    "axes[2].set_title('Gamma')\n",
    "axes[2].legend()\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('Spot Price S')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Black-Scholes is a starting point** — real markets have stochastic vol, jumps, skew\n",
    "2. **NNs can learn any pricing function** — from BS, Heston, or raw market data\n",
    "3. **Autograd Greeks are free** — just differentiate the pricing network\n",
    "4. **Speed advantage is massive** — microseconds vs seconds for Monte Carlo\n",
    "5. **No-arbitrage constraints** are important — use softplus, add penalty terms\n",
    "6. **Deep Hedging** is the frontier — learn hedging end-to-end without pricing\n",
    "\n",
    "### Next: Seminar\n",
    "Hands-on with PyTorch BS, NN pricing, and Greek computation."
   ]
  }
 ]
}