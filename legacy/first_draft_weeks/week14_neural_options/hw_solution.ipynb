{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 12 â€” SOLUTION\n",
    "\n",
    "# Neural Network Options Pricer\n",
    "\n",
    "**This is the instructor solution. Do not distribute to students.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import brentq\n",
    "import time\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Generate 500K Black-Scholes Training Samples (15 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Black-Scholes implementation\n",
    "def bs_call(S, K, T, sigma, r):\n",
    "    \"\"\"European call price under Black-Scholes.\"\"\"\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "    return S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n",
    "\n",
    "def bs_delta(S, K, T, sigma, r):\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n",
    "    return norm.cdf(d1)\n",
    "\n",
    "def bs_gamma(S, K, T, sigma, r):\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n",
    "    return norm.pdf(d1) / (S * sigma * np.sqrt(T))\n",
    "\n",
    "def bs_vega(S, K, T, sigma, r):\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n",
    "    return S * norm.pdf(d1) * np.sqrt(T)\n",
    "\n",
    "def bs_theta(S, K, T, sigma, r):\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "    return (-S * norm.pdf(d1) * sigma / (2 * np.sqrt(T))\n",
    "            - r * K * np.exp(-r * T) * norm.cdf(d2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 500K samples\n",
    "N = 500_000\n",
    "\n",
    "S_raw = np.random.uniform(50, 150, N)\n",
    "K_raw = np.random.uniform(50, 150, N)\n",
    "T_raw = np.random.uniform(0.05, 3.0, N)\n",
    "sigma_raw = np.random.uniform(0.05, 0.6, N)\n",
    "r_raw = np.random.uniform(0.0, 0.1, N)\n",
    "\n",
    "C_raw = bs_call(S_raw, K_raw, T_raw, sigma_raw, r_raw)\n",
    "\n",
    "# Normalized features\n",
    "log_moneyness = np.log(S_raw / K_raw)\n",
    "X_all = np.column_stack([log_moneyness, T_raw, sigma_raw, r_raw]).astype(np.float32)\n",
    "y_all = (C_raw / K_raw).astype(np.float32)  # normalized price\n",
    "\n",
    "# Store raw data for later use\n",
    "raw_data = np.column_stack([S_raw, K_raw, T_raw, sigma_raw, r_raw, C_raw])\n",
    "\n",
    "# Split: 80/10/10\n",
    "n_train = int(0.8 * N)\n",
    "n_val = int(0.1 * N)\n",
    "\n",
    "X_train = torch.tensor(X_all[:n_train]).to(device)\n",
    "y_train = torch.tensor(y_all[:n_train]).to(device)\n",
    "X_val = torch.tensor(X_all[n_train:n_train+n_val]).to(device)\n",
    "y_val = torch.tensor(y_all[n_train:n_train+n_val]).to(device)\n",
    "X_test = torch.tensor(X_all[n_train+n_val:]).to(device)\n",
    "y_test = torch.tensor(y_all[n_train+n_val:]).to(device)\n",
    "\n",
    "print(f\"Dataset: {N:,} samples\")\n",
    "print(f\"Train: {n_train:,} | Val: {n_val:,} | Test: {N - n_train - n_val:,}\")\n",
    "print(f\"\\nFeature statistics:\")\n",
    "feat_names = ['log(S/K)', 'T', 'sigma', 'r']\n",
    "for i, name in enumerate(feat_names):\n",
    "    print(f\"  {name:10s}: mean={X_all[:, i].mean():.4f}, std={X_all[:, i].std():.4f}, \"\n",
    "          f\"range=[{X_all[:, i].min():.4f}, {X_all[:, i].max():.4f}]\")\n",
    "print(f\"\\nTarget (C/K): mean={y_all.mean():.4f}, std={y_all.std():.4f}, range=[{y_all.min():.4f}, {y_all.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Train NN Pricer, Evaluate Extrapolation (25 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptionPricer(nn.Module):\n",
    "    def __init__(self, input_dim=4, hidden=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Softplus(),  # positive output\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "model = OptionPricer(hidden=256).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "batch_size = 8192\n",
    "n_epochs = 80\n",
    "train_losses, val_losses = [], []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    perm = torch.randperm(n_train, device=device)\n",
    "    epoch_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for i in range(0, n_train, batch_size):\n",
    "        idx = perm[i:i+batch_size]\n",
    "        pred = model(X_train[idx])\n",
    "        loss = loss_fn(pred, y_train[idx])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    train_losses.append(epoch_loss / n_batches)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = model(X_val)\n",
    "        val_loss = loss_fn(val_pred, y_val).item()\n",
    "    val_losses.append(val_loss)\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch+1:3d} | Train: {train_losses[-1]:.2e} | Val: {val_loss:.2e} | LR: {lr:.1e}\")\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(best_state)\n",
    "\n",
    "plt.plot(train_losses, label='Train')\n",
    "plt.plot(val_losses, label='Validation')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_pred = model(X_test).cpu().numpy()\n",
    "    test_true = y_test.cpu().numpy()\n",
    "\n",
    "errors = test_pred - test_true\n",
    "mse = (errors**2).mean()\n",
    "mae = np.abs(errors).mean()\n",
    "r2 = 1 - (errors**2).sum() / ((test_true - test_true.mean())**2).sum()\n",
    "\n",
    "print(f\"Test MSE:  {mse:.2e}\")\n",
    "print(f\"Test MAE:  {mae:.2e}\")\n",
    "print(f\"Test R2:   {r2:.6f}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].scatter(test_true[:5000], test_pred[:5000], s=1, alpha=0.3)\n",
    "axes[0].plot([0, test_true.max()], [0, test_true.max()], 'r--', linewidth=2)\n",
    "axes[0].set_title(f'Predicted vs True (R2 = {r2:.4f})')\n",
    "axes[0].set_xlabel('True C/K')\n",
    "axes[0].set_ylabel('Predicted C/K')\n",
    "\n",
    "axes[1].hist(errors, bins=100, edgecolor='black', linewidth=0.3)\n",
    "axes[1].set_title(f'Error Distribution (MAE = {mae:.2e})')\n",
    "axes[1].set_xlabel('Error')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrapolation test\n",
    "S_ext = np.linspace(20, 200, 500).astype(np.float32)\n",
    "K_fix, T_fix, sig_fix, r_fix = 100.0, 1.0, 0.2, 0.05\n",
    "\n",
    "bs_ext = bs_call(S_ext, K_fix, T_fix, sig_fix, r_fix)\n",
    "\n",
    "log_m_ext = np.log(S_ext / K_fix)\n",
    "X_ext = np.column_stack([\n",
    "    log_m_ext,\n",
    "    np.full_like(S_ext, T_fix),\n",
    "    np.full_like(S_ext, sig_fix),\n",
    "    np.full_like(S_ext, r_fix),\n",
    "]).astype(np.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    nn_ext = model(torch.tensor(X_ext).to(device)).cpu().numpy() * K_fix\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(S_ext, bs_ext, label='Black-Scholes', linewidth=2)\n",
    "axes[0].plot(S_ext, nn_ext, '--', label='Neural Network', linewidth=2)\n",
    "axes[0].axvspan(50, 150, alpha=0.1, color='green', label='Training range')\n",
    "axes[0].set_title('Extrapolation Test')\n",
    "axes[0].set_xlabel('Spot Price S')\n",
    "axes[0].set_ylabel('Call Price')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(S_ext, np.abs(bs_ext - nn_ext))\n",
    "axes[1].axvspan(50, 150, alpha=0.1, color='green', label='Training range')\n",
    "axes[1].set_title('Absolute Pricing Error')\n",
    "axes[1].set_xlabel('Spot Price S')\n",
    "axes[1].set_ylabel('|BS - NN| ($)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "in_range = (S_ext >= 50) & (S_ext <= 150)\n",
    "print(f\"MAE in training range:  ${np.abs(bs_ext[in_range] - nn_ext[in_range]).mean():.4f}\")\n",
    "print(f\"MAE outside range:      ${np.abs(bs_ext[~in_range] - nn_ext[~in_range]).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commentary:** The NN pricer achieves very high accuracy within the training range (R2 > 0.9999). Extrapolation accuracy degrades outside the training range, particularly for deep in-the-money options where prices grow linearly. This is expected -- neural networks are interpolators, not extrapolators. In production, the training range should cover all realistic input values with a margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Autograd Greeks vs Analytical (20 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nn_greeks(model, S_vals, K, T, sigma, r):\n",
    "    \"\"\"Compute Delta, Gamma, Vega, Theta from NN using autograd.\"\"\"\n",
    "    S_t = torch.tensor(S_vals, dtype=torch.float32, requires_grad=True).to(device)\n",
    "    sigma_t = torch.tensor(np.full_like(S_vals, sigma), dtype=torch.float32,\n",
    "                           requires_grad=True).to(device)\n",
    "    T_t = torch.tensor(np.full_like(S_vals, T), dtype=torch.float32,\n",
    "                       requires_grad=True).to(device)\n",
    "    r_t = torch.full_like(S_t, r)\n",
    "    \n",
    "    log_m = torch.log(S_t / K)\n",
    "    x = torch.stack([log_m, T_t, sigma_t, r_t], dim=1)\n",
    "    price_norm = model(x)\n",
    "    price = price_norm * K\n",
    "    \n",
    "    total = price.sum()\n",
    "    \n",
    "    # First-order Greeks\n",
    "    grads = torch.autograd.grad(total, [S_t, sigma_t, T_t], create_graph=True)\n",
    "    delta_t = grads[0]\n",
    "    vega_t = grads[1]\n",
    "    theta_t = grads[2]\n",
    "    \n",
    "    # Gamma (second derivative)\n",
    "    gamma_t = torch.autograd.grad(delta_t.sum(), S_t)[0]\n",
    "    \n",
    "    return {\n",
    "        'price': price.detach().cpu().numpy(),\n",
    "        'delta': delta_t.detach().cpu().numpy(),\n",
    "        'gamma': gamma_t.detach().cpu().numpy(),\n",
    "        'vega': vega_t.detach().cpu().numpy(),\n",
    "        'theta': theta_t.detach().cpu().numpy(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute for range of spot prices\n",
    "S_plot = np.linspace(70, 130, 400).astype(np.float32)\n",
    "K_v, T_v, sig_v, r_v = 100.0, 1.0, 0.2, 0.05\n",
    "\n",
    "nn_g = compute_nn_greeks(model, S_plot, K_v, T_v, sig_v, r_v)\n",
    "\n",
    "analytical = {\n",
    "    'price': bs_call(S_plot, K_v, T_v, sig_v, r_v),\n",
    "    'delta': bs_delta(S_plot, K_v, T_v, sig_v, r_v),\n",
    "    'gamma': bs_gamma(S_plot, K_v, T_v, sig_v, r_v),\n",
    "    'vega': bs_vega(S_plot, K_v, T_v, sig_v, r_v),\n",
    "    'theta': bs_theta(S_plot, K_v, T_v, sig_v, r_v),\n",
    "}\n",
    "\n",
    "# Plot comparisons\n",
    "greeks_names = ['delta', 'gamma', 'vega', 'theta']\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 8))\n",
    "\n",
    "# Top row: Greek values\n",
    "for i, greek in enumerate(greeks_names):\n",
    "    axes[0, i].plot(S_plot, analytical[greek], label='Analytical', linewidth=2)\n",
    "    axes[0, i].plot(S_plot, nn_g[greek], '--', label='NN Autograd', linewidth=2)\n",
    "    axes[0, i].set_title(greek.capitalize())\n",
    "    axes[0, i].set_xlabel('S')\n",
    "    axes[0, i].legend(fontsize=8)\n",
    "\n",
    "# Bottom row: Errors\n",
    "mae_results = {}\n",
    "for i, greek in enumerate(greeks_names):\n",
    "    error = nn_g[greek] - analytical[greek]\n",
    "    mae_results[greek] = np.abs(error).mean()\n",
    "    axes[1, i].plot(S_plot, error, color='red', alpha=0.7)\n",
    "    axes[1, i].axhline(0, color='gray', linestyle='--', linewidth=0.5)\n",
    "    axes[1, i].set_title(f'{greek.capitalize()} Error (MAE={mae_results[greek]:.2e})')\n",
    "    axes[1, i].set_xlabel('S')\n",
    "\n",
    "plt.suptitle('Neural Greeks vs Analytical Greeks', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGreek MAE Summary:\")\n",
    "for greek, mae in mae_results.items():\n",
    "    print(f\"  {greek:8s}: {mae:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commentary:** Delta and Vega are approximated well by the NN. Gamma (second derivative) is the hardest to approximate because differentiating twice amplifies the approximation errors of the network. The NN produces smoother Gamma profiles than analytical, which can actually be desirable for hedging (less noisy hedge ratios). Theta is similarly well-approximated. In practice, if Greeks accuracy is critical, one can add a Greek-matching loss term during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Heston Stochastic Volatility Model Pricing (25 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heston_mc_price(S0, K, T, v0, kappa, theta, xi, rho, r,\n",
    "                    n_paths=50_000, n_steps=100):\n",
    "    \"\"\"Price European call under Heston model using Euler Monte Carlo.\"\"\"\n",
    "    dt = T / n_steps\n",
    "    sqrt_dt = np.sqrt(dt)\n",
    "    \n",
    "    S = np.full(n_paths, S0, dtype=np.float64)\n",
    "    v = np.full(n_paths, v0, dtype=np.float64)\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        z1 = np.random.standard_normal(n_paths)\n",
    "        z2 = rho * z1 + np.sqrt(1 - rho**2) * np.random.standard_normal(n_paths)\n",
    "        \n",
    "        v_pos = np.maximum(v, 0)  # full truncation scheme\n",
    "        sqrt_v = np.sqrt(v_pos)\n",
    "        \n",
    "        S = S * np.exp((r - 0.5 * v_pos) * dt + sqrt_v * sqrt_dt * z1)\n",
    "        v = v + kappa * (theta - v_pos) * dt + xi * sqrt_v * sqrt_dt * z2\n",
    "    \n",
    "    payoff = np.maximum(S - K, 0)\n",
    "    price = np.exp(-r * T) * payoff.mean()\n",
    "    return price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heston parameters\n",
    "kappa = 2.0\n",
    "theta_h = 0.04  # long-run variance\n",
    "xi = 0.3\n",
    "rho = -0.7\n",
    "r_h = 0.05\n",
    "\n",
    "# Verify: single price\n",
    "price_heston = heston_mc_price(100, 100, 1.0, 0.04, kappa, theta_h, xi, rho, r_h)\n",
    "price_bs = bs_call(100, 100, 1.0, np.sqrt(0.04), r_h)\n",
    "print(f\"Heston price: ${price_heston:.4f}\")\n",
    "print(f\"BS price (sigma=0.2): ${price_bs:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data for Heston NN\n",
    "# Vary: S, K, T, v0.  Fix: kappa, theta, xi, rho, r\n",
    "N_heston = 50_000\n",
    "\n",
    "S_h = np.random.uniform(60, 140, N_heston)\n",
    "K_h = np.random.uniform(60, 140, N_heston)\n",
    "T_h = np.random.uniform(0.1, 2.0, N_heston)\n",
    "v0_h = np.random.uniform(0.01, 0.09, N_heston)  # initial variance\n",
    "\n",
    "print(f\"Generating {N_heston:,} Heston prices (this may take a few minutes)...\")\n",
    "C_heston = np.zeros(N_heston)\n",
    "t_start = time.time()\n",
    "\n",
    "for i in range(N_heston):\n",
    "    C_heston[i] = heston_mc_price(\n",
    "        S_h[i], K_h[i], T_h[i], v0_h[i],\n",
    "        kappa, theta_h, xi, rho, r_h,\n",
    "        n_paths=10_000, n_steps=50,\n",
    "    )\n",
    "    if (i + 1) % 10_000 == 0:\n",
    "        elapsed = time.time() - t_start\n",
    "        print(f\"  {i+1:,}/{N_heston:,} ({elapsed:.0f}s)\")\n",
    "\n",
    "mc_time = time.time() - t_start\n",
    "print(f\"\\nTotal MC time: {mc_time:.1f}s ({mc_time/N_heston*1000:.1f}ms per price)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Heston training data\n",
    "log_m_h = np.log(S_h / K_h)\n",
    "sqrt_v0 = np.sqrt(v0_h)  # use sqrt(v0) as feature (more intuitive)\n",
    "X_heston = np.column_stack([log_m_h, T_h, sqrt_v0]).astype(np.float32)\n",
    "y_heston = (C_heston / K_h).astype(np.float32)\n",
    "\n",
    "# Split\n",
    "n_train_h = int(0.85 * N_heston)\n",
    "X_h_tr = torch.tensor(X_heston[:n_train_h]).to(device)\n",
    "y_h_tr = torch.tensor(y_heston[:n_train_h]).to(device)\n",
    "X_h_te = torch.tensor(X_heston[n_train_h:]).to(device)\n",
    "y_h_te = torch.tensor(y_heston[n_train_h:]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Heston NN\n",
    "heston_model = nn.Sequential(\n",
    "    nn.Linear(3, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 1),\n",
    "    nn.Softplus(),\n",
    ").to(device)\n",
    "\n",
    "h_optimizer = torch.optim.Adam(heston_model.parameters(), lr=1e-3)\n",
    "h_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(h_optimizer, patience=10, factor=0.5)\n",
    "\n",
    "h_train_losses, h_val_losses = [], []\n",
    "batch_size = 2048\n",
    "\n",
    "for epoch in range(100):\n",
    "    heston_model.train()\n",
    "    perm = torch.randperm(n_train_h, device=device)\n",
    "    epoch_loss = 0\n",
    "    n_b = 0\n",
    "    \n",
    "    for i in range(0, n_train_h, batch_size):\n",
    "        idx = perm[i:i+batch_size]\n",
    "        pred = heston_model(X_h_tr[idx]).squeeze(-1)\n",
    "        loss = nn.MSELoss()(pred, y_h_tr[idx])\n",
    "        h_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        h_optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        n_b += 1\n",
    "    \n",
    "    h_train_losses.append(epoch_loss / n_b)\n",
    "    \n",
    "    heston_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = nn.MSELoss()(heston_model(X_h_te).squeeze(-1), y_h_te).item()\n",
    "    h_val_losses.append(val_loss)\n",
    "    h_scheduler.step(val_loss)\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d} | Train: {h_train_losses[-1]:.2e} | Val: {val_loss:.2e}\")\n",
    "\n",
    "plt.plot(h_train_losses, label='Train')\n",
    "plt.plot(h_val_losses, label='Val')\n",
    "plt.title('Heston NN Training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speed comparison\n",
    "# MC pricing: time 100 prices\n",
    "t0 = time.time()\n",
    "for i in range(100):\n",
    "    _ = heston_mc_price(100, 100, 1.0, 0.04, kappa, theta_h, xi, rho, r_h,\n",
    "                        n_paths=10_000, n_steps=50)\n",
    "mc_100 = time.time() - t0\n",
    "\n",
    "# NN pricing: time 100 prices\n",
    "test_x = torch.tensor([[0.0, 1.0, 0.2]], dtype=torch.float32).to(device)\n",
    "test_x = test_x.repeat(100, 1)\n",
    "heston_model.eval()\n",
    "t0 = time.time()\n",
    "with torch.no_grad():\n",
    "    _ = heston_model(test_x)\n",
    "nn_100 = time.time() - t0\n",
    "\n",
    "print(f\"Monte Carlo: {mc_100:.3f}s for 100 prices ({mc_100/100*1000:.1f}ms per price)\")\n",
    "print(f\"Neural Net:  {nn_100:.5f}s for 100 prices ({nn_100/100*1000:.3f}ms per price)\")\n",
    "print(f\"Speedup:     {mc_100 / nn_100:.0f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Heston vol smile: compare Heston vs BS implied vol\n",
    "strikes = np.linspace(70, 130, 30)\n",
    "v0_test = 0.04  # initial vol = 20%\n",
    "\n",
    "heston_prices = []\n",
    "for K_i in strikes:\n",
    "    p = heston_mc_price(100, K_i, 1.0, v0_test, kappa, theta_h, xi, rho, r_h,\n",
    "                        n_paths=50_000, n_steps=100)\n",
    "    heston_prices.append(p)\n",
    "heston_prices = np.array(heston_prices)\n",
    "\n",
    "# Invert BS to get implied vol\n",
    "def implied_vol(price, S, K, T, r):\n",
    "    try:\n",
    "        return brentq(lambda sig: bs_call(S, K, T, sig, r) - price, 0.01, 2.0)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "ivs = [implied_vol(p, 100, K_i, 1.0, r_h) for p, K_i in zip(heston_prices, strikes)]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(strikes, heston_prices, 'o-', label='Heston')\n",
    "bs_prices_compare = bs_call(100, strikes, 1.0, np.sqrt(v0_test), r_h)\n",
    "axes[0].plot(strikes, bs_prices_compare, 's--', label=f'BS (sigma=0.2)')\n",
    "axes[0].set_title('Heston vs BS Prices')\n",
    "axes[0].set_xlabel('Strike K')\n",
    "axes[0].set_ylabel('Call Price')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(strikes, ivs, 'o-', label='Heston IV')\n",
    "axes[1].axhline(np.sqrt(v0_test), color='red', linestyle='--', label='BS flat vol = 0.2')\n",
    "axes[1].set_title('Implied Volatility Smile (Heston)')\n",
    "axes[1].set_xlabel('Strike K')\n",
    "axes[1].set_ylabel('Implied Volatility')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The Heston model produces a volatility smile/skew due to:\")\n",
    "print(\"  - Negative correlation (rho=-0.7): leverage effect, OTM puts are expensive\")\n",
    "print(\"  - Stochastic volatility: fat tails in return distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Implied Volatility Surface Learning (15 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a parametric IV surface\n",
    "# sigma(K, T) = sigma_0 + alpha * (K/S - 1)^2 + beta / sqrt(T)\n",
    "\n",
    "sigma_0 = 0.20\n",
    "alpha = 0.8    # smile curvature\n",
    "beta = 0.02    # term structure\n",
    "\n",
    "def true_iv_surface(moneyness, T):\n",
    "    \"\"\"Parametric IV surface: moneyness = K/S.\"\"\"\n",
    "    return sigma_0 + alpha * (moneyness - 1)**2 + beta / np.sqrt(T)\n",
    "\n",
    "# Generate training data\n",
    "N_iv = 50_000\n",
    "moneyness_data = np.random.uniform(0.7, 1.3, N_iv)  # K/S\n",
    "T_iv_data = np.random.uniform(0.1, 3.0, N_iv)\n",
    "\n",
    "iv_true = true_iv_surface(moneyness_data, T_iv_data)\n",
    "\n",
    "# Add small noise (realistic market data has noise)\n",
    "iv_noisy = iv_true + np.random.normal(0, 0.005, N_iv)\n",
    "iv_noisy = np.clip(iv_noisy, 0.05, 1.0)\n",
    "\n",
    "# Features and targets\n",
    "X_iv = np.column_stack([moneyness_data, T_iv_data]).astype(np.float32)\n",
    "y_iv = iv_noisy.astype(np.float32)\n",
    "\n",
    "n_tr_iv = int(0.85 * N_iv)\n",
    "X_iv_tr = torch.tensor(X_iv[:n_tr_iv]).to(device)\n",
    "y_iv_tr = torch.tensor(y_iv[:n_tr_iv]).to(device)\n",
    "X_iv_te = torch.tensor(X_iv[n_tr_iv:]).to(device)\n",
    "y_iv_te = torch.tensor(y_iv[n_tr_iv:]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train IV surface NN\n",
    "iv_model = nn.Sequential(\n",
    "    nn.Linear(2, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1),\n",
    "    nn.Softplus(),  # positive vol\n",
    ").to(device)\n",
    "\n",
    "iv_opt = torch.optim.Adam(iv_model.parameters(), lr=1e-3)\n",
    "iv_sched = torch.optim.lr_scheduler.ReduceLROnPlateau(iv_opt, patience=10, factor=0.5)\n",
    "\n",
    "for epoch in range(80):\n",
    "    iv_model.train()\n",
    "    perm = torch.randperm(n_tr_iv, device=device)\n",
    "    epoch_loss = 0\n",
    "    n_b = 0\n",
    "    \n",
    "    for i in range(0, n_tr_iv, 2048):\n",
    "        idx = perm[i:i+2048]\n",
    "        pred = iv_model(X_iv_tr[idx]).squeeze(-1)\n",
    "        loss = nn.MSELoss()(pred, y_iv_tr[idx])\n",
    "        iv_opt.zero_grad()\n",
    "        loss.backward()\n",
    "        iv_opt.step()\n",
    "        epoch_loss += loss.item()\n",
    "        n_b += 1\n",
    "    \n",
    "    iv_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = nn.MSELoss()(iv_model(X_iv_te).squeeze(-1), y_iv_te).item()\n",
    "    iv_sched.step(val_loss)\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d} | Train: {epoch_loss/n_b:.2e} | Val: {val_loss:.2e}\")\n",
    "\n",
    "print(\"IV surface model trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize true vs learned IV surface\n",
    "m_grid = np.linspace(0.7, 1.3, 60)\n",
    "t_grid = np.linspace(0.1, 3.0, 60)\n",
    "M, TT = np.meshgrid(m_grid, t_grid)\n",
    "\n",
    "# True surface\n",
    "IV_true = true_iv_surface(M, TT)\n",
    "\n",
    "# Learned surface\n",
    "grid_flat = np.column_stack([M.ravel(), TT.ravel()]).astype(np.float32)\n",
    "iv_model.eval()\n",
    "with torch.no_grad():\n",
    "    IV_learned = iv_model(torch.tensor(grid_flat).to(device)).cpu().numpy().reshape(M.shape)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5), subplot_kw={'projection': '3d'})\n",
    "\n",
    "axes[0].plot_surface(M, TT, IV_true, cmap='viridis', alpha=0.8)\n",
    "axes[0].set_title('True IV Surface')\n",
    "axes[0].set_xlabel('Moneyness (K/S)')\n",
    "axes[0].set_ylabel('Maturity T')\n",
    "axes[0].set_zlabel('IV')\n",
    "\n",
    "axes[1].plot_surface(M, TT, IV_learned, cmap='viridis', alpha=0.8)\n",
    "axes[1].set_title('Learned IV Surface')\n",
    "axes[1].set_xlabel('Moneyness (K/S)')\n",
    "axes[1].set_ylabel('Maturity T')\n",
    "axes[1].set_zlabel('IV')\n",
    "\n",
    "axes[2].plot_surface(M, TT, np.abs(IV_true - IV_learned), cmap='Reds', alpha=0.8)\n",
    "axes[2].set_title('Absolute Error')\n",
    "axes[2].set_xlabel('Moneyness (K/S)')\n",
    "axes[2].set_ylabel('Maturity T')\n",
    "axes[2].set_zlabel('|Error|')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Surface MAE: {np.abs(IV_true - IV_learned).mean():.6f}\")\n",
    "print(f\"Surface Max Error: {np.abs(IV_true - IV_learned).max():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-section plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Vol smile at different maturities\n",
    "for T_val in [0.25, 0.5, 1.0, 2.0]:\n",
    "    axes[0].plot(m_grid, true_iv_surface(m_grid, T_val), '--', alpha=0.5)\n",
    "    x_slice = np.column_stack([m_grid, np.full_like(m_grid, T_val)]).astype(np.float32)\n",
    "    with torch.no_grad():\n",
    "        iv_nn = iv_model(torch.tensor(x_slice).to(device)).cpu().numpy().ravel()\n",
    "    axes[0].plot(m_grid, iv_nn, label=f'T={T_val}')\n",
    "\n",
    "axes[0].set_title('Vol Smile at Different Maturities (solid=NN, dashed=true)')\n",
    "axes[0].set_xlabel('Moneyness (K/S)')\n",
    "axes[0].set_ylabel('Implied Volatility')\n",
    "axes[0].legend()\n",
    "\n",
    "# Term structure at different moneyness\n",
    "for m_val in [0.8, 0.9, 1.0, 1.1, 1.2]:\n",
    "    axes[1].plot(t_grid, true_iv_surface(m_val, t_grid), '--', alpha=0.5)\n",
    "    x_slice = np.column_stack([np.full_like(t_grid, m_val), t_grid]).astype(np.float32)\n",
    "    with torch.no_grad():\n",
    "        iv_nn = iv_model(torch.tensor(x_slice).to(device)).cpu().numpy().ravel()\n",
    "    axes[1].plot(t_grid, iv_nn, label=f'K/S={m_val}')\n",
    "\n",
    "axes[1].set_title('IV Term Structure (solid=NN, dashed=true)')\n",
    "axes[1].set_xlabel('Maturity T')\n",
    "axes[1].set_ylabel('Implied Volatility')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commentary:**\n",
    "\n",
    "The NN learns the IV surface accurately, capturing both the smile (curvature in moneyness) and the term structure (higher vol at short maturities). The model handles the noise in the training data well, producing a smooth surface. In practice, this approach replaces parametric models like SVI with a more flexible learned surface that can adapt to arbitrary market conditions. The main risk is extrapolation -- the model should not be used outside the training range of moneyness and maturity values."
   ]
  }
 ]
}