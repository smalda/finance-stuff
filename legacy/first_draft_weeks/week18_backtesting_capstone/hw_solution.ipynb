{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 14 — Capstone Solution: End-to-End ML Trading Strategy\n",
    "\n",
    "**Course:** ML for Quantitative Finance  \n",
    "**Status:** SOLUTION — do not distribute to students before deadline\n",
    "\n",
    "---\n",
    "\n",
    "**Strategy:** Cross-sectional momentum + mean-reversion + volatility model  \n",
    "**Model:** XGBoost with expanding-window walk-forward  \n",
    "**Portfolio:** Long-short quintile with 10 bps/side transaction costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TICKERS = [\n",
    "    'AAPL', 'MSFT', 'AMZN', 'GOOGL', 'META', 'NVDA', 'JPM', 'JNJ', 'V', 'PG',\n",
    "    'UNH', 'HD', 'MA', 'DIS', 'BAC', 'XOM', 'CSCO', 'PFE', 'COST', 'ABT',\n",
    "    'PEP', 'AVGO', 'CRM', 'NKE', 'CVX', 'WMT', 'MRK', 'LLY', 'ABBV', 'INTC',\n",
    "    'T', 'VZ', 'QCOM', 'TXN', 'PM', 'UNP', 'NEE', 'LOW', 'BMY', 'AMGN',\n",
    "    'MDT', 'HON', 'SBUX', 'GS', 'MS', 'BLK', 'GILD', 'MMC', 'ADP', 'AMT',\n",
    "    'CME', 'CI', 'LRCX', 'MO', 'MDLZ', 'SO', 'DUK', 'CL', 'ZTS', 'BDX',\n",
    "    'REGN', 'ITW', 'APD', 'SHW', 'FISV', 'NOC', 'ICE', 'CSX', 'WM', 'FDX',\n",
    "    'EMR', 'PNC', 'USB', 'NSC', 'CCI', 'D', 'GM', 'F', 'TGT', 'AEP',\n",
    "]\n",
    "\n",
    "cache_path = Path('w14_capstone_cache.pkl')\n",
    "if cache_path.exists():\n",
    "    raw = pd.read_pickle(cache_path)\n",
    "else:\n",
    "    raw = yf.download(TICKERS, start='2010-01-01', end='2024-12-31', progress=True)\n",
    "    raw.to_pickle(cache_path)\n",
    "\n",
    "prices = raw['Close'].ffill()\n",
    "volume = raw['Volume'].ffill()\n",
    "returns_daily = prices.pct_change()\n",
    "\n",
    "# Drop tickers with >20% missing\n",
    "good = prices.isnull().mean() < 0.2\n",
    "prices = prices.loc[:, good]\n",
    "volume = volume.loc[:, prices.columns]\n",
    "returns_daily = returns_daily[prices.columns]\n",
    "\n",
    "monthly_prices = prices.resample('M').last()\n",
    "monthly_returns = monthly_prices.pct_change()\n",
    "monthly_volume = volume.resample('M').mean()\n",
    "\n",
    "print(f\"Universe: {prices.shape[1]} stocks\")\n",
    "print(f\"Period: {prices.index[0].date()} to {prices.index[-1].date()}\")\n",
    "print(f\"Monthly observations: {len(monthly_prices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Feature Engineering (18 features across 4 categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}\n",
    "\n",
    "# --- Momentum (5 features) ---\n",
    "features['mom_1m'] = monthly_prices.pct_change(1)\n",
    "features['mom_3m'] = monthly_prices.pct_change(3)\n",
    "features['mom_6m'] = monthly_prices.pct_change(6)\n",
    "features['mom_12m'] = monthly_prices.pct_change(12)\n",
    "features['mom_12m_skip1'] = monthly_prices.pct_change(12).shift(1)\n",
    "\n",
    "# --- Reversal (2 features) ---\n",
    "features['reversal_1m'] = -monthly_prices.pct_change(1)\n",
    "features['reversal_1w'] = -(prices.pct_change(5)).resample('M').last()\n",
    "\n",
    "# --- Volatility (4 features) ---\n",
    "features['vol_20d'] = returns_daily.rolling(20).std().resample('M').last()\n",
    "features['vol_60d'] = returns_daily.rolling(60).std().resample('M').last()\n",
    "features['vol_ratio'] = features['vol_20d'] / (features['vol_60d'] + 1e-8)\n",
    "# Vol-of-vol: rolling std of rolling vol\n",
    "daily_vol = returns_daily.rolling(20).std()\n",
    "features['vol_of_vol'] = daily_vol.rolling(60).std().resample('M').last()\n",
    "\n",
    "# --- Volume (3 features) ---\n",
    "features['volume_ratio_5_60'] = (volume.rolling(5).mean() / volume.rolling(60).mean()).resample('M').last()\n",
    "features['dollar_volume'] = (prices * volume).rolling(20).mean().resample('M').last()\n",
    "features['volume_trend'] = (volume.rolling(5).mean() / volume.rolling(20).mean()).resample('M').last()\n",
    "\n",
    "# --- Technical (4 features) ---\n",
    "features['ma_50_ratio'] = (prices / prices.rolling(50).mean()).resample('M').last()\n",
    "features['ma_200_ratio'] = (prices / prices.rolling(200).mean()).resample('M').last()\n",
    "# Bollinger band position\n",
    "bb_mid = prices.rolling(20).mean()\n",
    "bb_std = prices.rolling(20).std()\n",
    "features['bb_position'] = ((prices - bb_mid) / (2 * bb_std + 1e-8)).resample('M').last()\n",
    "# RSI approximation (14-day)\n",
    "delta = prices.diff()\n",
    "gain = delta.clip(lower=0).rolling(14).mean()\n",
    "loss = (-delta.clip(upper=0)).rolling(14).mean()\n",
    "rs = gain / (loss + 1e-8)\n",
    "features['rsi_14'] = (100 - 100 / (1 + rs)).resample('M').last()\n",
    "\n",
    "# Target: next month return\n",
    "target = monthly_returns.shift(-1)\n",
    "\n",
    "print(f\"Features: {len(features)}\")\n",
    "for name in features:\n",
    "    print(f\"  {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build panel dataset\n",
    "months = sorted(set.intersection(*[set(f.index) for f in features.values()]))\n",
    "months = [m for m in months if pd.Timestamp('2012-01-01') <= m <= pd.Timestamp('2024-06-30')]\n",
    "\n",
    "X_all, y_all, dates_all, tickers_all = [], [], [], []\nfor month in months:\n",
    "    X_cs = pd.DataFrame({name: feat.loc[month] for name, feat in features.items() if month in feat.index})\n",
    "    y_cs = target.loc[month] if month in target.index else pd.Series(dtype=float)\n",
    "    valid = X_cs.dropna().index.intersection(y_cs.dropna().index)\n",
    "    if len(valid) > 10:\n",
    "        # Rank-transform features cross-sectionally\n",
    "        X_ranked = X_cs.loc[valid].rank(pct=True)\n",
    "        X_all.append(X_ranked)\n",
    "        y_all.append(y_cs.loc[valid])\n",
    "        dates_all.extend([month] * len(valid))\n",
    "        tickers_all.extend(valid.tolist())\n",
    "\n",
    "X_panel = pd.concat(X_all)\n",
    "y_panel = pd.concat(y_all)\n",
    "dates_panel = np.array(dates_all)\n",
    "tickers_panel = np.array(tickers_all)\n",
    "\n",
    "print(f\"Panel: {len(X_panel)} obs, {X_panel.shape[1]} features, {len(months)} months\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Labeling\n",
    "\n",
    "Using forward 1-month returns as labels. Justification:\n",
    "- Monthly rebalancing frequency is standard for cross-sectional equity strategies\n",
    "- Regression target (continuous returns) rather than classification\n",
    "- Triple-barrier is better suited for single-stock entry/exit timing, not cross-sectional ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label statistics\n",
    "print(\"Target (next-month return) statistics:\")\n",
    "print(f\"  Mean: {y_panel.mean():.4f}\")\n",
    "print(f\"  Std: {y_panel.std():.4f}\")\n",
    "print(f\"  Skew: {stats.skew(y_panel.dropna()):.2f}\")\n",
    "print(f\"  Kurtosis: {stats.kurtosis(y_panel.dropna(), fisher=False):.2f}\")\n",
    "print(f\"  % positive: {(y_panel > 0).mean():.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Model Training (XGBoost with Expanding Window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost parameters (conservative to avoid overfitting)\n",
    "xgb_params = {\n",
    "    'max_depth': 4,\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 200,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'reg_alpha': 1.0,\n",
    "    'reg_lambda': 1.0,\n",
    "    'verbosity': 0,\n",
    "}\n",
    "\n",
    "# Check IC on validation set first\n",
    "val_start = pd.Timestamp('2016-01-31')\n",
    "val_end = pd.Timestamp('2018-01-31')\n",
    "\n",
    "train_mask = dates_panel < val_start\n",
    "val_mask = (dates_panel >= val_start) & (dates_panel < val_end)\n",
    "\n",
    "model_val = xgb.XGBRegressor(**xgb_params)\n",
    "model_val.fit(X_panel.values[train_mask], y_panel.values[train_mask])\n",
    "pred_val = model_val.predict(X_panel.values[val_mask])\n",
    "\n",
    "# IC per month on validation\n",
    "val_ics = []\n",
    "val_dates = dates_panel[val_mask]\n",
    "for m in np.unique(val_dates):\n",
    "    m_mask = val_dates == m\n",
    "    if m_mask.sum() > 5:\n",
    "        ic = stats.spearmanr(pred_val[m_mask], y_panel.values[val_mask][m_mask])[0]\n",
    "        val_ics.append(ic)\n",
    "\n",
    "print(f\"Validation IC (2016-2017):\")\n",
    "print(f\"  Mean IC: {np.mean(val_ics):.4f}\")\n",
    "print(f\"  IC > 0: {np.mean([x > 0 for x in val_ics]):.0%}\")\n",
    "print(f\"  IC t-stat: {np.mean(val_ics)/np.std(val_ics)*np.sqrt(len(val_ics)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Walk-Forward Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_start = pd.Timestamp('2018-01-31')\n",
    "pred_months = [m for m in months if m >= pred_start]\n",
    "\n",
    "portfolio_returns = []\n",
    "monthly_ics = []\n",
    "tc_per_side = 10  # bps\n",
    "tc_total = tc_per_side * 2 / 10000  # both sides\n",
    "\n",
    "prev_longs, prev_shorts = set(), set()\n",
    "\n",
    "for month in pred_months:\n",
    "    train_mask = (dates_panel < month) & (dates_panel >= pd.Timestamp('2012-01-01'))\n",
    "    test_mask = dates_panel == month\n",
    "\n",
    "    if test_mask.sum() < 10 or train_mask.sum() < 500:\n",
    "        continue\n",
    "\n",
    "    X_tr, y_tr = X_panel.values[train_mask], y_panel.values[train_mask]\n",
    "    X_te = X_panel.values[test_mask]\n",
    "    y_te = y_panel.values[test_mask]\n",
    "    te_tickers = tickers_panel[test_mask]\n",
    "\n",
    "    # Train fresh model each month (expanding window)\n",
    "    model = xgb.XGBRegressor(**xgb_params)\n",
    "    model.fit(X_tr, y_tr)\n",
    "    pred = model.predict(X_te)\n",
    "\n",
    "    # IC\n",
    "    ic = stats.spearmanr(pred, y_te)[0]\n",
    "    monthly_ics.append({'month': month, 'IC': ic})\n",
    "\n",
    "    # Long-short quintile portfolio\n",
    "    pred_series = pd.Series(pred, index=te_tickers)\n",
    "    n_stocks = len(pred_series) // 5\n",
    "    if n_stocks < 2:\n",
    "        continue\n",
    "\n",
    "    longs = set(pred_series.nlargest(n_stocks).index)\n",
    "    shorts = set(pred_series.nsmallest(n_stocks).index)\n",
    "\n",
    "    actual = pd.Series(y_te, index=te_tickers)\n",
    "    long_ret = actual.loc[list(longs)].mean()\n",
    "    short_ret = actual.loc[list(shorts)].mean()\n",
    "    gross_ret = long_ret - short_ret\n",
    "\n",
    "    # Turnover-adjusted costs\n",
    "    long_turnover = 1 - len(longs & prev_longs) / max(len(longs), 1)\n",
    "    short_turnover = 1 - len(shorts & prev_shorts) / max(len(shorts), 1)\n",
    "    avg_turnover = (long_turnover + short_turnover) / 2\n",
    "    cost = avg_turnover * tc_total\n",
    "\n",
    "    net_ret = gross_ret - cost\n",
    "\n",
    "    portfolio_returns.append({\n",
    "        'month': month,\n",
    "        'gross': gross_ret,\n",
    "        'net': net_ret,\n",
    "        'turnover': avg_turnover,\n",
    "        'long_ret': long_ret,\n",
    "        'short_ret': short_ret,\n",
    "    })\n",
    "\n",
    "    prev_longs, prev_shorts = longs, shorts\n",
    "\n",
    "results = pd.DataFrame(portfolio_returns).set_index('month')\n",
    "ic_df = pd.DataFrame(monthly_ics).set_index('month')\n",
    "print(f\"Backtest period: {results.index[0].date()} to {results.index[-1].date()}\")\n",
    "print(f\"Total months: {len(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deflated_sharpe_ratio(sharpe_obs, n_trials, T, skew=0, kurtosis=3):\n",
    "    euler_mascheroni = 0.5772\n",
    "    sr0 = np.sqrt(2 * np.log(max(n_trials, 2))) - (np.log(np.pi) + euler_mascheroni) / (2 * np.sqrt(2 * np.log(max(n_trials, 2))))\n",
    "    se_sr = np.sqrt((1 + 0.5 * sharpe_obs**2 - skew * sharpe_obs +\n",
    "                     (kurtosis - 3) / 4 * sharpe_obs**2) / T)\n",
    "    z = (sharpe_obs - sr0) / se_sr\n",
    "    return stats.norm.cdf(z)\n",
    "\n",
    "\n",
    "def full_tear_sheet(returns_series, name='Strategy'):\n",
    "    r = returns_series.dropna()\n",
    "    cum = (1 + r).cumprod()\n",
    "    dd = cum / cum.cummax() - 1\n",
    "\n",
    "    metrics = {\n",
    "        'Ann. Return': r.mean() * 12,\n",
    "        'Ann. Volatility': r.std() * np.sqrt(12),\n",
    "        'Sharpe': r.mean() / r.std() * np.sqrt(12) if r.std() > 0 else np.nan,\n",
    "        'Sortino': r.mean() / r[r < 0].std() * np.sqrt(12) if (r < 0).sum() > 0 else np.nan,\n",
    "        'Calmar': (r.mean() * 12) / abs(dd.min()) if dd.min() != 0 else np.nan,\n",
    "        'Max Drawdown': dd.min(),\n",
    "        'Hit Rate': (r > 0).mean(),\n",
    "        'Profit Factor': abs(r[r > 0].sum() / r[r < 0].sum()) if (r < 0).sum() > 0 else np.nan,\n",
    "        'Skewness': stats.skew(r),\n",
    "        'Kurtosis': stats.kurtosis(r, fisher=False),\n",
    "        'VaR 5%': np.percentile(r, 5),\n",
    "        'CVaR 5%': r[r <= np.percentile(r, 5)].mean(),\n",
    "        'Tail Ratio': abs(np.percentile(r, 95) / np.percentile(r, 5)) if np.percentile(r, 5) != 0 else np.nan,\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{'='*55}\")\n",
    "    print(f\" {name}\")\n",
    "    print(f\"{'='*55}\")\n",
    "    for k, v in metrics.items():\n",
    "        if 'Rate' in k or 'Return' in k or 'Volatility' in k or 'Drawdown' in k or 'VaR' in k or 'CVaR' in k:\n",
    "            print(f\"  {k:<20s}: {v:.1%}\")\n",
    "        else:\n",
    "            print(f\"  {k:<20s}: {v:.3f}\")\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# Performance\n",
    "metrics_gross = full_tear_sheet(results['gross'], 'XGBoost L/S (Gross)')\n",
    "metrics_net = full_tear_sheet(results['net'], 'XGBoost L/S (Net of 10bps/side)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deflated Sharpe Ratio\n",
    "# Honest accounting: we tried 1 model type (XGBoost), 1 parameter config,\n",
    "# but experimented with ~3 feature sets during development = ~3 trials\n",
    "n_trials_honest = 3\n",
    "r = results['net']\n",
    "sharpe = r.mean() / r.std() * np.sqrt(12)\n",
    "dsr = deflated_sharpe_ratio(\n",
    "    sharpe_obs=sharpe, n_trials=n_trials_honest, T=len(r),\n",
    "    skew=stats.skew(r), kurtosis=stats.kurtosis(r, fisher=False)\n",
    ")\n",
    "print(f\"\\nDeflated Sharpe Ratio:\")\n",
    "print(f\"  Observed Sharpe: {sharpe:.2f}\")\n",
    "print(f\"  Trials (honest): {n_trials_honest}\")\n",
    "print(f\"  DSR: {dsr:.3f} {'(significant at 95%)' if dsr > 0.95 else '(NOT significant at 95%)'}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IC analysis\n",
    "ic_vals = ic_df['IC'].values\n",
    "print(f\"\\nInformation Coefficient:\")\n",
    "print(f\"  Mean IC: {np.mean(ic_vals):.4f}\")\n",
    "print(f\"  IC Std: {np.std(ic_vals):.4f}\")\n",
    "print(f\"  IC > 0: {np.mean(ic_vals > 0):.0%}\")\n",
    "print(f\"  IC t-stat: {np.mean(ic_vals)/np.std(ic_vals)*np.sqrt(len(ic_vals)):.2f}\")\n",
    "print(f\"  ICIR (ann.): {np.mean(ic_vals)/np.std(ic_vals)*np.sqrt(12):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive plots\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 14))\n",
    "\n",
    "# 1. Cumulative returns\n",
    "cum_gross = (1 + results['gross']).cumprod()\n",
    "cum_net = (1 + results['net']).cumprod()\n",
    "axes[0, 0].plot(cum_gross, label='Gross', color='steelblue')\n",
    "axes[0, 0].plot(cum_net, label='Net (10bps/side)', color='salmon')\n",
    "axes[0, 0].set_title('Cumulative Returns')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_ylabel('Growth of $1')\n",
    "\n",
    "# 2. Drawdown\n",
    "dd = cum_net / cum_net.cummax() - 1\n",
    "axes[0, 1].fill_between(dd.index, dd.values, 0, color='salmon', alpha=0.5)\n",
    "axes[0, 1].set_title('Drawdown (Net)')\n",
    "axes[0, 1].set_ylabel('Drawdown')\n",
    "\n",
    "# 3. Monthly returns\n",
    "colors = ['steelblue' if x > 0 else 'salmon' for x in results['net']]\n",
    "axes[1, 0].bar(results.index, results['net'], color=colors, width=20)\n",
    "axes[1, 0].set_title('Monthly Net Returns')\n",
    "axes[1, 0].set_ylabel('Return')\n",
    "\n",
    "# 4. Rolling IC\n",
    "rolling_ic = ic_df['IC'].rolling(12).mean()\n",
    "axes[1, 1].plot(rolling_ic, color='steelblue')\n",
    "axes[1, 1].axhline(0, color='red', linestyle='--', alpha=0.5)\n",
    "axes[1, 1].set_title('Rolling 12-Month IC')\n",
    "axes[1, 1].set_ylabel('IC')\n",
    "\n",
    "# 5. Rolling Sharpe\n",
    "rolling_sharpe = results['net'].rolling(12).mean() / results['net'].rolling(12).std() * np.sqrt(12)\n",
    "axes[2, 0].plot(rolling_sharpe, color='steelblue')\n",
    "axes[2, 0].axhline(0, color='red', linestyle='--', alpha=0.5)\n",
    "axes[2, 0].set_title('Rolling 12-Month Sharpe')\n",
    "axes[2, 0].set_ylabel('Sharpe')\n",
    "\n",
    "# 6. Long vs Short leg\n",
    "cum_long = (1 + results['long_ret']).cumprod()\n",
    "cum_short = (1 + results['short_ret']).cumprod()\n",
    "axes[2, 1].plot(cum_long, label='Long leg', color='steelblue')\n",
    "axes[2, 1].plot(cum_short, label='Short leg', color='salmon')\n",
    "axes[2, 1].set_title('Long vs Short Leg')\n",
    "axes[2, 1].legend()\n",
    "axes[2, 1].set_ylabel('Growth of $1')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to SPY buy-and-hold\n",
    "spy_cache = Path('w14_spy_cache.pkl')\n",
    "if spy_cache.exists():\n",
    "    spy = pd.read_pickle(spy_cache)\n",
    "else:\n",
    "    spy = yf.download('SPY', start='2010-01-01', end='2024-12-31')['Close']\n",
    "    spy.to_pickle(spy_cache)\n",
    "\n",
    "spy_monthly = spy.resample('M').last().pct_change()\n",
    "# Align dates\n",
    "common_dates = results.index.intersection(spy_monthly.index)\n",
    "spy_aligned = spy_monthly.loc[common_dates]\n",
    "strat_aligned = results.loc[common_dates, 'net']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.plot((1 + strat_aligned).cumprod(), label='XGBoost L/S (net)', color='steelblue')\n",
    "ax.plot((1 + spy_aligned).cumprod(), label='SPY Buy & Hold', color='gray', alpha=0.7)\n",
    "ax.set_title('Strategy vs SPY')\n",
    "ax.set_ylabel('Growth of $1')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "spy_sharpe = spy_aligned.mean() / spy_aligned.std() * np.sqrt(12)\n",
    "print(f\"SPY Sharpe: {spy_sharpe:.2f}\")\n",
    "print(f\"Strategy Sharpe: {sharpe:.2f}\")\n",
    "print(f\"Correlation with SPY: {strat_aligned.corr(spy_aligned):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Analysis & Discussion\n",
    "\n",
    "### Source of Return\n",
    "\n",
    "This strategy exploits **cross-sectional momentum** (stocks that outperformed recently tend to continue) combined with **mean-reversion at short horizons** and **volatility signals**. The economic mechanism behind momentum is debated but likely involves:\n",
    "- Investor underreaction to news (behavioral)\n",
    "- Gradual information diffusion across investor types\n",
    "- Risk compensation for momentum crashes\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- **Regime dependence:** Momentum crashes (e.g., 2009 reversal, 2020 COVID snap-back) can cause severe drawdowns\n",
    "- **Crowding:** Momentum is a well-known factor — as more capital chases it, alpha decays\n",
    "- **Survivorship bias:** Our universe (current S&P 500 members) contains survivorship bias. Stocks in our universe that were small in 2012 are there because they succeeded\n",
    "- **Transaction costs:** We assumed 10 bps/side, which is reasonable for liquid large-caps but may underestimate costs for smaller names or during volatile periods\n",
    "\n",
    "### Overfitting Risk\n",
    "\n",
    "- We tried ~3 feature configurations during development → DSR accounts for this\n",
    "- Conservative XGBoost parameters (max_depth=4, subsample=0.7, strong regularization) reduce in-sample overfitting\n",
    "- Walk-forward methodology prevents look-ahead bias\n",
    "- The rolling Sharpe plot reveals whether performance is stable or deteriorating\n",
    "\n",
    "### Capacity\n",
    "\n",
    "- Universe of ~80 large-cap stocks → each quintile has ~16 stocks\n",
    "- With equal weighting, each position is ~6% of portfolio\n",
    "- For $100M AUM: ~$6M per position → easily executable for liquid large-caps\n",
    "- Capacity likely $500M-$1B before market impact becomes significant\n",
    "\n",
    "### Improvements\n",
    "\n",
    "- **Larger universe:** 200+ stocks for better diversification and more stable quintiles\n",
    "- **More features:** Sentiment (FinBERT embeddings from Week 10), fundamental data\n",
    "- **Ensemble:** Combine XGBoost with Ridge and LightGBM (Week 5 showed ensembles help)\n",
    "- **Meta-labeling:** Use primary model → meta-model filter (Week 6) to improve precision\n",
    "- **Dynamic position sizing:** Scale positions by model confidence or volatility regime"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}