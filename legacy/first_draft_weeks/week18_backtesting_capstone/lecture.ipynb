{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 14 — Backtesting, Evaluation & Causal Inference\n",
    "\n",
    "**Course:** ML for Quantitative Finance  \n",
    "**Type:** Lecture (90 min)\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "A bad model with honest backtesting is more valuable than a great model with flawed backtesting.  \n",
    "Most published strategies fail out-of-sample because their backtests are overfitted.  \n",
    "This lecture teaches you to evaluate strategies the way real quant firms do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Anatomy of a Flawed Backtest\n",
    "\n",
    "Common sources of backtest overfitting:\n",
    "\n",
    "1. **Look-ahead bias:** Using future information (even subtly, e.g., survivorship-biased universes)\n",
    "2. **Multiple testing:** Trying 1000 strategies and reporting the best one\n",
    "3. **Unrealistic assumptions:** Zero transaction costs, unlimited liquidity, instant execution\n",
    "4. **In-sample selection:** Choosing the \"best\" parameters using the same data you evaluate on\n",
    "\n",
    "**Key insight (Lopez de Prado):** A Sharpe ratio of 2.0 from trying 100 strategies is worth less than a Sharpe of 0.8 from a single hypothesis-driven strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: How easy it is to find a \"great\" strategy by chance\n",
    "np.random.seed(42)\n",
    "n_days = 2520  # 10 years\n",
    "n_trials = 1000\n",
    "\n",
    "# Simulate pure noise returns\n",
    "market_returns = np.random.normal(0.0003, 0.01, n_days)  # slight drift\n",
    "\n",
    "# Try random strategies\n",
    "best_sharpe = -np.inf\n",
    "sharpes = []\n",
    "for _ in range(n_trials):\n",
    "    # Random signal: go long/short based on random MA crossover params\n",
    "    fast = np.random.randint(2, 20)\n",
    "    slow = np.random.randint(20, 100)\n",
    "    prices = (1 + market_returns).cumprod()\n",
    "    signal = (pd.Series(prices).rolling(fast).mean() > pd.Series(prices).rolling(slow).mean()).astype(float) * 2 - 1\n",
    "    strat_returns = signal.shift(1).values * market_returns\n",
    "    strat_returns = strat_returns[~np.isnan(strat_returns)]\n",
    "    sr = np.mean(strat_returns) / np.std(strat_returns) * np.sqrt(252)\n",
    "    sharpes.append(sr)\n",
    "    if sr > best_sharpe:\n",
    "        best_sharpe = sr\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].hist(sharpes, bins=50, color='steelblue', edgecolor='white')\n",
    "axes[0].axvline(best_sharpe, color='red', linestyle='--', label=f'Best: {best_sharpe:.2f}')\n",
    "axes[0].set_title(f'Sharpe Ratios from {n_trials} Random Strategies on Noise')\n",
    "axes[0].set_xlabel('Sharpe Ratio')\n",
    "axes[0].legend()\n",
    "\n",
    "# Expected max Sharpe under null\n",
    "trials_range = np.arange(1, 501)\n",
    "expected_max = stats.norm.ppf(1 - 1/trials_range)  # approx E[max(Z_1,...,Z_n)]\n",
    "axes[1].plot(trials_range, expected_max, color='steelblue')\n",
    "axes[1].set_xlabel('Number of Strategies Tried')\n",
    "axes[1].set_ylabel('Expected Best Sharpe (under null)')\n",
    "axes[1].set_title('Multiple Testing: More Tries → Higher \"Best\" Sharpe')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"With {n_trials} random strategies on pure noise, best Sharpe = {best_sharpe:.2f}\")\n",
    "print(f\"This is statistically meaningless — it's selection bias.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Deflated Sharpe Ratio\n",
    "\n",
    "Bailey & Lopez de Prado (2014): Adjust the Sharpe ratio for:\n",
    "- Number of strategies tried ($N$)\n",
    "- Skewness and kurtosis of returns\n",
    "- Length of the backtest\n",
    "\n",
    "$$\\text{DSR} = P\\left[\\hat{SR} > SR_0 \\;\\middle|\\; \\hat{\\gamma}_3, \\hat{\\gamma}_4, T, N\\right]$$\n",
    "\n",
    "where $SR_0$ is the expected max Sharpe under the null (depends on $N$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deflated_sharpe_ratio(sharpe_obs, n_trials, T, skew=0, kurtosis=3):\n",
    "    \"\"\"Probability that observed Sharpe is genuine (not from multiple testing).\n",
    "    \n",
    "    Args:\n",
    "        sharpe_obs: Observed annualized Sharpe ratio\n",
    "        n_trials: Number of strategies tried\n",
    "        T: Number of return observations\n",
    "        skew: Skewness of returns\n",
    "        kurtosis: Kurtosis of returns (3 = normal)\n",
    "    \n",
    "    Returns:\n",
    "        DSR probability (0 to 1)\n",
    "    \"\"\"\n",
    "    # Expected max Sharpe under null (Euler-Mascheroni approximation)\n",
    "    euler_mascheroni = 0.5772\n",
    "    sr0 = np.sqrt(2 * np.log(n_trials)) - (np.log(np.pi) + euler_mascheroni) / (2 * np.sqrt(2 * np.log(n_trials)))\n",
    "    \n",
    "    # Standard error of Sharpe ratio (Lo, 2002)\n",
    "    se_sr = np.sqrt((1 + 0.5 * sharpe_obs**2 - skew * sharpe_obs + \n",
    "                     (kurtosis - 3) / 4 * sharpe_obs**2) / T)\n",
    "    \n",
    "    # PSR: probability that true SR > sr0\n",
    "    z = (sharpe_obs - sr0) / se_sr\n",
    "    dsr = stats.norm.cdf(z)\n",
    "    return dsr\n",
    "\n",
    "\n",
    "# Example: A strategy with Sharpe 1.5\n",
    "print(\"Deflated Sharpe Ratio Analysis:\")\n",
    "print(\"=\"*50)\n",
    "for n in [1, 5, 10, 50, 100, 500]:\n",
    "    dsr = deflated_sharpe_ratio(sharpe_obs=1.5, n_trials=n, T=2520)\n",
    "    print(f\"  Tried {n:>3} strategies: DSR = {dsr:.3f} {'✓' if dsr > 0.95 else '✗'}\")\n",
    "\n",
    "print(\"\\n→ The same Sharpe of 1.5 is convincing if you tried 1 strategy,\")\n",
    "print(\"  but meaningless if you tried 500.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize DSR surface\n",
    "sharpes_grid = np.linspace(0.5, 3.0, 50)\n",
    "trials_grid = [1, 5, 10, 50, 100, 500]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for n in trials_grid:\n",
    "    dsrs = [deflated_sharpe_ratio(sr, n, T=2520) for sr in sharpes_grid]\n",
    "    ax.plot(sharpes_grid, dsrs, label=f'N={n} trials')\n",
    "\n",
    "ax.axhline(0.95, color='red', linestyle='--', alpha=0.5, label='95% threshold')\n",
    "ax.set_xlabel('Observed Sharpe Ratio')\n",
    "ax.set_ylabel('Deflated Sharpe Ratio (probability)')\n",
    "ax.set_title('Deflated Sharpe Ratio vs. Number of Trials')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Walk-Forward Optimization\n",
    "\n",
    "The only honest way to backtest:\n",
    "\n",
    "```\n",
    "For each month t:\n",
    "  1. Train model on data up to month t-1 (expanding or rolling window)\n",
    "  2. Generate predictions for month t\n",
    "  3. Form portfolio based on predictions\n",
    "  4. Record actual returns for month t\n",
    "  5. Move to month t+1\n",
    "```\n",
    "\n",
    "**Critical:** Never retrain using any information from the test period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for walk-forward demo\n",
    "TICKERS = [\n",
    "    'AAPL', 'MSFT', 'AMZN', 'GOOGL', 'META', 'NVDA', 'JPM', 'JNJ', 'V', 'PG',\n",
    "    'UNH', 'HD', 'MA', 'DIS', 'BAC', 'XOM', 'CSCO', 'PFE', 'COST', 'ABT',\n",
    "    'PEP', 'AVGO', 'CRM', 'NKE', 'CVX', 'WMT', 'MRK', 'LLY', 'ABBV', 'INTC',\n",
    "    'T', 'VZ', 'QCOM', 'TXN', 'PM', 'UNP', 'NEE', 'LOW', 'BMY', 'AMGN',\n",
    "]\n",
    "\n",
    "cache_path = Path('w14_data_cache.pkl')\n",
    "if cache_path.exists():\n",
    "    raw = pd.read_pickle(cache_path)\n",
    "else:\n",
    "    raw = yf.download(TICKERS, start='2010-01-01', end='2024-12-31', progress=True)\n",
    "    raw.to_pickle(cache_path)\n",
    "\n",
    "prices = raw['Close'].ffill().dropna(axis=1, thresh=int(0.8 * len(raw)))\n",
    "returns_daily = prices.pct_change()\n",
    "monthly_prices = prices.resample('M').last()\n",
    "monthly_returns = monthly_prices.pct_change()\n",
    "\n",
    "# Features\n",
    "features = {}\n",
    "features['mom_1m'] = monthly_prices.pct_change(1)\n",
    "features['mom_3m'] = monthly_prices.pct_change(3)\n",
    "features['mom_6m'] = monthly_prices.pct_change(6)\n",
    "features['mom_12m_skip1'] = monthly_prices.pct_change(12).shift(1)\n",
    "features['vol_20d'] = returns_daily.rolling(20).std().resample('M').last()\n",
    "features['vol_60d'] = returns_daily.rolling(60).std().resample('M').last()\n",
    "target = monthly_returns.shift(-1)\n",
    "\n",
    "print(f\"Universe: {prices.shape[1]} stocks, {len(monthly_prices)} months\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk-forward backtest\n",
    "pred_start = pd.Timestamp('2016-01-31')\n",
    "months = sorted(monthly_prices.index)\n",
    "pred_months = [m for m in months if m >= pred_start and m in target.index]\n",
    "\n",
    "portfolio_returns = []\n",
    "\n",
    "for month in pred_months:\n",
    "    # Build cross-section for this month\n",
    "    train_months = [m for m in months if m < month and m >= pd.Timestamp('2012-01-01')]\n",
    "    if len(train_months) < 24:\n",
    "        continue\n",
    "\n",
    "    # Assemble training data\n",
    "    X_train, y_train = [], []\n",
    "    for tm in train_months:\n",
    "        X_cs = pd.DataFrame({n: f.loc[tm] for n, f in features.items() if tm in f.index})\n",
    "        y_cs = target.loc[tm] if tm in target.index else pd.Series(dtype=float)\n",
    "        valid = X_cs.dropna().index.intersection(y_cs.dropna().index)\n",
    "        if len(valid) > 5:\n",
    "            X_train.append(X_cs.loc[valid])\n",
    "            y_train.append(y_cs.loc[valid])\n",
    "\n",
    "    if not X_train:\n",
    "        continue\n",
    "\n",
    "    X_tr = pd.concat(X_train)\n",
    "    y_tr = pd.concat(y_train)\n",
    "\n",
    "    # Test data for this month\n",
    "    X_te = pd.DataFrame({n: f.loc[month] for n, f in features.items() if month in f.index})\n",
    "    y_te = target.loc[month] if month in target.index else pd.Series(dtype=float)\n",
    "    valid_te = X_te.dropna().index.intersection(y_te.dropna().index)\n",
    "    if len(valid_te) < 5:\n",
    "        continue\n",
    "\n",
    "    # Train and predict\n",
    "    model = RandomForestRegressor(n_estimators=100, max_depth=4, n_jobs=-1, random_state=42)\n",
    "    model.fit(X_tr.values, y_tr.values)\n",
    "    pred = pd.Series(model.predict(X_te.loc[valid_te].values), index=valid_te)\n",
    "\n",
    "    # Long-short portfolio: top quintile long, bottom quintile short\n",
    "    n_stocks = len(pred) // 5\n",
    "    if n_stocks < 2:\n",
    "        continue\n",
    "    longs = pred.nlargest(n_stocks).index\n",
    "    shorts = pred.nsmallest(n_stocks).index\n",
    "    actual = y_te.loc[valid_te]\n",
    "    port_ret = actual.loc[longs].mean() - actual.loc[shorts].mean()\n",
    "\n",
    "    # Apply transaction costs (10 bps per side)\n",
    "    tc = 0.001  # 10 bps per side × 2 sides\n",
    "    portfolio_returns.append({'month': month, 'return': port_ret - tc})\n",
    "\n",
    "results = pd.DataFrame(portfolio_returns).set_index('month')\n",
    "print(f\"Walk-forward backtest: {len(results)} months\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "cum_returns = (1 + results['return']).cumprod()\n",
    "sharpe = results['return'].mean() / results['return'].std() * np.sqrt(12)\n",
    "max_dd = (cum_returns / cum_returns.cummax() - 1).min()\n",
    "sortino_denom = results['return'][results['return'] < 0].std()\n",
    "sortino = results['return'].mean() / sortino_denom * np.sqrt(12) if sortino_denom > 0 else np.nan\n",
    "calmar = (results['return'].mean() * 12) / abs(max_dd) if max_dd != 0 else np.nan\n",
    "\n",
    "print(\"Walk-Forward Strategy Performance:\")\n",
    "print(f\"  Annualized Return: {results['return'].mean() * 12:.1%}\")\n",
    "print(f\"  Sharpe Ratio: {sharpe:.2f}\")\n",
    "print(f\"  Sortino Ratio: {sortino:.2f}\")\n",
    "print(f\"  Calmar Ratio: {calmar:.2f}\")\n",
    "print(f\"  Max Drawdown: {max_dd:.1%}\")\n",
    "print(f\"  Hit Rate: {(results['return'] > 0).mean():.0%}\")\n",
    "\n",
    "# Deflated Sharpe\n",
    "ret_vals = results['return'].values\n",
    "dsr = deflated_sharpe_ratio(\n",
    "    sharpe_obs=sharpe, n_trials=1, T=len(ret_vals),\n",
    "    skew=stats.skew(ret_vals), kurtosis=stats.kurtosis(ret_vals, fisher=False)\n",
    ")\n",
    "print(f\"  DSR (1 trial): {dsr:.3f}\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "axes[0].plot(cum_returns, color='steelblue')\n",
    "axes[0].set_title(f'Walk-Forward Long-Short Portfolio (Sharpe={sharpe:.2f})')\n",
    "axes[0].set_ylabel('Cumulative Return')\n",
    "\n",
    "drawdown = cum_returns / cum_returns.cummax() - 1\n",
    "axes[1].fill_between(drawdown.index, drawdown.values, 0, color='salmon', alpha=0.5)\n",
    "axes[1].set_title('Drawdown')\n",
    "axes[1].set_ylabel('Drawdown')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transaction Costs and Realistic Assumptions\n",
    "\n",
    "**The cost stack:**\n",
    "- Commission: ~0-2 bps (near zero for large brokers)\n",
    "- Bid-ask spread: 2-10 bps for liquid large-caps\n",
    "- Market impact: Depends on order size / ADV\n",
    "- **Rule of thumb:** 5-10 bps per side for liquid US equities\n",
    "\n",
    "**Turnover matters:** A strategy that turns over 200% per month needs 4× the alpha of one that turns over 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impact of transaction costs\n",
    "gross_returns = results['return'] + 0.001  # add back the 10bps we subtracted\n",
    "tc_levels = [0, 5, 10, 20, 50]  # bps per side\n",
    "\n",
    "print(\"Impact of Transaction Costs (assuming 100% monthly turnover):\")\n",
    "print(\"=\"*60)\n",
    "for tc_bps in tc_levels:\n",
    "    tc = tc_bps * 2 / 10000  # both sides\n",
    "    net = gross_returns - tc\n",
    "    sr = net.mean() / net.std() * np.sqrt(12)\n",
    "    ann_ret = net.mean() * 12\n",
    "    print(f\"  {tc_bps:>2} bps/side: Sharpe = {sr:.2f}, Ann. Return = {ann_ret:.1%}\")\n",
    "\n",
    "print(\"\\n→ Many academic strategies look great at 0 costs but die at realistic costs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Causal Inference in Factor Investing\n",
    "\n",
    "**Lopez de Prado (2023): Most factors are mirages.**\n",
    "\n",
    "The standard workflow:\n",
    "1. Mine data for patterns → 2. Find correlation → 3. Publish \"factor\"\n",
    "\n",
    "The correct workflow:\n",
    "1. Economic theory → 2. Causal hypothesis → 3. Testable prediction → 4. Backtest\n",
    "\n",
    "**Why this matters:** A correlation that happens to exist in-sample will break out-of-sample.  \n",
    "A causal mechanism will persist because it reflects how markets actually work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Spurious factors\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "n_candidates = 200\n",
    "\n",
    "# Generate random \"factors\" and random returns\n",
    "returns = np.random.normal(0, 0.02, n)\n",
    "factors = np.random.normal(0, 1, (n, n_candidates))\n",
    "\n",
    "# Find the \"best\" factor by in-sample correlation\n",
    "correlations = [np.corrcoef(factors[:, i], returns)[0, 1] for i in range(n_candidates)]\n",
    "best_idx = np.argmax(np.abs(correlations))\n",
    "\n",
    "print(f\"Best 'factor' out of {n_candidates} random ones:\")\n",
    "print(f\"  In-sample correlation: {correlations[best_idx]:.4f}\")\n",
    "print(f\"  t-stat: {correlations[best_idx] * np.sqrt(n-2) / np.sqrt(1 - correlations[best_idx]**2):.2f}\")\n",
    "print(f\"  Looks significant! But it's pure noise.\")\n",
    "\n",
    "# Out-of-sample test\n",
    "oos_returns = np.random.normal(0, 0.02, n)\n",
    "oos_corr = np.corrcoef(factors[:, best_idx], oos_returns)[0, 1]\n",
    "print(f\"\\n  Out-of-sample correlation: {oos_corr:.4f}\")\n",
    "print(f\"  → The 'factor' completely fails out-of-sample.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Strategy Evaluation Metrics\n",
    "\n",
    "Beyond the Sharpe ratio — a comprehensive evaluation toolkit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_tear_sheet(returns_series, name='Strategy', rf=0.0):\n",
    "    \"\"\"Compute comprehensive strategy metrics.\"\"\"\n",
    "    r = returns_series.dropna()\n",
    "    excess = r - rf / 12  # monthly rf\n",
    "    cum = (1 + r).cumprod()\n",
    "    dd = cum / cum.cummax() - 1\n",
    "\n",
    "    metrics = {\n",
    "        'Ann. Return': r.mean() * 12,\n",
    "        'Ann. Volatility': r.std() * np.sqrt(12),\n",
    "        'Sharpe': excess.mean() / r.std() * np.sqrt(12) if r.std() > 0 else np.nan,\n",
    "        'Sortino': excess.mean() / r[r < 0].std() * np.sqrt(12) if (r < 0).sum() > 0 else np.nan,\n",
    "        'Calmar': (r.mean() * 12) / abs(dd.min()) if dd.min() != 0 else np.nan,\n",
    "        'Max Drawdown': dd.min(),\n",
    "        'Hit Rate': (r > 0).mean(),\n",
    "        'Profit Factor': abs(r[r > 0].sum() / r[r < 0].sum()) if (r < 0).sum() > 0 else np.nan,\n",
    "        'Skewness': stats.skew(r),\n",
    "        'Kurtosis': stats.kurtosis(r, fisher=False),\n",
    "        'VaR 5%': np.percentile(r, 5),\n",
    "        'CVaR 5%': r[r <= np.percentile(r, 5)].mean(),\n",
    "        'Tail Ratio': abs(np.percentile(r, 95) / np.percentile(r, 5)) if np.percentile(r, 5) != 0 else np.nan,\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\" {name} — Performance Summary\")\n",
    "    print(f\"{'='*50}\")\n",
    "    for k, v in metrics.items():\n",
    "        if isinstance(v, float):\n",
    "            if 'Rate' in k or 'Return' in k or 'Volatility' in k or 'Drawdown' in k or 'VaR' in k or 'CVaR' in k:\n",
    "                print(f\"  {k:<20s}: {v:.1%}\")\n",
    "            else:\n",
    "                print(f\"  {k:<20s}: {v:.3f}\")\n",
    "    return metrics\n",
    "\n",
    "\n",
    "metrics = full_tear_sheet(results['return'], name='Walk-Forward RF Long-Short')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. What Quant Firms Look For\n",
    "\n",
    "1. **Intellectual honesty:** Do you acknowledge limitations? Or do you oversell?\n",
    "2. **Realistic assumptions:** Transaction costs, slippage, capacity\n",
    "3. **Out-of-sample validation:** Walk-forward, not in-sample optimization\n",
    "4. **Economic intuition:** Why should this strategy work? What is the source of return?\n",
    "5. **Clean code:** Readable, reproducible, well-documented\n",
    "\n",
    "**The hierarchy:**\n",
    "```\n",
    "Theory → Hypothesis → Prediction → Data → Backtest → Evaluation\n",
    "NOT:\n",
    "Data → Mine → Find pattern → Backtest → Publish\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Multiple testing kills most backtests.** The Deflated Sharpe Ratio corrects for this.\n",
    "2. **Walk-forward is the gold standard.** Never train on test data, never look ahead.\n",
    "3. **Transaction costs are real.** A strategy with Sharpe 2.0 at 0 costs may have Sharpe 0.3 at realistic costs.\n",
    "4. **Correlation ≠ causation.** Data-mined factors fail out-of-sample. Start from economic theory.\n",
    "5. **Honesty > performance.** A Sharpe of 0.8 with honest methodology beats a Sharpe of 3.0 from overfitting.\n",
    "\n",
    "**The capstone project should demonstrate all of these principles.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}