{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10 -- From FinBERT to LLM Embeddings\n",
    "\n",
    "**Key question:** How has NLP for finance evolved from bag-of-words to LLM embeddings, and what does the frontier look like today?\n",
    "\n",
    "---\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. Evolution of NLP in finance: three eras\n",
    "2. FinBERT as baseline: sentiment analysis\n",
    "3. The LLM embedding revolution (Chen-Kelly-Xiu 2023)\n",
    "4. Sentence-transformers for financial text embedding\n",
    "5. Using API-based embeddings (OpenAI, Anthropic)\n",
    "6. Agentic AI: AlphaGPT, Qlib RD-Agent\n",
    "7. Demo: FinBERT sentiment + sentence-transformer embeddings\n",
    "8. Key papers and references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "\n",
    "print('Imports ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Evolution of NLP in Finance: Three Eras\n",
    "\n",
    "### Era 1: Bag-of-Words and Dictionaries (2000-2018)\n",
    "- **Loughran-McDonald dictionary** (2011): a curated list of positive/negative financial words\n",
    "- Count word frequencies, compute sentiment scores\n",
    "- Simple, interpretable, but misses context (\"not profitable\" = positive + negative?)\n",
    "\n",
    "### Era 2: FinBERT and Fine-Tuned Transformers (2019-2022)\n",
    "- **FinBERT** (Araci 2019, Huang et al. 2020): BERT fine-tuned on financial text\n",
    "- Understands context: \"not profitable\" is correctly negative\n",
    "- Classification: positive / negative / neutral\n",
    "- Limitation: fixed categories, no rich representation\n",
    "\n",
    "### Era 3: LLM Embeddings and Agentic AI (2023+)\n",
    "- **Chen-Kelly-Xiu (2023):** showed that LLM embeddings of news text predict stock returns better than any previous NLP method\n",
    "- **Key insight:** do not classify text into sentiment -- instead, embed it into a high-dimensional vector and let the model learn what matters\n",
    "- **Agentic AI (2024+):** systems like Man Group's AlphaGPT and Microsoft's Qlib RD-Agent that autonomously generate and test alpha hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the three eras\n",
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "\n",
    "eras = [\n",
    "    ('Bag-of-Words\\n(Loughran-McDonald)', 2000, 2018, '#e74c3c'),\n",
    "    ('FinBERT\\n(Fine-tuned BERT)', 2019, 2022, '#f39c12'),\n",
    "    ('LLM Embeddings\\n+ Agentic AI', 2023, 2026, '#27ae60'),\n",
    "]\n",
    "\n",
    "for i, (name, start, end, color) in enumerate(eras):\n",
    "    ax.barh(0, end - start, left=start, height=0.5, color=color, alpha=0.8, edgecolor='white')\n",
    "    ax.text((start + end) / 2, 0, name, ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Key milestones\n",
    "milestones = [\n",
    "    (2011, 'L-M Dictionary'),\n",
    "    (2019, 'FinBERT'),\n",
    "    (2023, 'Chen-Kelly-Xiu'),\n",
    "    (2024, 'AlphaGPT'),\n",
    "]\n",
    "for year, label in milestones:\n",
    "    ax.annotate(label, xy=(year, -0.35), fontsize=8, ha='center',\n",
    "                arrowprops=dict(arrowstyle='->', color='black'), xytext=(year, -0.6))\n",
    "\n",
    "ax.set_xlim(1998, 2027)\n",
    "ax.set_ylim(-0.8, 0.5)\n",
    "ax.set_yticks([])\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_title('Three Eras of NLP in Finance', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. FinBERT as Baseline\n",
    "\n",
    "**FinBERT** is a BERT model fine-tuned on ~50,000 analyst reports and financial news articles.\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Input text: \"Apple reports record quarterly revenue\"\n",
    "     --> BERT tokenizer (subword tokens)\n",
    "     --> 12-layer transformer encoder\n",
    "     --> [CLS] token pooling\n",
    "     --> Linear classification head\n",
    "     --> Output: {positive: 0.92, negative: 0.03, neutral: 0.05}\n",
    "```\n",
    "\n",
    "**Strengths:**\n",
    "- Understands financial context\n",
    "- Pre-trained, easy to use\n",
    "- Good for sentiment classification tasks\n",
    "\n",
    "**Limitations:**\n",
    "- Fixed to 3 classes (positive/negative/neutral)\n",
    "- Does not capture magnitude or nuance\n",
    "- 110M params = limited capacity\n",
    "- \"Earnings beat expectations\" and \"earnings massively beat expectations\" get similar scores\n",
    "\n",
    "**Popular variants:**\n",
    "- `ProsusAI/finbert` (most commonly used)\n",
    "- `yiyanghkust/finbert-tone`\n",
    "- `ahmedrachid/FinancialBERT-Sentiment-Analysis`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. The LLM Embedding Revolution\n",
    "\n",
    "### Chen-Kelly-Xiu (2023): \"Expected Returns and Large Language Models\"\n",
    "\n",
    "**Key finding:** Instead of classifying text into sentiment categories, embed the entire text into a continuous vector space, then use the embedding directly for return prediction.\n",
    "\n",
    "**Why this works better:**\n",
    "\n",
    "| Approach | Information Retained |\n",
    "|----------|--------------------|\n",
    "| Bag-of-words | Word frequencies |\n",
    "| FinBERT sentiment | 3 numbers (P/N/N probabilities) |\n",
    "| LLM embedding | 768-4096 dimensional vector |\n",
    "\n",
    "**The math:**\n",
    "```\n",
    "r_{i,t+1} = f(E(text_{i,t})) + epsilon\n",
    "\n",
    "where:\n",
    "  r_{i,t+1}    = next-period return for stock i\n",
    "  E(text_{i,t}) = LLM embedding of news about stock i at time t\n",
    "  f(.)          = a learned function (neural net or XGBoost)\n",
    "```\n",
    "\n",
    "**Results (Chen-Kelly-Xiu 2023):**\n",
    "- LLM embeddings achieve OOS R-squared of ~2% for monthly returns\n",
    "- Outperforms FinBERT sentiment, Loughran-McDonald, and all prior NLP methods\n",
    "- The alpha is distinct from known risk factors (Fama-French, momentum, etc.)\n",
    "\n",
    "**Why?** The embedding captures *nuance* that sentiment scores miss:\n",
    "- Magnitude: \"slight miss\" vs. \"massive miss\"\n",
    "- Context: \"revenue grew despite macro headwinds\" (resilience signal)\n",
    "- Forward-looking language: \"management raised guidance\" (future expectation)\n",
    "- Cross-referencing: implicit comparisons to competitors or industry trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustration: Information loss from sentiment vs. embeddings\n",
    "\n",
    "headlines = [\n",
    "    \"Apple reports slight miss on iPhone revenue\",\n",
    "    \"Apple reports massive miss on iPhone revenue, shares plunge 10%\",\n",
    "    \"Apple beats estimates with record services revenue\",\n",
    "    \"Apple slightly edges past lowered expectations\",\n",
    "    \"Apple's iPhone sales decline amid weak China demand\",\n",
    "]\n",
    "\n",
    "# Simulated FinBERT sentiment (3 numbers per headline)\n",
    "finbert_scores = [\n",
    "    [0.15, 0.70, 0.15],  # negative\n",
    "    [0.05, 0.90, 0.05],  # very negative\n",
    "    [0.85, 0.05, 0.10],  # positive\n",
    "    [0.40, 0.20, 0.40],  # neutral-ish positive\n",
    "    [0.10, 0.75, 0.15],  # negative\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# FinBERT: only 3 numbers\n",
    "labels_short = [h[:40] + '...' for h in headlines]\n",
    "fb_df = pd.DataFrame(finbert_scores, columns=['Positive', 'Negative', 'Neutral'],\n",
    "                     index=labels_short)\n",
    "fb_df.plot(kind='barh', stacked=True, ax=axes[0],\n",
    "           color=['#27ae60', '#e74c3c', '#95a5a6'], edgecolor='white')\n",
    "axes[0].set_title('FinBERT: 3 Numbers Per Headline')\n",
    "axes[0].set_xlabel('Probability')\n",
    "axes[0].legend(loc='lower right')\n",
    "\n",
    "# LLM Embedding: 768-dim heatmap (simulated)\n",
    "np.random.seed(42)\n",
    "fake_embeddings = np.random.randn(5, 50) * 0.5  # show 50 of 768 dims\n",
    "# Make embeddings reflect semantic similarity\n",
    "fake_embeddings[1] = fake_embeddings[0] + np.random.randn(50) * 0.2  # similar to 0\n",
    "fake_embeddings[3] = fake_embeddings[2] + np.random.randn(50) * 0.3  # similar to 2\n",
    "\n",
    "im = axes[1].imshow(fake_embeddings, aspect='auto', cmap='RdBu_r', vmin=-1.5, vmax=1.5)\n",
    "axes[1].set_yticks(range(5))\n",
    "axes[1].set_yticklabels(labels_short, fontsize=8)\n",
    "axes[1].set_xlabel('Embedding Dimension (showing 50 of 768)')\n",
    "axes[1].set_title('LLM Embedding: 768 Numbers Per Headline')\n",
    "plt.colorbar(im, ax=axes[1], shrink=0.8)\n",
    "\n",
    "plt.suptitle('Information Content: Sentiment vs. Embedding', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Key point: Headlines 1 and 2 look similar under FinBERT (both \"negative\"),')\n",
    "print('but the embedding captures the MAGNITUDE difference (slight vs. massive miss).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Sentence-Transformers for Financial Text Embedding\n",
    "\n",
    "**Sentence-transformers** (Reimers & Gurevych 2019) are BERT-like models optimized to produce meaningful sentence-level embeddings.\n",
    "\n",
    "**Why use sentence-transformers instead of raw BERT?**\n",
    "- BERT is designed for token-level tasks (NER, question answering)\n",
    "- Its [CLS] token is not a good sentence embedding out of the box\n",
    "- Sentence-transformers are trained with contrastive learning to make semantically similar sentences have similar embeddings\n",
    "\n",
    "**Popular models:**\n",
    "\n",
    "| Model | Dim | Speed | Quality |\n",
    "|-------|-----|-------|---------|\n",
    "| `all-MiniLM-L6-v2` | 384 | Fast | Good |\n",
    "| `all-mpnet-base-v2` | 768 | Medium | Better |\n",
    "| `e5-large-v2` | 1024 | Slow | Best open-source |\n",
    "\n",
    "**Usage:**\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(['Apple beats earnings', 'Google misses revenue'])\n",
    "# embeddings.shape = (2, 384)\n",
    "```\n",
    "\n",
    "**For finance:** You can use these embeddings directly as features for return prediction, or fine-tune on financial text pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. API-Based Embeddings\n",
    "\n",
    "### OpenAI Embeddings\n",
    "```python\n",
    "import openai\n",
    "response = openai.Embedding.create(\n",
    "    input='Apple reports record revenue',\n",
    "    model='text-embedding-3-small'  # 1536-dim\n",
    ")\n",
    "embedding = response['data'][0]['embedding']  # list of 1536 floats\n",
    "```\n",
    "\n",
    "### Anthropic (via Messages API)\n",
    "- Anthropic does not offer a dedicated embedding endpoint (as of early 2026)\n",
    "- Workaround: use Claude to extract structured features from text (not raw embeddings)\n",
    "- Or: use sentence-transformers locally for embeddings, Claude for text understanding tasks\n",
    "\n",
    "### Cost Considerations\n",
    "\n",
    "| Method | Cost per 1M tokens | Dimensions | Latency |\n",
    "|--------|-------------------|------------|----------|\n",
    "| OpenAI text-embedding-3-small | ~$0.02 | 1536 | ~100ms |\n",
    "| OpenAI text-embedding-3-large | ~$0.13 | 3072 | ~200ms |\n",
    "| Sentence-transformers (local) | $0 (compute only) | 384-1024 | ~10ms |\n",
    "\n",
    "For a quant fund processing 100K news articles per day, API costs add up. Local models are often preferred for production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Agentic AI for Quant Finance\n",
    "\n",
    "The frontier is moving beyond embeddings toward *agentic systems* that autonomously generate and test alpha hypotheses.\n",
    "\n",
    "### Man Group -- AlphaGPT (2024)\n",
    "- **Paper:** \"Can Large Language Models Beat Wall Street?\" (Man Group, 2024)\n",
    "- An LLM agent that:\n",
    "  1. Reads financial research papers and news\n",
    "  2. Proposes alpha factor hypotheses\n",
    "  3. Writes code to compute the factors\n",
    "  4. Backtests them\n",
    "  5. Iterates based on results\n",
    "- Human quant researchers supervise and curate the output\n",
    "- **Key result:** Some LLM-generated factors showed genuine alpha in out-of-sample testing\n",
    "\n",
    "### Microsoft Qlib -- RD-Agent (2024)\n",
    "- Open-source agent built on Microsoft's Qlib framework\n",
    "- Autonomous research-and-development loop:\n",
    "  1. Generate factor hypothesis\n",
    "  2. Implement in code\n",
    "  3. Run backtest\n",
    "  4. Analyze results\n",
    "  5. Refine hypothesis\n",
    "- **GitHub:** [microsoft/RD-Agent](https://github.com/microsoft/RD-Agent)\n",
    "\n",
    "### What this means\n",
    "\n",
    "- NLP is no longer just about *reading* text -- it is about *reasoning* with it\n",
    "- LLMs are becoming research tools, not just data processing tools\n",
    "- The quant researcher's role shifts from feature engineering to supervising AI agents\n",
    "- But: the agents still need human oversight, domain knowledge, and risk management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Demo: FinBERT Sentiment + Sentence-Transformer Embeddings\n",
    "\n",
    "We will:\n",
    "1. Run FinBERT on sample financial headlines\n",
    "2. Generate sentence-transformer embeddings\n",
    "3. Visualize the embedding space\n",
    "4. Compare the information content\n",
    "\n",
    "**Setup:**\n",
    "```bash\n",
    "pip install transformers sentence-transformers torch scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample financial headlines\n",
    "headlines = [\n",
    "    # Positive\n",
    "    \"Apple reports record quarterly revenue, beats analyst expectations\",\n",
    "    \"NVIDIA surges 15% on strong AI chip demand, raises guidance\",\n",
    "    \"JPMorgan posts best quarter in history, increases dividend\",\n",
    "    \"Tesla deliveries exceed estimates, stock rallies on optimism\",\n",
    "    # Negative\n",
    "    \"Meta shares plunge after disappointing ad revenue guidance\",\n",
    "    \"Boeing reports massive loss, faces regulatory scrutiny\",\n",
    "    \"Bank of America misses earnings, warns of rising credit losses\",\n",
    "    \"Pfizer cuts full-year forecast amid declining vaccine demand\",\n",
    "    # Neutral / Mixed\n",
    "    \"Microsoft reports in-line results, cloud growth moderates\",\n",
    "    \"Google faces antitrust ruling, but ad revenue holds steady\",\n",
    "    \"Amazon's AWS growth slows but retail margins improve\",\n",
    "    \"Fed holds rates steady, signals possible cut in September\",\n",
    "]\n",
    "\n",
    "labels = ['Positive'] * 4 + ['Negative'] * 4 + ['Mixed'] * 4\n",
    "print(f'{len(headlines)} headlines loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FinBERT sentiment analysis\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "    import torch\n",
    "\n",
    "    finbert_model_name = 'ProsusAI/finbert'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(finbert_model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(finbert_model_name)\n",
    "    model.eval()\n",
    "\n",
    "    finbert_results = []\n",
    "    for headline in headlines:\n",
    "        inputs = tokenizer(headline, return_tensors='pt', truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=1).squeeze().numpy()\n",
    "        finbert_results.append({\n",
    "            'headline': headline[:60] + '...' if len(headline) > 60 else headline,\n",
    "            'positive': probs[0],\n",
    "            'negative': probs[1],\n",
    "            'neutral': probs[2],\n",
    "            'sentiment': ['positive', 'negative', 'neutral'][np.argmax(probs)],\n",
    "        })\n",
    "    finbert_available = True\n",
    "    print('FinBERT analysis complete.')\n",
    "\n",
    "except ImportError:\n",
    "    print('transformers not available. Using simulated FinBERT results.')\n",
    "    np.random.seed(42)\n",
    "    finbert_results = []\n",
    "    sentiment_map = {'Positive': [0.8, 0.1, 0.1], 'Negative': [0.1, 0.8, 0.1], 'Mixed': [0.3, 0.3, 0.4]}\n",
    "    for headline, label in zip(headlines, labels):\n",
    "        base = sentiment_map[label]\n",
    "        noise = np.random.dirichlet([10, 10, 10]) * 0.1\n",
    "        probs = np.array(base) + noise\n",
    "        probs /= probs.sum()\n",
    "        finbert_results.append({\n",
    "            'headline': headline[:60] + '...' if len(headline) > 60 else headline,\n",
    "            'positive': probs[0],\n",
    "            'negative': probs[1],\n",
    "            'neutral': probs[2],\n",
    "            'sentiment': ['positive', 'negative', 'neutral'][np.argmax(probs)],\n",
    "        })\n",
    "    finbert_available = False\n",
    "\n",
    "fb_df = pd.DataFrame(finbert_results)\n",
    "print(fb_df[['headline', 'sentiment', 'positive', 'negative', 'neutral']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence-transformer embeddings\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "    st_model = SentenceTransformer('all-MiniLM-L6-v2')  # 384-dim, fast\n",
    "    embeddings = st_model.encode(headlines)\n",
    "    st_available = True\n",
    "    print(f'Embeddings shape: {embeddings.shape}')\n",
    "\n",
    "except ImportError:\n",
    "    print('sentence-transformers not available. Using simulated embeddings.')\n",
    "    np.random.seed(42)\n",
    "    embeddings = np.random.randn(len(headlines), 384)\n",
    "    # Make similar headlines have similar embeddings\n",
    "    for i in range(4):\n",
    "        embeddings[i] += 1.0  # positive cluster\n",
    "    for i in range(4, 8):\n",
    "        embeddings[i] -= 1.0  # negative cluster\n",
    "    st_available = False\n",
    "    print(f'Simulated embeddings shape: {embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embedding space with t-SNE / PCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# PCA to 2D\n",
    "pca = PCA(n_components=2)\n",
    "emb_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter plot\n",
    "color_map = {'Positive': '#27ae60', 'Negative': '#e74c3c', 'Mixed': '#f39c12'}\n",
    "for label in ['Positive', 'Negative', 'Mixed']:\n",
    "    mask = [l == label for l in labels]\n",
    "    axes[0].scatter(emb_2d[mask, 0], emb_2d[mask, 1], c=color_map[label],\n",
    "                    label=label, s=80, edgecolors='white', zorder=3)\n",
    "\n",
    "# Annotate points\n",
    "for i, headline in enumerate(headlines):\n",
    "    short = headline.split(',')[0][:30]\n",
    "    axes[0].annotate(short, (emb_2d[i, 0], emb_2d[i, 1]), fontsize=7, alpha=0.7)\n",
    "\n",
    "axes[0].set_title('Embedding Space (PCA 2D Projection)')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel('PC1')\n",
    "axes[0].set_ylabel('PC2')\n",
    "\n",
    "# Cosine similarity heatmap\n",
    "cos_sim = cosine_similarity(embeddings)\n",
    "short_labels = [h[:30] + '...' for h in headlines]\n",
    "im = axes[1].imshow(cos_sim, cmap='RdBu_r', vmin=-0.2, vmax=1.0)\n",
    "axes[1].set_xticks(range(len(headlines)))\n",
    "axes[1].set_yticks(range(len(headlines)))\n",
    "axes[1].set_xticklabels(range(len(headlines)), fontsize=8)\n",
    "axes[1].set_yticklabels(range(len(headlines)), fontsize=8)\n",
    "axes[1].set_title('Pairwise Cosine Similarity')\n",
    "plt.colorbar(im, ax=axes[1], shrink=0.8)\n",
    "\n",
    "plt.suptitle('Sentence-Transformer Embeddings of Financial Headlines', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Headline index:')\n",
    "for i, h in enumerate(headlines):\n",
    "    print(f'  {i}: {h[:65]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare information content: FinBERT (3 dims) vs. embedding (384 dims)\n",
    "\n",
    "# FinBERT similarity (based on 3 sentiment scores)\n",
    "fb_vectors = np.array([[r['positive'], r['negative'], r['neutral']] for r in finbert_results])\n",
    "fb_sim = cosine_similarity(fb_vectors)\n",
    "\n",
    "# Embedding similarity\n",
    "emb_sim = cosine_similarity(embeddings)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "im1 = axes[0].imshow(fb_sim, cmap='RdBu_r', vmin=-0.2, vmax=1.0)\n",
    "axes[0].set_title('FinBERT Similarity (3 dims)')\n",
    "plt.colorbar(im1, ax=axes[0], shrink=0.8)\n",
    "\n",
    "im2 = axes[1].imshow(emb_sim, cmap='RdBu_r', vmin=-0.2, vmax=1.0)\n",
    "axes[1].set_title('Embedding Similarity (384 dims)')\n",
    "plt.colorbar(im2, ax=axes[1], shrink=0.8)\n",
    "\n",
    "plt.suptitle('Richer Representations Capture More Nuance', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Notice: FinBERT groups ALL negative headlines as similar.')\n",
    "print('Embeddings distinguish between \"Boeing loss\" and \"Pfizer forecast cut\" -- different stocks, different stories.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Key Papers and References\n",
    "\n",
    "### NLP for Finance -- Foundational\n",
    "- Loughran & McDonald, \"When is a Liability Not a Liability?\" (Journal of Finance, 2011). The financial sentiment dictionary.\n",
    "- Araci, \"FinBERT: Financial Sentiment Analysis with Pre-trained Language Models\" (2019). [arXiv:1908.10063](https://arxiv.org/abs/1908.10063)\n",
    "- Huang, Wang, Yang, \"FinBERT: A Large Language Model for Extracting Information from Financial Text\" (2020).\n",
    "\n",
    "### LLM Embeddings for Return Prediction\n",
    "- Chen, Kelly, Xiu, \"Expected Returns and Large Language Models\" (2023). The landmark paper showing LLM embeddings outperform traditional NLP for return prediction. [SSRN](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4416687)\n",
    "- Reimers & Gurevych, \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\" (2019). [arXiv:1908.10084](https://arxiv.org/abs/1908.10084)\n",
    "\n",
    "### Agentic AI\n",
    "- Man Group, \"AlphaGPT: Can Large Language Models Beat Wall Street?\" (2024). LLM agents for autonomous alpha research.\n",
    "- Microsoft Research, \"RD-Agent: An Autonomous Research-Development Agent for Factor Discovery\" (2024). [GitHub](https://github.com/microsoft/RD-Agent)\n",
    "\n",
    "### Practical Resources\n",
    "- Sentence-Transformers library: [sbert.net](https://www.sbert.net/)\n",
    "- HuggingFace FinBERT: [ProsusAI/finbert](https://huggingface.co/ProsusAI/finbert)\n",
    "- OpenAI Embedding API: [platform.openai.com](https://platform.openai.com/docs/guides/embeddings)\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "1. NLP for finance has evolved from word counting (2000s) to contextual understanding (FinBERT, 2019) to rich embeddings (2023+)\n",
    "2. LLM embeddings capture far more information than sentiment classification (384-4096 dims vs. 3 classes)\n",
    "3. Chen-Kelly-Xiu (2023) showed that embeddings outperform all prior NLP methods for return prediction\n",
    "4. Sentence-transformers provide a practical, free, local option for embedding financial text\n",
    "5. Agentic AI (AlphaGPT, RD-Agent) represents the next frontier: LLMs as autonomous quant researchers\n",
    "6. The quant researcher's role is shifting from feature engineering to supervising AI systems"
   ]
  }
 ]
}