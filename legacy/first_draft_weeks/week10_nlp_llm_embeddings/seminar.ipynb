{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10 Seminar -- From FinBERT to LLM Embeddings\n",
    "\n",
    "**Duration:** 90 minutes\n",
    "\n",
    "| Exercise | Topic | Time |\n",
    "|----------|-------|------|\n",
    "| 1 | FinBERT sentiment classification | 25 min |\n",
    "| 2 | Sentence-transformer embeddings + clustering | 25 min |\n",
    "| 3 | Build a text-to-alpha pipeline | 20 min |\n",
    "| 4 | Discussion: Will LLMs replace quant researchers? | 20 min |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "\n",
    "print('Imports ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset: financial headlines with dates and tickers\n",
    "news_data = pd.DataFrame({\n",
    "    'date': pd.to_datetime([\n",
    "        '2024-01-25', '2024-01-25', '2024-01-26', '2024-01-26',\n",
    "        '2024-02-01', '2024-02-01', '2024-02-02', '2024-02-02',\n",
    "        '2024-02-15', '2024-02-15', '2024-02-16', '2024-02-16',\n",
    "        '2024-03-01', '2024-03-01', '2024-03-05', '2024-03-05',\n",
    "        '2024-03-10', '2024-03-10', '2024-03-15', '2024-03-15',\n",
    "        '2024-04-01', '2024-04-01', '2024-04-10', '2024-04-10',\n",
    "    ]),\n",
    "    'ticker': [\n",
    "        'AAPL', 'MSFT', 'AAPL', 'GOOGL',\n",
    "        'META', 'AMZN', 'NVDA', 'TSLA',\n",
    "        'JPM', 'BAC', 'AAPL', 'MSFT',\n",
    "        'GOOGL', 'META', 'NVDA', 'AMZN',\n",
    "        'TSLA', 'JPM', 'AAPL', 'MSFT',\n",
    "        'NVDA', 'GOOGL', 'META', 'AMZN',\n",
    "    ],\n",
    "    'headline': [\n",
    "        \"Apple beats Q1 estimates with strong iPhone sales in emerging markets\",\n",
    "        \"Microsoft cloud revenue grows 30%, AI products drive enterprise adoption\",\n",
    "        \"Apple warns of slowing China demand, shares dip 3% in after-hours\",\n",
    "        \"Google announces $70B share buyback program, stock hits all-time high\",\n",
    "        \"Meta's Reality Labs loses $4.6B, but core advertising rebounds sharply\",\n",
    "        \"Amazon Web Services growth accelerates for third straight quarter\",\n",
    "        \"NVIDIA revenue more than triples on unprecedented AI chip demand\",\n",
    "        \"Tesla misses delivery targets, price cuts fail to boost demand\",\n",
    "        \"JPMorgan raises dividend by 10%, signals confidence in economic outlook\",\n",
    "        \"Bank of America warns of rising consumer credit delinquencies\",\n",
    "        \"Apple Vision Pro launch receives mixed reviews from early adopters\",\n",
    "        \"Microsoft faces EU antitrust probe over Teams bundling practices\",\n",
    "        \"Google DeepMind achieves breakthrough in protein structure prediction\",\n",
    "        \"Meta launches next-gen Llama model, open-sources weights for researchers\",\n",
    "        \"NVIDIA announces Blackwell GPU architecture, orders backlogged through 2025\",\n",
    "        \"Amazon expands same-day delivery to 30 new metro areas\",\n",
    "        \"Tesla recalls 2.2 million vehicles over warning light software issue\",\n",
    "        \"JPMorgan CEO warns of geopolitical risks to global economic recovery\",\n",
    "        \"Apple services revenue hits record $23B, offsetting hardware decline\",\n",
    "        \"Microsoft Copilot adoption surges among Fortune 500 companies\",\n",
    "        \"NVIDIA data center revenue hits $18.4B, exceeding all estimates\",\n",
    "        \"Google Cloud achieves operating profitability for first time\",\n",
    "        \"Meta cuts 10,000 jobs in latest round of efficiency measures\",\n",
    "        \"Amazon's advertising business grows 27%, becomes third profit pillar\",\n",
    "    ],\n",
    "    # Simulated next-day returns\n",
    "    'next_day_return': [\n",
    "        0.025, 0.018, -0.031, 0.042,\n",
    "        -0.008, 0.015, 0.068, -0.045,\n",
    "        0.012, -0.022, -0.005, -0.015,\n",
    "        0.020, 0.010, 0.035, 0.008,\n",
    "        -0.038, -0.010, 0.015, 0.022,\n",
    "        0.055, 0.028, -0.020, 0.018,\n",
    "    ],\n",
    "})\n",
    "\n",
    "print(f'Loaded {len(news_data)} headlines for {news_data[\"ticker\"].nunique()} tickers.')\n",
    "news_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 1: FinBERT Sentiment Classification (25 min)\n",
    "\n",
    "**Goal:** Run FinBERT on all headlines and analyze the relationship between sentiment and next-day returns.\n",
    "\n",
    "### Tasks\n",
    "1. Load `ProsusAI/finbert` and classify each headline\n",
    "2. Add sentiment scores (positive, negative, neutral) to the dataframe\n",
    "3. Compute the average next-day return by sentiment class\n",
    "4. Measure the information coefficient between sentiment score and return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Run FinBERT\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('ProsusAI/finbert')\n",
    "    finbert = AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert')\n",
    "    finbert.eval()\n",
    "\n",
    "    sentiments = []\n",
    "    for headline in news_data['headline']:\n",
    "        inputs = tokenizer(headline, return_tensors='pt', truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            logits = finbert(**inputs).logits\n",
    "        probs = torch.softmax(logits, dim=1).squeeze().numpy()\n",
    "        sentiments.append({'pos': probs[0], 'neg': probs[1], 'neu': probs[2]})\n",
    "\n",
    "    finbert_loaded = True\n",
    "    print('FinBERT analysis complete.')\n",
    "\n",
    "except ImportError:\n",
    "    print('transformers not available. Using simulated FinBERT scores.')\n",
    "    np.random.seed(42)\n",
    "    sentiments = []\n",
    "    for ret in news_data['next_day_return']:\n",
    "        # Simulate: correlated with actual return (imperfect signal)\n",
    "        if ret > 0.01:\n",
    "            base = [0.7, 0.1, 0.2]\n",
    "        elif ret < -0.01:\n",
    "            base = [0.1, 0.7, 0.2]\n",
    "        else:\n",
    "            base = [0.25, 0.25, 0.50]\n",
    "        noise = np.random.dirichlet([8, 8, 8])\n",
    "        probs = 0.7 * np.array(base) + 0.3 * noise\n",
    "        probs /= probs.sum()\n",
    "        sentiments.append({'pos': probs[0], 'neg': probs[1], 'neu': probs[2]})\n",
    "    finbert_loaded = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Add scores to dataframe\n",
    "news_data['fb_positive'] = [s['pos'] for s in sentiments]\n",
    "news_data['fb_negative'] = [s['neg'] for s in sentiments]\n",
    "news_data['fb_neutral'] = [s['neu'] for s in sentiments]\n",
    "news_data['fb_sentiment'] = news_data[['fb_positive', 'fb_negative', 'fb_neutral']].idxmax(axis=1)\n",
    "news_data['fb_sentiment'] = news_data['fb_sentiment'].str.replace('fb_', '')\n",
    "\n",
    "# Net sentiment score: positive - negative\n",
    "news_data['fb_net_score'] = news_data['fb_positive'] - news_data['fb_negative']\n",
    "\n",
    "print(news_data[['headline', 'fb_sentiment', 'fb_net_score', 'next_day_return']].head(10).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Average return by sentiment class\n",
    "sentiment_returns = news_data.groupby('fb_sentiment')['next_day_return'].agg(['mean', 'std', 'count'])\n",
    "print('Average Next-Day Return by FinBERT Sentiment:')\n",
    "print(sentiment_returns.round(4).to_string())\n",
    "\n",
    "# Task 4: Information coefficient\n",
    "ic = np.corrcoef(news_data['fb_net_score'], news_data['next_day_return'])[0, 1]\n",
    "print(f'\\nIC (FinBERT net score vs. next-day return): {ic:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Sentiment vs. return scatter\n",
    "colors = {'positive': '#27ae60', 'negative': '#e74c3c', 'neutral': '#95a5a6'}\n",
    "for sent in ['positive', 'negative', 'neutral']:\n",
    "    mask = news_data['fb_sentiment'] == sent\n",
    "    axes[0].scatter(news_data.loc[mask, 'fb_net_score'],\n",
    "                    news_data.loc[mask, 'next_day_return'],\n",
    "                    c=colors[sent], label=sent, s=60, edgecolors='white', alpha=0.8)\n",
    "\n",
    "axes[0].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0].axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0].set_xlabel('FinBERT Net Score (pos - neg)')\n",
    "axes[0].set_ylabel('Next-Day Return')\n",
    "axes[0].set_title(f'FinBERT Sentiment vs. Return (IC={ic:.3f})')\n",
    "axes[0].legend()\n",
    "\n",
    "# Bar chart of average returns\n",
    "bar_colors = [colors.get(s, 'gray') for s in sentiment_returns.index]\n",
    "axes[1].bar(sentiment_returns.index, sentiment_returns['mean'], color=bar_colors, edgecolor='white')\n",
    "axes[1].set_ylabel('Average Next-Day Return')\n",
    "axes[1].set_title('Average Return by Sentiment Class')\n",
    "axes[1].axhline(y=0, color='gray', linestyle='--')\n",
    "\n",
    "plt.suptitle('Exercise 1: FinBERT Sentiment Analysis', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 -- Discussion\n",
    "- Is the IC statistically significant with only 24 observations?\n",
    "- Does the sentiment capture *magnitude* of the move?\n",
    "- What information is lost by reducing a headline to 3 probabilities?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Sentence-Transformer Embeddings + Clustering (25 min)\n",
    "\n",
    "**Goal:** Generate rich embeddings of financial headlines and explore their structure via clustering.\n",
    "\n",
    "### Tasks\n",
    "1. Encode all headlines with `all-MiniLM-L6-v2`\n",
    "2. Compute pairwise cosine similarity\n",
    "3. Cluster embeddings (K-Means, k=4)\n",
    "4. Visualize clusters in 2D (PCA)\n",
    "5. Analyze: do clusters correspond to sectors, sentiment, or something else?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Encode headlines\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    st_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = st_model.encode(news_data['headline'].tolist())\n",
    "    st_loaded = True\n",
    "    print(f'Embeddings computed: {embeddings.shape}')\n",
    "\n",
    "except ImportError:\n",
    "    print('sentence-transformers not available. Using simulated embeddings.')\n",
    "    np.random.seed(42)\n",
    "    embeddings = np.random.randn(len(news_data), 384)\n",
    "    # Add sector-based structure\n",
    "    sector_map = {\n",
    "        'AAPL': 0, 'MSFT': 0, 'GOOGL': 0, 'META': 0, 'AMZN': 0, 'NVDA': 0, 'TSLA': 0,\n",
    "        'JPM': 1, 'BAC': 1,\n",
    "    }\n",
    "    for i, ticker in enumerate(news_data['ticker']):\n",
    "        sector_id = sector_map.get(ticker, 0)\n",
    "        embeddings[i, :10] += sector_id * 2  # sector signal\n",
    "        embeddings[i, 10:20] += news_data.iloc[i]['next_day_return'] * 50  # sentiment signal\n",
    "    st_loaded = False\n",
    "    print(f'Simulated embeddings: {embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cos_sim = cosine_similarity(embeddings)\n",
    "\n",
    "# Find most similar headline pairs\n",
    "n = len(news_data)\n",
    "pairs = []\n",
    "for i in range(n):\n",
    "    for j in range(i + 1, n):\n",
    "        pairs.append((i, j, cos_sim[i, j]))\n",
    "\n",
    "pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print('Top 5 most similar headline pairs:')\n",
    "for i, j, sim in pairs[:5]:\n",
    "    print(f'  Sim={sim:.3f}')\n",
    "    print(f'    [{news_data.iloc[i][\"ticker\"]}] {news_data.iloc[i][\"headline\"][:70]}')\n",
    "    print(f'    [{news_data.iloc[j][\"ticker\"]}] {news_data.iloc[j][\"headline\"][:70]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: K-Means clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "news_data['cluster'] = kmeans.fit_predict(embeddings)\n",
    "\n",
    "print('Cluster composition:')\n",
    "for c in range(4):\n",
    "    mask = news_data['cluster'] == c\n",
    "    tickers_in = news_data.loc[mask, 'ticker'].unique()\n",
    "    avg_ret = news_data.loc[mask, 'next_day_return'].mean()\n",
    "    print(f'  Cluster {c}: {mask.sum()} headlines, tickers={list(tickers_in)}, avg return={avg_ret:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Visualize in 2D\n",
    "pca = PCA(n_components=2)\n",
    "emb_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Color by cluster\n",
    "cluster_colors = ['#e74c3c', '#2980b9', '#27ae60', '#f39c12']\n",
    "for c in range(4):\n",
    "    mask = news_data['cluster'] == c\n",
    "    axes[0].scatter(emb_2d[mask, 0], emb_2d[mask, 1],\n",
    "                    c=cluster_colors[c], label=f'Cluster {c}',\n",
    "                    s=60, edgecolors='white', alpha=0.8)\n",
    "\n",
    "axes[0].set_title('Headlines Colored by K-Means Cluster')\n",
    "axes[0].set_xlabel('PC1')\n",
    "axes[0].set_ylabel('PC2')\n",
    "axes[0].legend()\n",
    "\n",
    "# Color by sector\n",
    "tech = ['AAPL', 'MSFT', 'GOOGL', 'META', 'AMZN', 'NVDA', 'TSLA']\n",
    "finance = ['JPM', 'BAC']\n",
    "\n",
    "for i, row in news_data.iterrows():\n",
    "    color = '#2980b9' if row['ticker'] in tech else '#e74c3c'\n",
    "    label = 'Tech' if row['ticker'] in tech else 'Finance'\n",
    "    axes[1].scatter(emb_2d[i, 0], emb_2d[i, 1], c=color, s=60, edgecolors='white', alpha=0.8)\n",
    "    axes[1].annotate(row['ticker'], (emb_2d[i, 0], emb_2d[i, 1]), fontsize=7, alpha=0.7)\n",
    "\n",
    "# Manual legend\n",
    "axes[1].scatter([], [], c='#2980b9', label='Tech', s=60)\n",
    "axes[1].scatter([], [], c='#e74c3c', label='Finance', s=60)\n",
    "axes[1].set_title('Headlines Colored by Sector')\n",
    "axes[1].set_xlabel('PC1')\n",
    "axes[1].set_ylabel('PC2')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.suptitle('Exercise 2: Embedding Clustering', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 -- Task 5: Analysis\n",
    "\n",
    "- Do clusters align with sectors (tech vs. finance)?\n",
    "- Or do they align with themes (earnings beats, guidance, risk events)?\n",
    "- What does this tell you about what the embedding captures?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Text-to-Alpha Pipeline (20 min)\n",
    "\n",
    "**Goal:** Build a simple pipeline that goes from text to return prediction.\n",
    "\n",
    "```\n",
    "Headlines --> Embeddings --> Dimensionality reduction --> XGBoost --> Return prediction\n",
    "```\n",
    "\n",
    "### Tasks\n",
    "1. Reduce embedding dimensionality (PCA to 20 components)\n",
    "2. Combine with FinBERT sentiment as additional features\n",
    "3. Train XGBoost to predict next-day return\n",
    "4. Evaluate: IC, R-squared, classification accuracy for direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: PCA on embeddings\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_full = PCA(n_components=20)\n",
    "emb_pca = pca_full.fit_transform(embeddings)\n",
    "\n",
    "emb_features = pd.DataFrame(\n",
    "    emb_pca,\n",
    "    columns=[f'emb_pc{i}' for i in range(20)],\n",
    "    index=news_data.index\n",
    ")\n",
    "\n",
    "print(f'PCA embeddings: {emb_features.shape}')\n",
    "print(f'Variance explained: {pca_full.explained_variance_ratio_.sum():.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Combine features\n",
    "feature_df = pd.concat([\n",
    "    emb_features,\n",
    "    news_data[['fb_positive', 'fb_negative', 'fb_neutral', 'fb_net_score']].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "target = news_data['next_day_return'].values\n",
    "\n",
    "print(f'Feature matrix: {feature_df.shape}')\n",
    "print(f'Features: {list(feature_df.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Train models (using leave-one-out due to small dataset)\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_predict\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Three models: FinBERT only, Embeddings only, Combined\n",
    "finbert_cols = ['fb_positive', 'fb_negative', 'fb_neutral', 'fb_net_score']\n",
    "emb_cols = [f'emb_pc{i}' for i in range(20)]\n",
    "all_cols = emb_cols + finbert_cols\n",
    "\n",
    "models = {\n",
    "    'FinBERT only': finbert_cols,\n",
    "    'Embedding only': emb_cols,\n",
    "    'Combined': all_cols,\n",
    "}\n",
    "\n",
    "results = []\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "for name, cols in models.items():\n",
    "    X = feature_df[cols].values\n",
    "    xgb = XGBRegressor(n_estimators=50, max_depth=3, learning_rate=0.1,\n",
    "                       subsample=0.8, random_state=42)\n",
    "    \n",
    "    # Leave-one-out predictions\n",
    "    predictions = cross_val_predict(xgb, X, target, cv=loo)\n",
    "    \n",
    "    ic = np.corrcoef(target, predictions)[0, 1]\n",
    "    r2 = r2_score(target, predictions)\n",
    "    dir_acc = np.mean(np.sign(target) == np.sign(predictions))\n",
    "    \n",
    "    results.append({'Model': name, 'IC': ic, 'R2': r2, 'Dir Acc': dir_acc})\n",
    "    print(f'{name}: IC={ic:.4f}, R2={r2:.4f}, Dir Acc={dir_acc:.1%}')\n",
    "\n",
    "results_df = pd.DataFrame(results).set_index('Model')\n",
    "print()\n",
    "print(results_df.round(4).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# IC comparison\n",
    "colors = ['#f39c12', '#2980b9', '#27ae60']\n",
    "axes[0].bar(results_df.index, results_df['IC'], color=colors, edgecolor='white')\n",
    "axes[0].set_ylabel('Information Coefficient')\n",
    "axes[0].set_title('IC: Text-to-Alpha Pipeline')\n",
    "axes[0].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# Predicted vs. actual (combined model)\n",
    "X_all = feature_df[all_cols].values\n",
    "xgb_combined = XGBRegressor(n_estimators=50, max_depth=3, learning_rate=0.1,\n",
    "                            subsample=0.8, random_state=42)\n",
    "preds_combined = cross_val_predict(xgb_combined, X_all, target, cv=loo)\n",
    "\n",
    "axes[1].scatter(preds_combined, target, c='#2980b9', s=60, edgecolors='white', alpha=0.8)\n",
    "axes[1].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[1].axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Regression line\n",
    "z = np.polyfit(preds_combined, target, 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(preds_combined.min(), preds_combined.max(), 50)\n",
    "axes[1].plot(x_line, p(x_line), 'r--', alpha=0.7)\n",
    "\n",
    "axes[1].set_xlabel('Predicted Return')\n",
    "axes[1].set_ylabel('Actual Return')\n",
    "axes[1].set_title('Combined Model: Predicted vs. Actual')\n",
    "\n",
    "plt.suptitle('Exercise 3: Text-to-Alpha Pipeline', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 -- Key Takeaways\n",
    "\n",
    "- Embeddings typically carry more predictive information than 3-class sentiment\n",
    "- The combined model can leverage both structured sentiment and unstructured embedding information\n",
    "- With only 24 headlines, these results are highly noisy -- real pipelines use thousands of headlines\n",
    "- In production: daily aggregation of embeddings per ticker, with proper time-series CV\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Discussion -- Will LLMs Replace Quant Researchers? (20 min)\n",
    "\n",
    "### Discussion Questions\n",
    "\n",
    "1. **The Chen-Kelly-Xiu result:** They showed LLM embeddings outperform traditional NLP for return prediction. But the embedding model itself does not understand finance -- it just maps text to vectors. Where does the \"intelligence\" reside?\n",
    "\n",
    "2. **AlphaGPT and RD-Agent:** These systems can autonomously propose and test alpha hypotheses. If an LLM can write a factor, backtest it, and iterate -- what is the human quant researcher doing?\n",
    "\n",
    "3. **The moat question:** If everyone uses the same LLM (GPT-4, Claude, Llama) for embeddings, is there alpha in the embeddings themselves? Or does the alpha come from *what you do with them* (the downstream model, the data, the execution)?\n",
    "\n",
    "4. **Interpretability:** A portfolio manager asks: \"Why did we go long NVDA today?\" You say: \"Because dimension 47 of the sentence-transformer embedding was 0.3 standard deviations above its 90-day mean.\" Is this acceptable?\n",
    "\n",
    "5. **Data pipeline challenges:** In production, you need to process thousands of headlines per day, each with different tickers, sources, and reliability levels. The NLP model is easy -- the engineering is hard. Does this change the value proposition?\n",
    "\n",
    "### Positions to Debate\n",
    "\n",
    "**\"LLMs will replace most quant researchers within 5 years\"**\n",
    "- AlphaGPT can already generate and test factors autonomously\n",
    "- LLM embeddings outperform decades of hand-crafted NLP features\n",
    "- Coding is becoming a commodity -- LLMs write backtests better than juniors\n",
    "\n",
    "**\"LLMs are tools, not replacements\"**\n",
    "- Alpha discovery requires domain knowledge that LLMs lack\n",
    "- Risk management, portfolio construction, and execution are not NLP problems\n",
    "- The hard part is data acquisition and cleaning, not model building\n",
    "- Regulatory requirements demand human oversight and accountability\n",
    "\n",
    "**\"The real change is in workflow, not headcount\"**\n",
    "- One researcher with LLM tools can do the work of five without them\n",
    "- The role shifts from coding to supervising and curating\n",
    "- Domain experts become more valuable, not less, as they guide the AI"
   ]
  }
 ]
}