{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10 — Homework: Text Alpha — FinBERT vs. LLM Embeddings\n",
    "\n",
    "**Course:** ML for Quantitative Finance  \n",
    "**Due:** Before Week 11 lecture\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "Compare three eras of financial NLP for return prediction:\n",
    "- **Baseline:** FinBERT sentiment (3 scores per headline)\n",
    "- **Modern:** Sentence-transformer embeddings (384-dim per headline)\n",
    "- **Combined:** Both feature sets together\n",
    "\n",
    "Demonstrate that richer text representations carry more predictive information.\n",
    "\n",
    "## Setup\n",
    "\n",
    "```bash\n",
    "pip install transformers sentence-transformers xgboost scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preparation (10 pts)\n",
    "\n",
    "1. Load the Financial PhraseBank dataset (built into `datasets` library, or download from HuggingFace)\n",
    "2. Filter to sentences with ≥75% annotator agreement\n",
    "3. Report: class distribution, average sentence length\n",
    "4. Assign simulated stock tickers and dates for the pipeline demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load Financial PhraseBank\n",
    "#   Option A: from datasets import load_dataset\n",
    "#             dataset = load_dataset('financial_phrasebank', 'sentences_75agree')\n",
    "#   Option B: Manual download from https://huggingface.co/datasets/financial_phrasebank\n",
    "#   Option C: Use the provided synthetic dataset below as fallback\n",
    "\n",
    "# TODO: If using fallback, create a dataset of ~200 financial headlines\n",
    "#   with columns: ['text', 'label', 'ticker', 'date', 'next_day_return']\n",
    "# TODO: Report class distribution and basic stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: FinBERT Baseline (20 pts)\n",
    "\n",
    "1. Load `ProsusAI/finbert` from HuggingFace\n",
    "2. Score all headlines → get (positive, negative, neutral) probabilities\n",
    "3. Compute `net_sentiment = positive - negative`\n",
    "4. Aggregate to stock-level daily sentiment (average across headlines per ticker per day)\n",
    "5. Measure IC between `net_sentiment` and next-day return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load FinBERT\n",
    "#   from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "#   tokenizer = AutoTokenizer.from_pretrained('ProsusAI/finbert')\n",
    "#   model = AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert')\n",
    "\n",
    "# TODO: Score all headlines (batch or loop)\n",
    "# TODO: Compute net_sentiment = positive - negative\n",
    "# TODO: Aggregate per ticker per day\n",
    "# TODO: Compute IC(net_sentiment, next_day_return)\n",
    "# TODO: Plot sentiment vs return scatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Sentence-Transformer Embeddings (25 pts)\n",
    "\n",
    "1. Encode all headlines with `all-MiniLM-L6-v2` (384-dim, runs on M4)\n",
    "2. Reduce dimensions with PCA (keep 10 and 20 components — compare both)\n",
    "3. Aggregate to stock-level daily embedding (average embeddings per ticker per day)\n",
    "4. Visualize embedding space: PCA scatter colored by sentiment label\n",
    "5. Compute pairwise cosine similarity — do semantically similar headlines cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load sentence-transformer\n",
    "#   from sentence_transformers import SentenceTransformer\n",
    "#   model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "#   embeddings = model.encode(headlines)\n",
    "\n",
    "# TODO: PCA to 10 and 20 components\n",
    "# TODO: Aggregate per ticker per day\n",
    "# TODO: Plot PCA 2D scatter colored by sentiment\n",
    "# TODO: Plot cosine similarity heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Model Comparison (25 pts)\n",
    "\n",
    "Train XGBoost to predict next-day return using 4 feature sets:\n",
    "\n",
    "| Model | Features |\n",
    "|-------|----------|\n",
    "| A | Price/volume features only (momentum, vol) |\n",
    "| B | A + FinBERT sentiment (3 scores) |\n",
    "| C | A + embedding PCA (10-20 components) |\n",
    "| D | A + both text feature sets |\n",
    "\n",
    "Evaluate with:\n",
    "- Information coefficient (Spearman rank correlation)\n",
    "- Direction accuracy (% correct sign)\n",
    "- Report the **marginal contribution** of each text feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Construct feature matrices for Models A, B, C, D\n",
    "# TODO: Train XGBoost with cross-validation (temporal split or LOO for small data)\n",
    "# TODO: Compute IC, direction accuracy for each model\n",
    "# TODO: Build comparison table\n",
    "# TODO: Bar chart comparing IC across models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Signal Decay Analysis (10 pts)\n",
    "\n",
    "1. For both FinBERT sentiment and embedding features, compute IC at different horizons:\n",
    "   - 1-day, 2-day, 5-day, 10-day, 20-day forward returns\n",
    "2. Plot IC vs horizon for both methods\n",
    "3. Which decays faster? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute forward returns at multiple horizons\n",
    "# TODO: Compute IC(text_feature, forward_return) for each horizon\n",
    "# TODO: Plot signal decay curves for FinBERT vs embeddings\n",
    "# TODO: Discuss which decays faster and why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Discussion (10 pts)\n",
    "\n",
    "Answer in 2-3 sentences each:\n",
    "\n",
    "1. Why do embeddings outperform 3-class sentiment? What information is preserved?\n",
    "2. In production, you need to process 10,000 headlines per day. Compare the cost/benefit of:\n",
    "   - Free sentence-transformers (local, ~10ms per headline)\n",
    "   - OpenAI embedding API (~$0.02 per 1M tokens)\n",
    "   - Fine-tuning FinBERT on your own data\n",
    "3. If everyone uses the same embedding model, is there still alpha? Where does the edge come from?\n",
    "4. How would you handle conflicting headlines about the same stock on the same day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your discussion in markdown cells below"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}