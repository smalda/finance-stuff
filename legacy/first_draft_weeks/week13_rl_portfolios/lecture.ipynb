{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 11 — Reinforcement Learning for Portfolio Management\n",
    "\n",
    "**Key ideas:**\n",
    "- Trading is a sequential decision problem — naturally fits the MDP framework\n",
    "- RL agents learn policies that map market states to portfolio actions\n",
    "- RL is hard in finance: non-stationarity, partial observability, delayed rewards\n",
    "- Realistic expectation: RL learns risk management, not alpha generation\n",
    "\n",
    "**Outline:**\n",
    "1. Trading as an MDP\n",
    "2. Key RL algorithms for finance\n",
    "3. Why RL is hard in finance\n",
    "4. FinRL architecture\n",
    "5. Where RL works in industry\n",
    "6. Demo: Custom gym environment for portfolio allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Trading as a Markov Decision Process (MDP)\n",
    "\n",
    "An MDP is defined by $(S, A, P, R, \\gamma)$:\n",
    "\n",
    "| Component | Finance Interpretation |\n",
    "|-----------|------------------------|\n",
    "| **State** $s_t$ | Portfolio weights, asset prices, technical features, account balance |\n",
    "| **Action** $a_t$ | Target portfolio weights (how much to allocate to each asset) |\n",
    "| **Transition** $P(s_{t+1} \\mid s_t, a_t)$ | Market dynamics (unknown, non-stationary) |\n",
    "| **Reward** $r_t$ | Portfolio return, risk-adjusted return (Sharpe), or custom |\n",
    "| **Discount** $\\gamma$ | Time preference (typically close to 1 for daily trading) |\n",
    "\n",
    "The agent's goal: find a policy $\\pi(a_t \\mid s_t)$ that maximizes cumulative discounted reward:\n",
    "\n",
    "$$\\max_\\pi \\; \\mathbb{E} \\left[ \\sum_{t=0}^{T} \\gamma^t \\, r_t \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State representation\n",
    "\n",
    "A practical state vector for portfolio RL:\n",
    "\n",
    "$$s_t = \\big[ w_t^{(1)}, \\ldots, w_t^{(n)}, \\; \\text{ret}_t^{(1)}, \\ldots, \\text{ret}_t^{(n)}, \\; \\text{features}_t \\big]$$\n",
    "\n",
    "Where:\n",
    "- $w_t^{(i)}$ = current weight of asset $i$\n",
    "- $\\text{ret}_t^{(i)}$ = recent return(s) of asset $i$\n",
    "- $\\text{features}_t$ = MACD, RSI, volatility, etc.\n",
    "\n",
    "### Reward design choices\n",
    "\n",
    "| Reward | Formula | Pros | Cons |\n",
    "|--------|---------|------|------|\n",
    "| Raw return | $r_t = \\sum_i w_i \\cdot r_i^{(t)}$ | Simple | Ignores risk |\n",
    "| Sharpe-based | $r_t = \\frac{\\bar{r}}{\\sigma_r}$ (rolling) | Risk-adjusted | Noisy estimate |\n",
    "| Return - penalty | $r_t = r_p - \\lambda \\cdot \\text{drawdown}_t$ | Controls drawdowns | Sensitive to $\\lambda$ |\n",
    "| Differential Sharpe | $r_t = \\frac{\\partial \\text{Sharpe}}{\\partial r_p}$ | Incremental | Harder to implement |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick illustration: different reward signals for the same portfolio return stream\n",
    "T = 252\n",
    "portfolio_returns = np.random.normal(0.0005, 0.015, T)\n",
    "\n",
    "# Raw return reward\n",
    "reward_raw = portfolio_returns.copy()\n",
    "\n",
    "# Sharpe-based (rolling 20-day)\n",
    "window = 20\n",
    "reward_sharpe = np.zeros(T)\n",
    "for t in range(window, T):\n",
    "    chunk = portfolio_returns[t - window:t]\n",
    "    reward_sharpe[t] = chunk.mean() / (chunk.std() + 1e-8)\n",
    "\n",
    "# Return - drawdown penalty\n",
    "cumulative = np.cumprod(1 + portfolio_returns)\n",
    "running_max = np.maximum.accumulate(cumulative)\n",
    "drawdown = (running_max - cumulative) / running_max\n",
    "lam = 2.0\n",
    "reward_dd = portfolio_returns - lam * drawdown\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for ax, rw, title in zip(axes, [reward_raw, reward_sharpe, reward_dd],\n",
    "                          ['Raw Return', 'Rolling Sharpe', 'Return - 2*Drawdown']):\n",
    "    ax.plot(rw, linewidth=0.8)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Day')\n",
    "    ax.axhline(0, color='gray', linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Key RL Algorithms for Finance\n",
    "\n",
    "| Algorithm | Type | Action Space | Key Idea | Finance Fit |\n",
    "|-----------|------|-------------|----------|-------------|\n",
    "| **DQN** | Value-based | Discrete | Q-network with experience replay | Good for discrete actions (buy/hold/sell) |\n",
    "| **PPO** | Policy gradient | Continuous | Clipped surrogate objective, stable training | Most popular for portfolios |\n",
    "| **A2C** | Actor-critic | Continuous | Advantage function reduces variance | Faster but less stable than PPO |\n",
    "| **SAC** | Actor-critic | Continuous | Maximum entropy — explores more | Good for complex action spaces |\n",
    "| **DDPG** | Actor-critic | Continuous | Deterministic policy gradient | Off-policy, sample efficient |\n",
    "\n",
    "### Why PPO is the default choice\n",
    "\n",
    "PPO uses a clipped objective that prevents large policy updates:\n",
    "\n",
    "$$L^{\\text{CLIP}}(\\theta) = \\mathbb{E}_t \\left[ \\min\\left( r_t(\\theta) \\hat{A}_t, \\; \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\hat{A}_t \\right) \\right]$$\n",
    "\n",
    "- $r_t(\\theta) = \\pi_\\theta(a_t|s_t) / \\pi_{\\theta_{\\text{old}}}(a_t|s_t)$ — probability ratio\n",
    "- $\\hat{A}_t$ — estimated advantage\n",
    "- $\\epsilon \\approx 0.2$ — clip range\n",
    "\n",
    "This is great for finance because:\n",
    "1. Stable training (no catastrophic policy shifts)\n",
    "2. Works with continuous action spaces (portfolio weights)\n",
    "3. Robust to hyperparameters\n",
    "4. Easy to parallelize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Why RL Is Hard in Finance\n",
    "\n",
    "### Problem 1: Non-stationarity\n",
    "Markets change regimes — a policy trained on bull markets fails in bear markets.\n",
    "The transition function $P(s_{t+1}|s_t, a_t)$ is not fixed.\n",
    "\n",
    "### Problem 2: Partial observability\n",
    "The agent sees prices and features but not:\n",
    "- Order flow / institutional positions\n",
    "- Macro policy announcements before they happen\n",
    "- Other agents' strategies\n",
    "\n",
    "This means the true state is partially hidden — technically a POMDP.\n",
    "\n",
    "### Problem 3: Delayed and noisy rewards\n",
    "- A good trade might look bad for weeks before paying off\n",
    "- Daily returns are extremely noisy (signal-to-noise ratio ~0.05)\n",
    "- Credit assignment is hard: which action caused the loss?\n",
    "\n",
    "### Problem 4: Sample efficiency\n",
    "- We have ONE history of the market (no parallel environments)\n",
    "- Financial data is expensive and limited\n",
    "- Simulated environments don't capture real market microstructure\n",
    "\n",
    "### Problem 5: Overfitting\n",
    "- RL agents are extremely good at memorizing training episodes\n",
    "- Backtested RL performance rarely generalizes to live trading\n",
    "- Unlike supervised ML, there's no clear train/test split for RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. FinRL Architecture\n",
    "\n",
    "FinRL is the most popular open-source library for financial RL. Its pipeline:\n",
    "\n",
    "```\n",
    "Data Layer         Environment Layer       Agent Layer         Backtest Layer\n",
    "----------         -----------------       -----------         --------------\n",
    "Yahoo Finance      StockTradingEnv         PPO / A2C /         Pyfolio\n",
    "Alpaca API    -->  (gym.Env)          -->  SAC / DDPG    -->   Quantstats\n",
    "WRDS               Custom rewards          (Stable-Baselines3) Custom metrics\n",
    "```\n",
    "\n",
    "### Data flow:\n",
    "1. **Download** OHLCV data + technical indicators\n",
    "2. **Preprocess** into a DataFrame with columns: date, tic, open, high, low, close, volume, features...\n",
    "3. **Create environment** that reads this data and exposes gym interface\n",
    "4. **Train agent** using Stable-Baselines3\n",
    "5. **Backtest** on held-out period\n",
    "\n",
    "### Key FinRL classes:\n",
    "```python\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from stable_baselines3 import PPO, A2C, SAC, DDPG\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Where RL Actually Works in Industry\n",
    "\n",
    "| Application | Why RL Works Here | Companies |\n",
    "|-------------|-------------------|----------|\n",
    "| **Optimal execution** | Clear reward (minimize slippage), fast feedback | JPMorgan, Goldman |\n",
    "| **Market making** | Stationary-ish dynamics, high-frequency feedback | Citadel, Two Sigma |\n",
    "| **Options hedging** | Can simulate with known models, clear objective | Various sell-side |\n",
    "| **Order routing** | Discrete actions, fast rewards | Most brokers |\n",
    "\n",
    "### Where RL struggles:\n",
    "- **Alpha generation**: too noisy, too non-stationary\n",
    "- **Long-horizon portfolio management**: delayed rewards, regime changes\n",
    "- **Low-frequency trading**: not enough data to train\n",
    "\n",
    "### Realistic expectations\n",
    "RL in portfolio management typically learns to:\n",
    "- Reduce drawdowns (risk management)\n",
    "- Smooth position transitions (reduce turnover)\n",
    "- Adapt to volatility regimes\n",
    "\n",
    "It does NOT typically:\n",
    "- Beat simple momentum or mean-reversion strategies\n",
    "- Generate consistent alpha\n",
    "- Work out-of-the-box without careful reward engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Demo: Custom Gym Environment for Portfolio Allocation\n",
    "\n",
    "We'll build a simple portfolio environment from scratch to understand the mechanics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePortfolioEnv(gym.Env):\n",
    "    \"\"\"A minimal portfolio allocation environment.\n",
    "    \n",
    "    State:  [current_weights (n), recent_returns (n), volatility (n)]\n",
    "    Action: target_weights (n) — softmax-normalized internally\n",
    "    Reward: portfolio return (or risk-adjusted variant)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, returns_data, lookback=20, transaction_cost=0.001):\n",
    "        super().__init__()\n",
    "        self.returns_data = returns_data  # (T, n_assets)\n",
    "        self.n_assets = returns_data.shape[1]\n",
    "        self.lookback = lookback\n",
    "        self.tc = transaction_cost\n",
    "        \n",
    "        # Observation: weights + returns + volatility\n",
    "        obs_dim = 3 * self.n_assets\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32\n",
    "        )\n",
    "        # Action: target weights (before softmax)\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-1, high=1, shape=(self.n_assets,), dtype=np.float32\n",
    "        )\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.t = self.lookback\n",
    "        self.weights = np.ones(self.n_assets) / self.n_assets  # equal weight\n",
    "        self.portfolio_value = 1.0\n",
    "        self.history = []\n",
    "        return self._get_obs(), {}\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        recent = self.returns_data[self.t - self.lookback:self.t]\n",
    "        mean_ret = recent.mean(axis=0)\n",
    "        vol = recent.std(axis=0) + 1e-8\n",
    "        obs = np.concatenate([self.weights, mean_ret, vol]).astype(np.float32)\n",
    "        return obs\n",
    "    \n",
    "    def _softmax(self, x):\n",
    "        e = np.exp(x - x.max())\n",
    "        return e / e.sum()\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Convert action to portfolio weights\n",
    "        new_weights = self._softmax(action)\n",
    "        \n",
    "        # Transaction costs\n",
    "        turnover = np.abs(new_weights - self.weights).sum()\n",
    "        tc_cost = self.tc * turnover\n",
    "        \n",
    "        # Portfolio return\n",
    "        asset_returns = self.returns_data[self.t]\n",
    "        port_return = np.dot(new_weights, asset_returns) - tc_cost\n",
    "        \n",
    "        # Update state\n",
    "        self.portfolio_value *= (1 + port_return)\n",
    "        self.weights = new_weights * (1 + asset_returns)\n",
    "        self.weights /= self.weights.sum()  # renormalize after price changes\n",
    "        self.t += 1\n",
    "        \n",
    "        self.history.append({\n",
    "            'portfolio_value': self.portfolio_value,\n",
    "            'return': port_return,\n",
    "            'turnover': turnover,\n",
    "        })\n",
    "        \n",
    "        # Episode ends when we run out of data\n",
    "        terminated = self.t >= len(self.returns_data)\n",
    "        \n",
    "        return self._get_obs(), port_return, terminated, False, {}\n",
    "\n",
    "print(\"SimplePortfolioEnv defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic stock returns for 5 assets\n",
    "n_days = 504  # ~2 years\n",
    "n_assets = 5\n",
    "asset_names = ['Tech', 'Finance', 'Healthcare', 'Energy', 'Consumer']\n",
    "\n",
    "# Different risk/return profiles\n",
    "mus = np.array([0.0008, 0.0004, 0.0006, 0.0003, 0.0005])\n",
    "sigmas = np.array([0.02, 0.015, 0.018, 0.025, 0.012])\n",
    "\n",
    "# Correlated returns\n",
    "corr = np.array([\n",
    "    [1.0, 0.5, 0.3, 0.2, 0.4],\n",
    "    [0.5, 1.0, 0.4, 0.3, 0.5],\n",
    "    [0.3, 0.4, 1.0, 0.2, 0.3],\n",
    "    [0.2, 0.3, 0.2, 1.0, 0.2],\n",
    "    [0.4, 0.5, 0.3, 0.2, 1.0],\n",
    "])\n",
    "cov = np.outer(sigmas, sigmas) * corr\n",
    "\n",
    "returns_data = np.random.multivariate_normal(mus, cov, n_days)\n",
    "\n",
    "# Visualize cumulative returns\n",
    "cumulative = np.cumprod(1 + returns_data, axis=0)\n",
    "for i, name in enumerate(asset_names):\n",
    "    plt.plot(cumulative[:, i], label=name)\n",
    "plt.title('Synthetic Asset Cumulative Returns')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Cumulative Return')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the environment manually\n",
    "env = SimplePortfolioEnv(returns_data)\n",
    "obs, info = env.reset()\n",
    "print(f\"Observation shape: {obs.shape}\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"\\nInitial obs (first 5 = weights):\")\n",
    "print(f\"  Weights: {obs[:5]}\")\n",
    "print(f\"  Mean returns: {obs[5:10]}\")\n",
    "print(f\"  Volatilities: {obs[10:15]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a random agent vs equal-weight baseline\n",
    "def run_episode(env, policy='random'):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        if policy == 'random':\n",
    "            action = env.action_space.sample()\n",
    "        elif policy == 'equal_weight':\n",
    "            action = np.zeros(env.n_assets)  # softmax(0,...,0) = equal weight\n",
    "        else:\n",
    "            action = policy(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "    return pd.DataFrame(env.history)\n",
    "\n",
    "# Compare random vs equal weight\n",
    "env = SimplePortfolioEnv(returns_data)\n",
    "\n",
    "results_random = run_episode(env, 'random')\n",
    "results_equal = run_episode(env, 'equal_weight')\n",
    "\n",
    "plt.plot(results_random['portfolio_value'].values, label='Random Agent', alpha=0.7)\n",
    "plt.plot(results_equal['portfolio_value'].values, label='Equal Weight', alpha=0.7)\n",
    "plt.title('Random Agent vs Equal Weight Baseline')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Portfolio Value')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Random agent final value:  {results_random['portfolio_value'].iloc[-1]:.4f}\")\n",
    "print(f\"Equal weight final value:  {results_equal['portfolio_value'].iloc[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a simple PPO agent using Stable-Baselines3\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Split data: train on first 75%, test on last 25%\n",
    "split = int(0.75 * n_days)\n",
    "train_returns = returns_data[:split]\n",
    "test_returns = returns_data[split:]\n",
    "\n",
    "# Create training environment\n",
    "train_env = DummyVecEnv([lambda: SimplePortfolioEnv(train_returns)])\n",
    "\n",
    "# Train PPO\n",
    "model = PPO(\n",
    "    'MlpPolicy',\n",
    "    train_env,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=128,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "print(\"Training PPO agent...\")\n",
    "model.learn(total_timesteps=50_000)\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "test_env = SimplePortfolioEnv(test_returns)\n",
    "\n",
    "def ppo_policy(obs):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    return action\n",
    "\n",
    "results_ppo = run_episode(test_env, ppo_policy)\n",
    "\n",
    "test_env_eq = SimplePortfolioEnv(test_returns)\n",
    "results_eq_test = run_episode(test_env_eq, 'equal_weight')\n",
    "\n",
    "plt.plot(results_ppo['portfolio_value'].values, label='PPO Agent')\n",
    "plt.plot(results_eq_test['portfolio_value'].values, label='Equal Weight')\n",
    "plt.title('PPO Agent vs Equal Weight (Test Period)')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Portfolio Value')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Performance metrics\n",
    "for name, res in [('PPO', results_ppo), ('Equal Weight', results_eq_test)]:\n",
    "    rets = res['return'].values\n",
    "    sharpe = np.sqrt(252) * rets.mean() / (rets.std() + 1e-8)\n",
    "    cum_ret = res['portfolio_value'].iloc[-1] - 1\n",
    "    max_dd = (res['portfolio_value'].cummax() - res['portfolio_value']).max()\n",
    "    print(f\"{name:15s} | Return: {cum_ret:+.2%} | Sharpe: {sharpe:.2f} | Max DD: {max_dd:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Trading fits the MDP framework** — state/action/reward are well-defined\n",
    "2. **PPO is the workhorse** — stable, works with continuous actions, easy to tune\n",
    "3. **Reward design matters enormously** — raw returns lead to risk-seeking behavior\n",
    "4. **RL in finance is hard** — non-stationarity, noise, overfitting\n",
    "5. **Real success stories** are in execution, market making, hedging — not alpha\n",
    "6. **Always compare to simple baselines** — equal weight is hard to beat\n",
    "\n",
    "### Next: Seminar\n",
    "We'll use FinRL to train multiple agents on real stock data and experiment with reward functions."
   ]
  }
 ]
}