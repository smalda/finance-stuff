{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 11 — SOLUTION\n",
    "\n",
    "# RL Portfolio Manager\n",
    "\n",
    "**This is the instructor solution. Do not distribute to students.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO, A2C, SAC\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import yfinance as yf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Data Setup (15 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download 30 stocks\n",
    "tickers = [\n",
    "    'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'JPM', 'JNJ', 'V', 'PG', 'UNH', 'HD',\n",
    "    'MA', 'DIS', 'NVDA', 'BAC', 'ADBE', 'CRM', 'CMCSA', 'XOM', 'CSCO', 'PFE',\n",
    "    'NFLX', 'ABT', 'KO', 'PEP', 'TMO', 'AVGO', 'COST', 'WMT', 'MRK', 'CVX',\n",
    "]\n",
    "\n",
    "print(f\"Downloading {len(tickers)} stocks...\")\n",
    "data = yf.download(tickers, start='2018-01-01', end='2024-01-01',\n",
    "                   auto_adjust=True, progress=False)\n",
    "prices = data['Close'][tickers].dropna()\n",
    "returns = prices.pct_change().dropna()\n",
    "\n",
    "print(f\"Prices shape: {prices.shape}\")\n",
    "print(f\"Date range: {prices.index[0].date()} to {prices.index[-1].date()}\")\n",
    "print(f\"Missing values: {prices.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split: 2018-2022 train, 2023 test\n",
    "train_mask = returns.index < '2023-01-01'\n",
    "test_mask = returns.index >= '2023-01-01'\n",
    "\n",
    "train_returns = returns[train_mask].values\n",
    "test_returns = returns[test_mask].values\n",
    "\n",
    "print(f\"Train: {train_returns.shape[0]} days ({returns.index[train_mask][0].date()} to {returns.index[train_mask][-1].date()})\")\n",
    "print(f\"Test:  {test_returns.shape[0]} days ({returns.index[test_mask][0].date()} to {returns.index[test_mask][-1].date()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioEnv(gym.Env):\n",
    "    \"\"\"Portfolio environment with configurable rewards for 30 stocks.\"\"\"\n",
    "    \n",
    "    def __init__(self, returns_data, lookback=20, transaction_cost=0.001,\n",
    "                 reward_type='return', reward_lambda=2.0):\n",
    "        super().__init__()\n",
    "        self.returns_data = returns_data\n",
    "        self.n_assets = returns_data.shape[1]\n",
    "        self.lookback = lookback\n",
    "        self.tc = transaction_cost\n",
    "        self.reward_type = reward_type\n",
    "        self.reward_lambda = reward_lambda\n",
    "        \n",
    "        obs_dim = 3 * self.n_assets\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32\n",
    "        )\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-1, high=1, shape=(self.n_assets,), dtype=np.float32\n",
    "        )\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.t = self.lookback\n",
    "        self.weights = np.ones(self.n_assets) / self.n_assets\n",
    "        self.portfolio_value = 1.0\n",
    "        self.peak_value = 1.0\n",
    "        self.returns_buffer = []\n",
    "        self.history = []\n",
    "        self.weight_history = []\n",
    "        return self._get_obs(), {}\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        recent = self.returns_data[self.t - self.lookback:self.t]\n",
    "        mean_ret = recent.mean(axis=0)\n",
    "        vol = recent.std(axis=0) + 1e-8\n",
    "        return np.concatenate([self.weights, mean_ret, vol]).astype(np.float32)\n",
    "    \n",
    "    def _softmax(self, x):\n",
    "        e = np.exp(x - x.max())\n",
    "        return e / e.sum()\n",
    "    \n",
    "    def _compute_reward(self, port_return):\n",
    "        if self.reward_type == 'return':\n",
    "            return port_return\n",
    "        elif self.reward_type == 'sharpe':\n",
    "            self.returns_buffer.append(port_return)\n",
    "            if len(self.returns_buffer) < 20:\n",
    "                return port_return\n",
    "            recent = np.array(self.returns_buffer[-20:])\n",
    "            return recent.mean() / (recent.std() + 1e-8)\n",
    "        elif self.reward_type == 'return_dd':\n",
    "            self.peak_value = max(self.peak_value, self.portfolio_value)\n",
    "            dd = (self.peak_value - self.portfolio_value) / self.peak_value\n",
    "            return port_return - self.reward_lambda * dd\n",
    "        return port_return\n",
    "    \n",
    "    def step(self, action):\n",
    "        new_weights = self._softmax(action)\n",
    "        turnover = np.abs(new_weights - self.weights).sum()\n",
    "        tc_cost = self.tc * turnover\n",
    "        \n",
    "        asset_returns = self.returns_data[self.t]\n",
    "        port_return = np.dot(new_weights, asset_returns) - tc_cost\n",
    "        \n",
    "        self.portfolio_value *= (1 + port_return)\n",
    "        self.weights = new_weights * (1 + asset_returns)\n",
    "        self.weights /= self.weights.sum()\n",
    "        self.t += 1\n",
    "        \n",
    "        reward = self._compute_reward(port_return)\n",
    "        \n",
    "        self.history.append({\n",
    "            'portfolio_value': self.portfolio_value,\n",
    "            'return': port_return,\n",
    "            'turnover': turnover,\n",
    "        })\n",
    "        self.weight_history.append(new_weights.copy())\n",
    "        \n",
    "        terminated = self.t >= len(self.returns_data)\n",
    "        return self._get_obs(), reward, terminated, False, {}\n",
    "\n",
    "# Verify environment\n",
    "env = PortfolioEnv(train_returns)\n",
    "obs, _ = env.reset()\n",
    "action = env.action_space.sample()\n",
    "obs2, reward, done, _, _ = env.step(action)\n",
    "print(f\"Obs shape: {obs.shape}, Action shape: {action.shape}\")\n",
    "print(f\"Reward: {reward:.6f}, Done: {done}\")\n",
    "print(\"Environment working correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Train PPO, A2C, SAC Agents (25 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback to track training rewards\n",
    "class RewardCallback(BaseCallback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.episode_rewards = []\n",
    "        self.current_rewards = 0\n",
    "    \n",
    "    def _on_step(self):\n",
    "        self.current_rewards += self.locals['rewards'][0]\n",
    "        if self.locals['dones'][0]:\n",
    "            self.episode_rewards.append(self.current_rewards)\n",
    "            self.current_rewards = 0\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train PPO\n",
    "print(\"Training PPO...\")\n",
    "ppo_env = DummyVecEnv([lambda: PortfolioEnv(train_returns)])\n",
    "ppo_cb = RewardCallback()\n",
    "ppo_model = PPO(\n",
    "    'MlpPolicy', ppo_env,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=512,\n",
    "    batch_size=128,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.2,\n",
    "    verbose=0,\n",
    "    seed=42,\n",
    ")\n",
    "ppo_model.learn(total_timesteps=150_000, callback=ppo_cb)\n",
    "print(f\"  Episodes: {len(ppo_cb.episode_rewards)}\")\n",
    "\n",
    "# Train A2C\n",
    "print(\"Training A2C...\")\n",
    "a2c_env = DummyVecEnv([lambda: PortfolioEnv(train_returns)])\n",
    "a2c_cb = RewardCallback()\n",
    "a2c_model = A2C(\n",
    "    'MlpPolicy', a2c_env,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=128,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    verbose=0,\n",
    "    seed=42,\n",
    ")\n",
    "a2c_model.learn(total_timesteps=150_000, callback=a2c_cb)\n",
    "print(f\"  Episodes: {len(a2c_cb.episode_rewards)}\")\n",
    "\n",
    "# Train SAC\n",
    "print(\"Training SAC...\")\n",
    "sac_env = DummyVecEnv([lambda: PortfolioEnv(train_returns)])\n",
    "sac_cb = RewardCallback()\n",
    "sac_model = SAC(\n",
    "    'MlpPolicy', sac_env,\n",
    "    learning_rate=3e-4,\n",
    "    buffer_size=100_000,\n",
    "    batch_size=256,\n",
    "    gamma=0.99,\n",
    "    tau=0.005,\n",
    "    learning_starts=1000,\n",
    "    verbose=0,\n",
    "    seed=42,\n",
    ")\n",
    "sac_model.learn(total_timesteps=150_000, callback=sac_cb)\n",
    "print(f\"  Episodes: {len(sac_cb.episode_rewards)}\")\n",
    "\n",
    "print(\"\\nAll agents trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "for name, cb in [('PPO', ppo_cb), ('A2C', a2c_cb), ('SAC', sac_cb)]:\n",
    "    if len(cb.episode_rewards) > 0:\n",
    "        # Smooth with rolling mean\n",
    "        rewards = pd.Series(cb.episode_rewards)\n",
    "        smoothed = rewards.rolling(min(5, len(rewards)), min_periods=1).mean()\n",
    "        ax.plot(smoothed.values, label=name)\n",
    "\n",
    "ax.set_title('Training Episode Rewards')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Cumulative Reward')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Benchmark Comparison (25 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def evaluate_agent(model, test_data, reward_type='return'):\n",
    "    env = PortfolioEnv(test_data, reward_type=reward_type)\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        if model is None:\n",
    "            action = np.zeros(env.n_assets)  # equal weight\n",
    "        elif callable(model):\n",
    "            action = model(obs, env)  # custom policy\n",
    "        else:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, _, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "    return pd.DataFrame(env.history), np.array(env.weight_history)\n",
    "\n",
    "def compute_metrics(results):\n",
    "    rets = results['return'].values\n",
    "    pv = results['portfolio_value']\n",
    "    return {\n",
    "        'Total Return': pv.iloc[-1] - 1,\n",
    "        'Ann. Sharpe': np.sqrt(252) * rets.mean() / (rets.std() + 1e-8),\n",
    "        'Max Drawdown': ((pv.cummax() - pv) / pv.cummax()).max(),\n",
    "        'Ann. Volatility': rets.std() * np.sqrt(252),\n",
    "        'Avg Turnover': results['turnover'].mean(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum variance policy (using rolling covariance from observation)\n",
    "def min_var_policy(obs, env):\n",
    "    \"\"\"Compute min-variance weights from recent returns.\"\"\"\n",
    "    recent = env.returns_data[env.t - env.lookback:env.t]\n",
    "    cov = np.cov(recent.T) + 1e-6 * np.eye(env.n_assets)\n",
    "    try:\n",
    "        inv_cov = np.linalg.inv(cov)\n",
    "        ones = np.ones(env.n_assets)\n",
    "        w = inv_cov @ ones / (ones @ inv_cov @ ones)\n",
    "        w = np.clip(w, 0, None)  # long-only\n",
    "        w /= w.sum()\n",
    "    except np.linalg.LinAlgError:\n",
    "        w = np.ones(env.n_assets) / env.n_assets\n",
    "    # Convert to action space (inverse softmax approximation)\n",
    "    action = np.log(w + 1e-8)\n",
    "    return action.astype(np.float32)\n",
    "\n",
    "# Momentum policy: overweight recent winners\n",
    "def momentum_policy(obs, env):\n",
    "    \"\"\"Simple momentum: weight proportional to recent return rank.\"\"\"\n",
    "    recent = env.returns_data[env.t - env.lookback:env.t]\n",
    "    cum_ret = (1 + recent).prod(axis=0) - 1\n",
    "    # Rank-based weights\n",
    "    ranks = cum_ret.argsort().argsort().astype(float)\n",
    "    action = (ranks - ranks.mean()) / (ranks.std() + 1e-8)\n",
    "    return action.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all strategies\n",
    "strategies = {\n",
    "    'PPO': ppo_model,\n",
    "    'A2C': a2c_model,\n",
    "    'SAC': sac_model,\n",
    "    'Equal Weight': None,\n",
    "    'Min Variance': min_var_policy,\n",
    "    'Momentum': momentum_policy,\n",
    "}\n",
    "\n",
    "all_results = {}\n",
    "all_metrics = {}\n",
    "\n",
    "for name, model in strategies.items():\n",
    "    res, weights = evaluate_agent(model, test_returns)\n",
    "    all_results[name] = res\n",
    "    all_metrics[name] = compute_metrics(res)\n",
    "\n",
    "# Plot cumulative returns\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "for name, res in all_results.items():\n",
    "    style = '--' if name in ['Equal Weight', 'Min Variance', 'Momentum'] else '-'\n",
    "    axes[0].plot(res['portfolio_value'].values, label=name, linestyle=style)\n",
    "\n",
    "axes[0].set_title('Cumulative Returns — All Strategies (Test Set)')\n",
    "axes[0].set_xlabel('Day')\n",
    "axes[0].set_ylabel('Portfolio Value')\n",
    "axes[0].legend(fontsize=9)\n",
    "\n",
    "# Drawdown comparison\n",
    "for name, res in all_results.items():\n",
    "    pv = res['portfolio_value']\n",
    "    dd = (pv.cummax() - pv) / pv.cummax()\n",
    "    style = '--' if name in ['Equal Weight', 'Min Variance', 'Momentum'] else '-'\n",
    "    axes[1].plot(dd.values, label=name, linestyle=style, alpha=0.7)\n",
    "\n",
    "axes[1].set_title('Drawdown — All Strategies')\n",
    "axes[1].set_xlabel('Day')\n",
    "axes[1].set_ylabel('Drawdown')\n",
    "axes[1].legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "metrics_df = pd.DataFrame(all_metrics).T\n",
    "metrics_df['Total Return'] = metrics_df['Total Return'].map('{:.2%}'.format)\n",
    "metrics_df['Ann. Sharpe'] = metrics_df['Ann. Sharpe'].map('{:.2f}'.format)\n",
    "metrics_df['Max Drawdown'] = metrics_df['Max Drawdown'].map('{:.2%}'.format)\n",
    "metrics_df['Ann. Volatility'] = metrics_df['Ann. Volatility'].map('{:.2%}'.format)\n",
    "metrics_df['Avg Turnover'] = metrics_df['Avg Turnover'].map('{:.4f}'.format)\n",
    "print(metrics_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commentary:**\n",
    "\n",
    "The RL agents produce results that are broadly comparable to classical baselines. Equal-weight remains a strong baseline due to its implicit diversification and zero-cost rebalancing. The minimum-variance portfolio tends to have lower volatility but also lower returns. RL agents can sometimes reduce drawdowns relative to equal-weight, particularly when trained with drawdown-penalizing rewards, but rarely generate significantly higher Sharpe ratios out-of-sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Reward Shaping Experiments (20 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train PPO with different reward functions\n",
    "reward_configs = [\n",
    "    ('return', {}),\n",
    "    ('sharpe', {}),\n",
    "    ('return_dd (lam=1)', {'reward_type': 'return_dd', 'reward_lambda': 1.0}),\n",
    "    ('return_dd (lam=5)', {'reward_type': 'return_dd', 'reward_lambda': 5.0}),\n",
    "]\n",
    "\n",
    "reward_models = {}\n",
    "for name, kwargs in reward_configs:\n",
    "    rtype = kwargs.get('reward_type', name)\n",
    "    rlam = kwargs.get('reward_lambda', 2.0)\n",
    "    print(f\"Training PPO with reward='{name}'...\")\n",
    "    env = DummyVecEnv([lambda rt=rtype, rl=rlam: PortfolioEnv(\n",
    "        train_returns, reward_type=rt, reward_lambda=rl\n",
    "    )])\n",
    "    model = PPO('MlpPolicy', env, learning_rate=3e-4, n_steps=512,\n",
    "                batch_size=128, n_epochs=10, verbose=0, seed=42)\n",
    "    model.learn(total_timesteps=100_000)\n",
    "    reward_models[name] = model\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate reward variants\n",
    "reward_metrics = {}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "for name, model in reward_models.items():\n",
    "    res, _ = evaluate_agent(model, test_returns)\n",
    "    reward_metrics[name] = compute_metrics(res)\n",
    "    axes[0].plot(res['portfolio_value'].values, label=name)\n",
    "    pv = res['portfolio_value']\n",
    "    dd = (pv.cummax() - pv) / pv.cummax()\n",
    "    axes[1].plot(dd.values, label=name, alpha=0.7)\n",
    "    axes[2].plot(pd.Series(res['turnover'].values).rolling(20).mean(), label=name, alpha=0.7)\n",
    "\n",
    "axes[0].set_title('Portfolio Value')\n",
    "axes[0].legend(fontsize=8)\n",
    "axes[1].set_title('Drawdown')\n",
    "axes[1].legend(fontsize=8)\n",
    "axes[2].set_title('Rolling Turnover (20d)')\n",
    "axes[2].legend(fontsize=8)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('Day')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(pd.DataFrame(reward_metrics).T.round(4).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis:**\n",
    "\n",
    "- The `return_dd` reward with lambda=5 most aggressively reduces drawdowns, but at the cost of lower returns. The agent becomes too conservative.\n",
    "- The `sharpe` reward produces moderate turnover and a balanced risk/return profile.\n",
    "- The raw `return` reward leads to the highest turnover as the agent chases short-term returns.\n",
    "- In practice, `return_dd` with lambda in the range 1-3 offers a good balance. The Sharpe-based reward is harder to tune because the rolling window introduces lag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Analysis of Agent Behavior (15 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze PPO agent behavior on test set\n",
    "best_model = ppo_model\n",
    "res, weights = evaluate_agent(best_model, test_returns)\n",
    "\n",
    "# Weight evolution — stacked area chart (top 10 stocks by average weight)\n",
    "weights_df = pd.DataFrame(weights, columns=tickers)\n",
    "top_10 = weights_df.mean().nlargest(10).index.tolist()\n",
    "other = weights_df.drop(columns=top_10).sum(axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "plot_df = weights_df[top_10].copy()\n",
    "plot_df['Other'] = other\n",
    "axes[0].stackplot(range(len(plot_df)), plot_df.values.T,\n",
    "                  labels=plot_df.columns, alpha=0.8)\n",
    "axes[0].set_title('PPO Agent — Portfolio Weight Evolution (Top 10 + Other)')\n",
    "axes[0].set_xlabel('Day')\n",
    "axes[0].set_ylabel('Weight')\n",
    "axes[0].legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=8)\n",
    "axes[0].set_xlim(0, len(plot_df))\n",
    "\n",
    "# Turnover\n",
    "axes[1].plot(res['turnover'].values, alpha=0.5, linewidth=0.8, label='Daily')\n",
    "axes[1].plot(pd.Series(res['turnover'].values).rolling(20).mean(),\n",
    "             color='red', linewidth=2, label='20-day MA')\n",
    "axes[1].set_title('PPO Agent — Daily Turnover')\n",
    "axes[1].set_xlabel('Day')\n",
    "axes[1].set_ylabel('Turnover')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting check: train vs test\n",
    "res_train, _ = evaluate_agent(best_model, train_returns)\n",
    "res_test, _ = evaluate_agent(best_model, test_returns)\n",
    "\n",
    "print(\"PPO — Train vs Test Performance:\")\n",
    "comparison = pd.DataFrame({\n",
    "    'Train': compute_metrics(res_train),\n",
    "    'Test': compute_metrics(res_test),\n",
    "})\n",
    "print(comparison.round(4).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-seed robustness\n",
    "print(\"\\nMulti-seed robustness (PPO, 5 seeds):\")\n",
    "seed_results = []\n",
    "\n",
    "for seed in range(5):\n",
    "    env = DummyVecEnv([lambda: PortfolioEnv(train_returns)])\n",
    "    model = PPO('MlpPolicy', env, learning_rate=3e-4, n_steps=512,\n",
    "                batch_size=128, n_epochs=10, seed=seed, verbose=0)\n",
    "    model.learn(total_timesteps=80_000)\n",
    "    res, _ = evaluate_agent(model, test_returns)\n",
    "    m = compute_metrics(res)\n",
    "    seed_results.append(m)\n",
    "\n",
    "seed_df = pd.DataFrame(seed_results)\n",
    "print(f\"{'Metric':<20} {'Mean':>10} {'Std':>10}\")\n",
    "print('-' * 40)\n",
    "for col in seed_df.columns:\n",
    "    print(f\"{col:<20} {seed_df[col].mean():>10.4f} {seed_df[col].std():>10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Behavioral Analysis:**\n",
    "\n",
    "The PPO agent tends to concentrate positions in a subset of stocks (typically those with the highest recent Sharpe ratios) and adjusts allocations based on recent volatility. Turnover is moderate — the agent does not trade every day but rebalances when return/risk profiles shift. The train/test gap in Sharpe ratio indicates some degree of overfitting, which is expected given that RL agents optimize over the full training trajectory. Multi-seed analysis shows moderate variance, suggesting the agent is learning some signal rather than purely memorizing. Overall, the agent behaves like a risk-aware momentum trader, but should not be deployed without further walk-forward validation and position concentration limits."
   ]
  }
 ]
}