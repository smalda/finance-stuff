{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 11 â€” RL Portfolio Manager\n",
    "\n",
    "**Total: 100 points**\n",
    "\n",
    "Build a reinforcement learning portfolio manager using 30 stocks. Train multiple agents, benchmark them against classical strategies, and analyze agent behavior.\n",
    "\n",
    "| Part | Topic | Points |\n",
    "|------|-------|--------|\n",
    "| 1 | Set up FinRL with 30 stocks | 15 |\n",
    "| 2 | Train PPO, A2C, SAC agents | 25 |\n",
    "| 3 | Benchmark against equal-weight, min-var, best ML model | 25 |\n",
    "| 4 | Reward shaping experiments | 20 |\n",
    "| 5 | Analysis of agent behavior | 15 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Data Setup (15 pts)\n",
    "\n",
    "**Requirements:**\n",
    "- Download daily OHLCV data for 30 stocks (e.g., DJIA constituents) from 2018-01-01 to 2024-01-01\n",
    "- Compute daily returns and basic technical indicators (MACD, RSI, or rolling volatility)\n",
    "- Split: Train on 2018-2022, Test on 2023\n",
    "- Build a `PortfolioEnv` (or use FinRL's `StockTradingEnv`) that:\n",
    "  - Takes the preprocessed data\n",
    "  - Exposes gym-compatible interface\n",
    "  - Supports configurable reward functions\n",
    "\n",
    "**Grading:**\n",
    "- 5 pts: Correct data download and cleaning (no NaNs, proper alignment)\n",
    "- 5 pts: Working gym environment with proper observation/action spaces\n",
    "- 5 pts: Correct train/test split respecting temporal ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggested tickers (DJIA-like universe)\n",
    "tickers = [\n",
    "    'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'JPM', 'JNJ', 'V', 'PG', 'UNH', 'HD',\n",
    "    'MA', 'DIS', 'NVDA', 'BAC', 'ADBE', 'CRM', 'CMCSA', 'XOM', 'CSCO', 'PFE',\n",
    "    'NFLX', 'ABT', 'KO', 'PEP', 'TMO', 'AVGO', 'COST', 'WMT', 'MRK', 'CVX',\n",
    "]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# 1. Download data using yfinance\n",
    "# 2. Compute returns\n",
    "# 3. Add technical indicators\n",
    "# 4. Build or configure environment\n",
    "# 5. Verify with env.reset() and env.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Train PPO, A2C, SAC Agents (25 pts)\n",
    "\n",
    "**Requirements:**\n",
    "- Train three agents using Stable-Baselines3: PPO, A2C, SAC\n",
    "- Use the same environment and reward function for fair comparison\n",
    "- Train each for at least 100K timesteps\n",
    "- Plot training reward curves (use callbacks or log)\n",
    "- Report final train performance for each\n",
    "\n",
    "**Grading:**\n",
    "- 5 pts: Correct PPO setup and training\n",
    "- 5 pts: Correct A2C setup and training\n",
    "- 5 pts: Correct SAC setup and training\n",
    "- 5 pts: Training curves plotted\n",
    "- 5 pts: Reasonable hyperparameter choices (documented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Create training environment\n",
    "# 2. Train PPO with documented hyperparameters\n",
    "# 3. Train A2C with documented hyperparameters\n",
    "# 4. Train SAC with documented hyperparameters\n",
    "# 5. Plot learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Benchmark Comparison (25 pts)\n",
    "\n",
    "**Requirements:**\n",
    "Compare all RL agents against these baselines on the TEST set:\n",
    "1. **Equal-weight** portfolio (rebalanced daily)\n",
    "2. **Minimum-variance** portfolio (using rolling covariance, rebalanced monthly)\n",
    "3. **Best ML model from Week 5** (if available; otherwise use a simple momentum strategy)\n",
    "\n",
    "**Metrics to report:**\n",
    "- Cumulative return\n",
    "- Annualized Sharpe ratio\n",
    "- Maximum drawdown\n",
    "- Annualized volatility\n",
    "- Average daily turnover\n",
    "\n",
    "**Deliverables:**\n",
    "- Cumulative return plot (all strategies on one chart)\n",
    "- Summary metrics table\n",
    "- 2-3 sentence commentary on results\n",
    "\n",
    "**Grading:**\n",
    "- 5 pts: Correct equal-weight baseline\n",
    "- 5 pts: Correct min-variance baseline\n",
    "- 5 pts: ML/momentum baseline\n",
    "- 5 pts: Clean comparison plot and table\n",
    "- 5 pts: Thoughtful commentary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Implement equal-weight baseline\n",
    "# 2. Implement min-variance baseline\n",
    "# 3. Implement momentum or ML baseline\n",
    "# 4. Evaluate all RL agents on test set\n",
    "# 5. Create comparison table and plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Reward Shaping Experiments (20 pts)\n",
    "\n",
    "**Requirements:**\n",
    "Using PPO, train agents with at least 3 different reward functions:\n",
    "1. Raw portfolio return: $r_t = r_p$\n",
    "2. Sharpe-based: $r_t = \\bar{r}_{20} / \\sigma_{20}$\n",
    "3. Return minus drawdown penalty: $r_t = r_p - \\lambda \\cdot \\text{drawdown}_t$\n",
    "\n",
    "For reward 3, try at least 2 values of $\\lambda$ (e.g., 1.0 and 5.0).\n",
    "\n",
    "**Deliverables:**\n",
    "- Test performance comparison across reward functions\n",
    "- How does the reward function affect turnover?\n",
    "- How does it affect drawdown behavior?\n",
    "- Which reward function would you use in practice? Why?\n",
    "\n",
    "**Grading:**\n",
    "- 5 pts: Correct implementation of 3+ reward functions\n",
    "- 5 pts: Lambda sensitivity analysis\n",
    "- 5 pts: Comparison plots and metrics\n",
    "- 5 pts: Thoughtful analysis of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Implement reward functions in environment\n",
    "# 2. Train PPO with each reward\n",
    "# 3. Compare on test set\n",
    "# 4. Analyze turnover and drawdown behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Analysis of Agent Behavior (15 pts)\n",
    "\n",
    "**Requirements:**\n",
    "Pick your best-performing RL agent and analyze its behavior:\n",
    "1. **Weight evolution**: Plot how portfolio weights change over time on the test set. Does the agent concentrate or diversify?\n",
    "2. **Turnover analysis**: Plot daily turnover. Does the agent trade too much?\n",
    "3. **Regime behavior**: How does the agent behave during volatile vs calm periods?\n",
    "4. **Overfitting check**: Report train vs test metrics. Train with 3 different seeds and report variance.\n",
    "\n",
    "**Deliverables:**\n",
    "- Weight evolution plot (stacked area chart)\n",
    "- Turnover time series\n",
    "- Train vs test comparison table\n",
    "- Multi-seed results (mean +/- std)\n",
    "- 3-5 sentences: Does this agent behave like a reasonable portfolio manager?\n",
    "\n",
    "**Grading:**\n",
    "- 5 pts: Weight and turnover visualizations\n",
    "- 5 pts: Overfitting analysis (seeds, train/test gap)\n",
    "- 5 pts: Behavioral interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Run best agent on test set, record weights at each step\n",
    "# 2. Create stacked area chart of weights\n",
    "# 3. Plot turnover over time\n",
    "# 4. Train with 3+ seeds, report mean/std\n",
    "# 5. Write analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Submission\n",
    "\n",
    "Submit this notebook with:\n",
    "- All code cells executed\n",
    "- All plots rendered\n",
    "- All commentary/analysis written in markdown cells\n",
    "- Training should be reproducible (set random seeds)"
   ]
  }
 ]
}