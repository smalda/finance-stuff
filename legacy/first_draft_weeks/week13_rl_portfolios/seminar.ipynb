{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 11 Seminar — RL for Portfolio Management (Hands-On)\n",
    "\n",
    "**Exercises:**\n",
    "1. Set up a FinRL environment with 10 stocks, train PPO (25 min)\n",
    "2. Custom reward functions: return, Sharpe-based, return - lambda * drawdown (25 min)\n",
    "3. Compare PPO vs A2C vs DDPG (20 min)\n",
    "4. Discussion: How to evaluate if RL agent learned vs overfit (20 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO, A2C, DDPG\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "import yfinance as yf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 1: Set Up Environment with 10 Stocks, Train PPO (25 min)\n",
    "\n",
    "We'll build a clean portfolio environment that works with real stock data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download 10 stocks\n",
    "tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'JPM',\n",
    "           'JNJ', 'XOM', 'PG', 'NVDA', 'V']\n",
    "\n",
    "print(\"Downloading stock data...\")\n",
    "data = yf.download(tickers, start='2019-01-01', end='2024-01-01',\n",
    "                   auto_adjust=True, progress=False)\n",
    "prices = data['Close'][tickers].dropna()\n",
    "returns = prices.pct_change().dropna().values\n",
    "\n",
    "print(f\"Data shape: {returns.shape}  ({returns.shape[0]} days, {returns.shape[1]} stocks)\")\n",
    "print(f\"Date range: {prices.index[1].date()} to {prices.index[-1].date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioEnv(gym.Env):\n",
    "    \"\"\"Portfolio allocation environment with configurable reward.\"\"\"\n",
    "    \n",
    "    def __init__(self, returns_data, lookback=20, transaction_cost=0.001,\n",
    "                 reward_type='return'):\n",
    "        super().__init__()\n",
    "        self.returns_data = returns_data\n",
    "        self.n_assets = returns_data.shape[1]\n",
    "        self.lookback = lookback\n",
    "        self.tc = transaction_cost\n",
    "        self.reward_type = reward_type\n",
    "        \n",
    "        obs_dim = 3 * self.n_assets  # weights + mean_ret + vol\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32\n",
    "        )\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-1, high=1, shape=(self.n_assets,), dtype=np.float32\n",
    "        )\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.t = self.lookback\n",
    "        self.weights = np.ones(self.n_assets) / self.n_assets\n",
    "        self.portfolio_value = 1.0\n",
    "        self.peak_value = 1.0\n",
    "        self.returns_history = []\n",
    "        self.history = []\n",
    "        return self._get_obs(), {}\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        recent = self.returns_data[self.t - self.lookback:self.t]\n",
    "        mean_ret = recent.mean(axis=0)\n",
    "        vol = recent.std(axis=0) + 1e-8\n",
    "        return np.concatenate([self.weights, mean_ret, vol]).astype(np.float32)\n",
    "    \n",
    "    def _softmax(self, x):\n",
    "        e = np.exp(x - x.max())\n",
    "        return e / e.sum()\n",
    "    \n",
    "    def _compute_reward(self, port_return):\n",
    "        if self.reward_type == 'return':\n",
    "            return port_return\n",
    "        \n",
    "        elif self.reward_type == 'sharpe':\n",
    "            self.returns_history.append(port_return)\n",
    "            if len(self.returns_history) < 20:\n",
    "                return port_return\n",
    "            recent = np.array(self.returns_history[-20:])\n",
    "            return recent.mean() / (recent.std() + 1e-8)\n",
    "        \n",
    "        elif self.reward_type == 'return_dd':\n",
    "            self.peak_value = max(self.peak_value, self.portfolio_value)\n",
    "            drawdown = (self.peak_value - self.portfolio_value) / self.peak_value\n",
    "            lam = 2.0\n",
    "            return port_return - lam * drawdown\n",
    "        \n",
    "        return port_return\n",
    "    \n",
    "    def step(self, action):\n",
    "        new_weights = self._softmax(action)\n",
    "        turnover = np.abs(new_weights - self.weights).sum()\n",
    "        tc_cost = self.tc * turnover\n",
    "        \n",
    "        asset_returns = self.returns_data[self.t]\n",
    "        port_return = np.dot(new_weights, asset_returns) - tc_cost\n",
    "        \n",
    "        self.portfolio_value *= (1 + port_return)\n",
    "        self.weights = new_weights * (1 + asset_returns)\n",
    "        self.weights /= self.weights.sum()\n",
    "        self.t += 1\n",
    "        \n",
    "        reward = self._compute_reward(port_return)\n",
    "        \n",
    "        self.history.append({\n",
    "            'portfolio_value': self.portfolio_value,\n",
    "            'return': port_return,\n",
    "            'turnover': turnover,\n",
    "        })\n",
    "        \n",
    "        terminated = self.t >= len(self.returns_data)\n",
    "        return self._get_obs(), reward, terminated, False, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split: 80/20\n",
    "split = int(0.8 * len(returns))\n",
    "train_returns = returns[:split]\n",
    "test_returns = returns[split:]\n",
    "\n",
    "print(f\"Train: {train_returns.shape[0]} days\")\n",
    "print(f\"Test:  {test_returns.shape[0]} days\")\n",
    "\n",
    "# Train PPO\n",
    "train_env = DummyVecEnv([lambda: PortfolioEnv(train_returns, reward_type='return')])\n",
    "\n",
    "ppo_model = PPO(\n",
    "    'MlpPolicy', train_env,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=256,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "print(\"Training PPO...\")\n",
    "ppo_model.learn(total_timesteps=100_000)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: run agent on test data\n",
    "def evaluate_agent(model, test_data, reward_type='return', deterministic=True):\n",
    "    env = PortfolioEnv(test_data, reward_type=reward_type)\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        if model is None:  # equal weight baseline\n",
    "            action = np.zeros(env.n_assets)\n",
    "        else:\n",
    "            action, _ = model.predict(obs, deterministic=deterministic)\n",
    "        obs, _, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "    return pd.DataFrame(env.history)\n",
    "\n",
    "def compute_metrics(results):\n",
    "    rets = results['return'].values\n",
    "    return {\n",
    "        'Total Return': results['portfolio_value'].iloc[-1] - 1,\n",
    "        'Sharpe': np.sqrt(252) * rets.mean() / (rets.std() + 1e-8),\n",
    "        'Max Drawdown': ((results['portfolio_value'].cummax() - results['portfolio_value']) \n",
    "                         / results['portfolio_value'].cummax()).max(),\n",
    "        'Avg Turnover': results['turnover'].mean(),\n",
    "    }\n",
    "\n",
    "# Evaluate\n",
    "res_ppo = evaluate_agent(ppo_model, test_returns)\n",
    "res_ew = evaluate_agent(None, test_returns)\n",
    "\n",
    "plt.plot(res_ppo['portfolio_value'].values, label='PPO')\n",
    "plt.plot(res_ew['portfolio_value'].values, label='Equal Weight')\n",
    "plt.title('PPO vs Equal Weight (Test Set)')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Portfolio Value')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "metrics = pd.DataFrame({\n",
    "    'PPO': compute_metrics(res_ppo),\n",
    "    'Equal Weight': compute_metrics(res_ew),\n",
    "})\n",
    "print(metrics.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 2: Custom Reward Functions (25 min)\n",
    "\n",
    "Train PPO with three different reward signals and compare behavior.\n",
    "\n",
    "| Reward | Formula |\n",
    "|--------|---------|\n",
    "| `return` | $r_t = r_p$ |\n",
    "| `sharpe` | $r_t = \\bar{r}_{20} / \\sigma_{20}$ |\n",
    "| `return_dd` | $r_t = r_p - 2 \\cdot \\text{drawdown}_t$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train agents with different reward functions\n",
    "reward_types = ['return', 'sharpe', 'return_dd']\n",
    "models = {}\n",
    "\n",
    "for rtype in reward_types:\n",
    "    print(f\"Training PPO with reward='{rtype}'...\")\n",
    "    env = DummyVecEnv([lambda rt=rtype: PortfolioEnv(train_returns, reward_type=rt)])\n",
    "    model = PPO(\n",
    "        'MlpPolicy', env,\n",
    "        learning_rate=3e-4,\n",
    "        n_steps=256,\n",
    "        batch_size=64,\n",
    "        n_epochs=10,\n",
    "        gamma=0.99,\n",
    "        verbose=0,\n",
    "    )\n",
    "    model.learn(total_timesteps=80_000)\n",
    "    models[rtype] = model\n",
    "    print(f\"  Done.\")\n",
    "\n",
    "print(\"\\nAll models trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all on test set\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "all_metrics = {}\n",
    "for rtype, model in models.items():\n",
    "    res = evaluate_agent(model, test_returns)\n",
    "    axes[0].plot(res['portfolio_value'].values, label=f'PPO ({rtype})')\n",
    "    all_metrics[f'PPO ({rtype})'] = compute_metrics(res)\n",
    "\n",
    "res_ew = evaluate_agent(None, test_returns)\n",
    "axes[0].plot(res_ew['portfolio_value'].values, label='Equal Weight', linestyle='--', color='black')\n",
    "all_metrics['Equal Weight'] = compute_metrics(res_ew)\n",
    "\n",
    "axes[0].set_title('Portfolio Value by Reward Type')\n",
    "axes[0].set_xlabel('Day')\n",
    "axes[0].set_ylabel('Portfolio Value')\n",
    "axes[0].legend()\n",
    "\n",
    "# Drawdown comparison\n",
    "for rtype, model in models.items():\n",
    "    res = evaluate_agent(model, test_returns)\n",
    "    pv = res['portfolio_value']\n",
    "    dd = (pv.cummax() - pv) / pv.cummax()\n",
    "    axes[1].plot(dd.values, label=f'PPO ({rtype})', alpha=0.7)\n",
    "\n",
    "axes[1].set_title('Drawdown by Reward Type')\n",
    "axes[1].set_xlabel('Day')\n",
    "axes[1].set_ylabel('Drawdown')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(pd.DataFrame(all_metrics).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "- **Raw return** reward: agent tends to concentrate positions (higher risk, higher potential return)\n",
    "- **Sharpe-based** reward: agent trades less, more diversified\n",
    "- **Return - drawdown** reward: agent actively avoids drawdowns, sometimes at cost of return\n",
    "\n",
    "**Key insight:** The reward function is the most important design choice in financial RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 3: Compare PPO vs A2C vs DDPG (20 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train A2C\n",
    "print(\"Training A2C...\")\n",
    "a2c_env = DummyVecEnv([lambda: PortfolioEnv(train_returns, reward_type='return')])\n",
    "a2c_model = A2C(\n",
    "    'MlpPolicy', a2c_env,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=128,\n",
    "    gamma=0.99,\n",
    "    verbose=0,\n",
    ")\n",
    "a2c_model.learn(total_timesteps=80_000)\n",
    "print(\"Done.\")\n",
    "\n",
    "# Train DDPG\n",
    "print(\"Training DDPG...\")\n",
    "ddpg_env = DummyVecEnv([lambda: PortfolioEnv(train_returns, reward_type='return')])\n",
    "n_actions = train_returns.shape[1]\n",
    "action_noise = NormalActionNoise(\n",
    "    mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions)\n",
    ")\n",
    "ddpg_model = DDPG(\n",
    "    'MlpPolicy', ddpg_env,\n",
    "    learning_rate=1e-3,\n",
    "    action_noise=action_noise,\n",
    "    buffer_size=50_000,\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    verbose=0,\n",
    ")\n",
    "ddpg_model.learn(total_timesteps=80_000)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all algorithms\n",
    "algo_models = {\n",
    "    'PPO': models['return'],\n",
    "    'A2C': a2c_model,\n",
    "    'DDPG': ddpg_model,\n",
    "}\n",
    "\n",
    "algo_metrics = {}\n",
    "for name, model in algo_models.items():\n",
    "    res = evaluate_agent(model, test_returns)\n",
    "    plt.plot(res['portfolio_value'].values, label=name)\n",
    "    algo_metrics[name] = compute_metrics(res)\n",
    "\n",
    "res_ew = evaluate_agent(None, test_returns)\n",
    "plt.plot(res_ew['portfolio_value'].values, label='Equal Weight', linestyle='--', color='black')\n",
    "algo_metrics['Equal Weight'] = compute_metrics(res_ew)\n",
    "\n",
    "plt.title('Algorithm Comparison (Test Set)')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Portfolio Value')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(pd.DataFrame(algo_metrics).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typical observations\n",
    "\n",
    "- **PPO**: Most stable training, consistent results across runs\n",
    "- **A2C**: Faster training but higher variance between runs\n",
    "- **DDPG**: Can learn good policies but sensitive to hyperparameters\n",
    "- **Equal weight** is often competitive — a humbling but important baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 4: Discussion — Learned or Overfit? (20 min)\n",
    "\n",
    "### How to tell if your RL agent learned something real:\n",
    "\n",
    "**1. Train vs Test performance gap**\n",
    "- If train Sharpe >> test Sharpe, the agent memorized the training data\n",
    "- Healthy gap: < 30% degradation\n",
    "\n",
    "**2. Robustness checks**\n",
    "- Train on multiple time periods — does the agent behave consistently?\n",
    "- Shuffle training data order — does it still learn?\n",
    "- Add noise to observations — graceful degradation?\n",
    "\n",
    "**3. Behavioral analysis**\n",
    "- Does the agent's allocation make economic sense?\n",
    "- Does it trade too much? (High turnover = overfitting to noise)\n",
    "- Does it concentrate in one stock? (Probably memorized that stock's run)\n",
    "\n",
    "**4. Multiple seeds**\n",
    "- Train 5-10 agents with different random seeds\n",
    "- Report mean +/- std of metrics\n",
    "- If variance across seeds > signal, you're overfitting\n",
    "\n",
    "**5. Walk-forward validation**\n",
    "- Train on 2019-2020, test on 2021\n",
    "- Train on 2019-2021, test on 2022\n",
    "- Train on 2019-2022, test on 2023\n",
    "- Look for consistent out-of-sample performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick robustness check: train vs test performance\n",
    "res_train = evaluate_agent(models['return'], train_returns)\n",
    "res_test = evaluate_agent(models['return'], test_returns)\n",
    "\n",
    "train_metrics = compute_metrics(res_train)\n",
    "test_metrics = compute_metrics(res_test)\n",
    "\n",
    "comparison = pd.DataFrame({'Train': train_metrics, 'Test': test_metrics})\n",
    "print(\"PPO (return reward) — Train vs Test:\")\n",
    "print(comparison.round(4))\n",
    "print(f\"\\nSharpe degradation: {(1 - test_metrics['Sharpe'] / (train_metrics['Sharpe'] + 1e-8)):.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-seed robustness\n",
    "print(\"Training PPO with 5 different seeds...\")\n",
    "seed_results = []\n",
    "\n",
    "for seed in range(5):\n",
    "    env = DummyVecEnv([lambda: PortfolioEnv(train_returns)])\n",
    "    model = PPO('MlpPolicy', env, learning_rate=3e-4, n_steps=256,\n",
    "                batch_size=64, seed=seed, verbose=0)\n",
    "    model.learn(total_timesteps=50_000)\n",
    "    res = evaluate_agent(model, test_returns)\n",
    "    m = compute_metrics(res)\n",
    "    seed_results.append(m)\n",
    "    print(f\"  Seed {seed}: Sharpe={m['Sharpe']:.3f}, Return={m['Total Return']:.3%}\")\n",
    "\n",
    "seed_df = pd.DataFrame(seed_results)\n",
    "print(f\"\\nMean Sharpe: {seed_df['Sharpe'].mean():.3f} +/- {seed_df['Sharpe'].std():.3f}\")\n",
    "print(f\"Mean Return: {seed_df['Total Return'].mean():.3%} +/- {seed_df['Total Return'].std():.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "1. Built a reusable `PortfolioEnv` with configurable reward functions\n",
    "2. Reward design is the single most important choice in financial RL\n",
    "3. PPO is the most reliable algorithm; DDPG can work but needs tuning\n",
    "4. Always check for overfitting: train/test gap, multi-seed, behavioral analysis\n",
    "5. Equal weight remains a tough baseline to beat consistently"
   ]
  }
 ]
}