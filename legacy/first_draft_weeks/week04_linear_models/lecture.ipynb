{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 — Cross-Sectional Return Prediction: Linear Models\n",
    "\n",
    "> *\"An R-squared of 0.4% on 3,000 stocks, rebalanced monthly, translates to a long-short portfolio with a Sharpe ratio above 2. That's better than 99% of hedge funds. Welcome to the economics of cross-sectional prediction, where tiny edges compound across enormous breadth.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2020, Shihao Gu, Bryan Kelly, and Dacheng Xiu published a paper in the *Review of Financial Studies* that ended a decades-long debate: can machine learning predict stock returns? They tested eight model classes — from ordinary least squares to deep neural networks — on 30,000 US stocks over 60 years, using 94 firm characteristics as features. The answer was yes. Neural networks achieved an out-of-sample monthly R-squared of roughly 0.4%.\n",
    "\n",
    "Before you scoff at that number, let's do some math. The Fundamental Law of Active Management (Week 3) says your information ratio is IC times the square root of breadth. An R-squared of 0.4% on 3,000 stocks, rebalanced monthly, translates to an IC around 0.05 and annual breadth of roughly 36,000 stock-months. That gives you an information ratio above 2 — which means a long-short portfolio Sharpe above 2. That's better than virtually every discretionary hedge fund on the planet. And the model \"explains\" less than half a percent of return variation.\n",
    "\n",
    "Here's the real surprise from the Gu-Kelly-Xiu paper: the improvement from neural networks over regularized linear models was modest. Elastic Net achieved an R-squared of ~0.2%. Neural networks got ~0.4%. The gap is real but not enormous. Most of the predictive power came from the *features*, not the model architecture. The model was a second-order effect.\n",
    "\n",
    "Everything you've learned so far has been building toward this moment. You have clean data (Week 1). You know how to make it stationary (Week 2). You understand what alpha means, how to measure it with the IC, and how the Fundamental Law turns tiny signal into portfolio performance (Week 3). Today, we put it all together. We build the cross-sectional prediction pipeline that is the bread and butter of quantitative asset management — and we start, deliberately, with the simplest models. Because if Ridge regression can produce meaningful out-of-sample signal with one hyperparameter, that tells you something profound about the structure of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "plt.rcParams.update({'figure.figsize': (10, 5), 'figure.dpi': 100,\n",
    "                      'axes.spines.top': False, 'axes.spines.right': False,\n",
    "                      'font.size': 11})\n",
    "sns.set_palette('colorblind')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our standard toolkit loaded, we're ready to build. We'll use `yfinance` for price and volume data, `scikit-learn` for the linear models, and `scipy` for the rank correlations that define our evaluation metric. Everything from here forward follows a clean pipeline: download data, engineer features, train models, evaluate with expanding-window cross-validation, and interpret results.\n",
    "\n",
    "Let's start by downloading a representative universe of stocks. We're using 30 well-known large-cap and mid-cap names — not the full S&P 500 (that would take a while), but enough to illustrate every concept. In the homework, you'll scale this to 200+ stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = ['AAPL', 'MSFT', 'AMZN', 'GOOGL', 'META', 'NVDA', 'JPM', 'JNJ',\n",
    "           'V', 'PG', 'UNH', 'HD', 'DIS', 'BAC', 'XOM', 'PFE', 'KO',\n",
    "           'CSCO', 'PEP', 'ABT', 'MRK', 'INTC', 'WMT', 'T', 'CVX',\n",
    "           'NEE', 'MDT', 'LIN', 'TXN', 'QCOM']\n",
    "\n",
    "raw = yf.download(tickers, start='2010-01-01', end='2024-01-01',\n",
    "                  auto_adjust=True, progress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have daily price and volume data for 30 stocks spanning 14 years — roughly 3,500 trading days each. That's enough cross-sectional breadth and time-series depth to demonstrate every concept in this lecture. Notice that we're pulling well-established companies with long trading histories; in the homework, you'll also deal with stocks that have shorter histories, more missing data, and wilder behavior.\n",
    "\n",
    "Let's extract what we need: closing prices and daily returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close = raw['Close'].dropna(how='all')\n",
    "volume = raw['Volume'].dropna(how='all')\n",
    "returns_daily = close.pct_change()\n",
    "display(Markdown(f\"**Universe:** {close.shape[1]} stocks, \"\n",
    "                 f\"{close.shape[0]} trading days\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. The Cross-Sectional Prediction Problem\n",
    "\n",
    "Here's a question that separates ML engineers from quant researchers: \"Predict whether Apple stock will go up next month.\" An ML engineer starts thinking about LSTMs, attention mechanisms, and historical price sequences. A quant researcher says: *wrong question.*\n",
    "\n",
    "The right question is: \"Will Apple outperform the median stock next month?\" This reframing — from absolute to relative prediction — is the most important conceptual shift in this entire course. It changes the loss function, the features, and the evaluation metric. And it makes the problem dramatically more tractable.\n",
    "\n",
    "Think about why. If you're predicting Apple's absolute return, you need to forecast the macroeconomy, interest rates, consumer spending, the iPhone cycle, and whatever Tim Cook had for breakfast. If you're predicting Apple's *relative* return — whether it beats the average stock — you only need to know whether Apple-specific factors (momentum, valuation, volatility) are more favorable than average. The macro stuff cancels out because it affects all stocks similarly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, the cross-sectional prediction problem at time $t$ is:\n",
    "\n",
    "Given features $\\mathbf{x}_{i,t}$ for stocks $i = 1, \\dots, N_t$, predict *excess returns*:\n",
    "\n",
    "$$r_{i,t+1}^{e} = r_{i,t+1} - r_{m,t+1}$$\n",
    "\n",
    "where $r_{m,t+1}$ is the market return (the average return across all stocks). The model:\n",
    "\n",
    "$$\\hat{r}_{i,t+1}^{e} = f(\\mathbf{x}_{i,t};\\; \\theta_t)$$\n",
    "\n",
    "We evaluate with the **Information Coefficient** — the rank correlation between predictions and realized returns:\n",
    "\n",
    "$$IC_t = \\text{Spearman}\\!\\left(\\hat{r}_{i,t+1}^{e},\\; r_{i,t+1}^{e}\\right)$$\n",
    "\n",
    "If you're an ML engineer, this is just Spearman's $\\rho$ between your predictions and labels. In finance, it has a name and a mystique, but it's the same thing. An IC of 0.05 — a rank correlation of 5% — is considered *excellent*. In image classification, 5% correlation would be garbage. In finance, it's a career."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what this data structure looks like in practice. The cross-sectional panel is a DataFrame indexed by (date, ticker), with columns for features and a target column for next-month excess return. This is the data format that every model in this course will consume — from the Ridge regression we build today through the neural networks of Week 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_close = close.resample('ME').last()\n",
    "monthly_ret = monthly_close.pct_change()\n",
    "market_ret = monthly_ret.mean(axis=1)\n",
    "excess_ret = monthly_ret.sub(market_ret, axis=0)\n",
    "\n",
    "panel_example = excess_ret.stack().reset_index()\n",
    "panel_example.columns = ['date', 'ticker', 'excess_return']\n",
    "display(panel_example.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That stacked format — every row is one stock in one month — is the canonical shape of cross-sectional data. At each date, you have $N$ stocks, each with an excess return. When you stack these cross-sections over time, you get a panel with $(N \\times T)$ rows. For 30 stocks over 14 years of monthly data, that's roughly 5,000 rows. For 3,000 stocks over 60 years (the Gu-Kelly-Xiu scale), it's over 2 million rows. The models are the same — only the scale changes.\n",
    "\n",
    "Now let's populate this panel with the features that decades of research have shown drive the cross-section of stock returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Feature Engineering — The Factors That Move Stocks\n",
    "\n",
    "In 1993, Narasimhan Jegadeesh and Sheridan Titman published one of the most influential papers in finance: stocks that went up in the past 12 months tend to keep going up, and stocks that went down tend to keep going down. This is the **momentum anomaly**, and it's been replicated in 40+ countries, across asset classes, and over centuries of data. The paper has over 12,000 citations. It remains the single most robust stock market anomaly ever documented.\n",
    "\n",
    "Your ML model will rediscover momentum. It has no choice — momentum is the strongest single predictor in the cross-section. But the feature set goes deeper. Decades of academic research have identified the characteristics that predict stock returns: momentum, value, size, volatility, quality, and liquidity. These aren't arbitrary choices — each represents a *reason* why some stocks outperform others. Momentum works because investors underreact to news. Value works (sometimes) because investors overpay for exciting growth stories. The low-volatility anomaly works because institutional investors chase high-beta stocks to hit their return targets, bidding them above fair value.\n",
    "\n",
    "> **Did You Know?** As of 2024, academic researchers have published over 400 \"factors\" that supposedly predict stock returns. Harvey, Liu & Zhu (2016) showed that with 400 factors tested, the standard statistical bar (t-stat > 2) is far too low — you'd expect 20 false positives by chance alone. The true threshold should be t > 3 or higher. This \"factor zoo\" problem means most published anomalies are probably noise. The handful that survive rigorous testing — momentum, value, quality — are the ones we'll focus on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard momentum feature uses a \"skip-1\" convention:\n",
    "\n",
    "$$\\text{MOM}_{12,1}(i, t) = \\frac{P_{i,t-1}}{P_{i,t-13}} - 1$$\n",
    "\n",
    "Why skip the most recent month? Because the most recent month shows *reversal* — recent losers tend to bounce back, and recent winners tend to fade. Momentum and reversal are opposite effects operating at different timescales. If you include the recent month, you're mixing a positive signal (12-month momentum) with a negative one (1-month reversal). Separating them gives the model two clean features instead of one noisy one.\n",
    "\n",
    "Volatility is also predictive — but in the *wrong* direction. High-volatility stocks tend to underperform, not outperform. This \"low-volatility anomaly\" violates the basic finance textbook claim that higher risk earns higher return. It persists because institutional investors are constrained to hit return targets, so they pile into high-beta stocks, bidding them above fair value.\n",
    "\n",
    "Let's compute the momentum family first — the features that carry the most cross-sectional signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mom_1m = monthly_close.pct_change(1)\n",
    "mom_3m = monthly_close.pct_change(3)\n",
    "mom_6m = monthly_close.pct_change(6)\n",
    "mom_12m_skip1 = monthly_close.shift(1).pct_change(11)\n",
    "reversal = mom_1m.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've built the momentum features using monthly prices. Notice the `shift(1).pct_change(11)` pattern for 12-month momentum with the skip — we shift the prices by one month (to exclude the most recent month) and then compute the 11-month return. That gives us the return from month $t-12$ to month $t-1$, exactly the Jegadeesh-Titman specification. The reversal feature is simply the prior month's return, entered as a separate feature so the model can learn its negative relationship with forward returns independently of the positive momentum signal.\n",
    "\n",
    "Now let's add volatility, volume, and size — the features that capture risk, liquidity, and scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_20d = returns_daily.rolling(20).std().resample('ME').last() * np.sqrt(252)\n",
    "vol_60d = returns_daily.rolling(60).std().resample('ME').last() * np.sqrt(252)\n",
    "log_price = np.log(monthly_close)\n",
    "dollar_vol = (close * volume).rolling(20).mean().resample('ME').last()\n",
    "log_dollar_vol = np.log(dollar_vol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using log-price as a size proxy since `yfinance` doesn't provide shares outstanding directly. It's imperfect — a $500 stock isn't necessarily a larger company than a $100 stock — but it correlates with market cap reasonably well for a lecture demo. In the homework, you'll compute proper log market cap. Log dollar volume captures liquidity — stocks that trade more dollars per day are easier to enter and exit without moving the price.\n",
    "\n",
    "Let's add two more technical features and then assemble the full matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_50 = close.rolling(50).mean()\n",
    "ma_200 = close.rolling(200).mean()\n",
    "ma_ratio = (ma_50 / ma_200).resample('ME').last()\n",
    "vol_adj_mom = mom_6m / vol_20d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The moving average ratio captures whether a stock is trending above or below its long-term average — when MA50 > MA200, the stock is in an uptrend. Volatility-adjusted momentum divides the raw 6-month return by its volatility, producing a Sharpe-like ratio that normalizes for risk. A stock that gained 20% with 10% volatility is a stronger momentum signal than one that gained 20% with 40% volatility.\n",
    "\n",
    "Now let's stack everything into the panel format our models need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict = {\n",
    "    'mom_1m': mom_1m, 'mom_3m': mom_3m, 'mom_6m': mom_6m,\n",
    "    'mom_12m_skip1': mom_12m_skip1, 'reversal': reversal,\n",
    "    'vol_20d': vol_20d, 'vol_60d': vol_60d, 'log_price': log_price,\n",
    "    'log_dollar_vol': log_dollar_vol, 'ma_ratio': ma_ratio,\n",
    "    'vol_adj_mom': vol_adj_mom,\n",
    "}\n",
    "\n",
    "feat_frames = []\n",
    "for name, df in features_dict.items():\n",
    "    s = df.stack(); s.name = name\n",
    "    feat_frames.append(s)\n",
    "\n",
    "features = pd.concat(feat_frames, axis=1)\n",
    "features.index.names = ['date', 'ticker']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've assembled 11 features for 30 stocks at monthly frequency. The MultiIndex `(date, ticker)` is the standard structure for cross-sectional panels: each row is one stock in one month. Let's inspect the shape, check for missing data, and look at the feature correlations — because all three will shape how the models behave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"**Panel shape:** {features.shape[0]:,} rows \"\n",
    "                 f\"({features.index.get_level_values('ticker').nunique()} stocks \"\n",
    "                 f\"x {features.index.get_level_values('date').nunique()} months)\"))\n",
    "display(Markdown(\"**Missing data by feature:**\"))\n",
    "display((features.isna().mean() * 100).round(1).to_frame('% missing'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the missing data pattern: features that require long lookback windows — like 12-month momentum and the moving average ratio (which needs 200 daily observations) — have more missing values at the start of the sample. The 20-day volatility fills in quickly, but 12-month momentum needs a full year of history before it can produce its first value. In the homework, where you'll work with 200+ stocks including some that were listed recently, the missing data problem is substantially worse. Cross-sectional median imputation — replacing NaN with the median feature value across all stocks at that date — is the standard approach at most quant funds.\n",
    "\n",
    "These features are also *correlated* with each other, and that correlation is about to cause problems for OLS. Let's see how correlated they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = features.dropna().corr()\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(corr, annot=True, fmt='.2f', cmap='RdBu_r', center=0,\n",
    "            vmin=-1, vmax=1, ax=ax, square=True, cbar_kws={'shrink': .7})\n",
    "ax.set_title('Feature Correlation Matrix')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study the blocks of high correlation. The momentum features cluster together — 1m, 3m, 6m, and 12m momentum are all measuring the same underlying tendency for winners to keep winning, just over different horizons. The volatility features are nearly identical (20-day and 60-day realized vol are computed from heavily overlapping windows). Volume correlates with size, because big companies trade more.\n",
    "\n",
    "When OLS tries to disentangle these correlated signals, it gives one momentum feature a coefficient of +50 and another -47 — the net effect is modest, but the individual coefficients are absurd and unstable. Add one month of new data and the signs can flip entirely. This is multicollinearity in action, and it's the disease that Ridge regression was invented to cure.\n",
    "\n",
    "Let's assemble the full panel with our target variable — next-month excess return — and apply the standard preprocessing before we train models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = excess_ret.shift(-1).stack()\n",
    "target.name = 'target'\n",
    "target.index.names = ['date', 'ticker']\n",
    "\n",
    "panel = features.join(target, how='inner').dropna()\n",
    "display(Markdown(f\"**Clean panel:** {len(panel):,} rows, \"\n",
    "                 f\"{panel.shape[1] - 1} features + 1 target\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target is the forward excess return — what each stock earned relative to the market in the *following* month. The `shift(-1)` ensures that at date $t$, the target is the return from $t$ to $t+1$. This is the standard label for cross-sectional prediction: we use features known at $t$ to predict the return realized at $t+1$.\n",
    "\n",
    "> **Did You Know?** Cross-sectional rank normalization — mapping each feature to the [0, 1] interval across stocks at each date — is used at virtually every quant fund and is so standard that academic papers don't even bother mentioning it. It removes outliers (a stock that rose 300% won't dominate the regression), makes features comparable (market cap and momentum are both on [0, 1]), and makes the model robust to distributional shifts over time. Think of it as the quantitative finance equivalent of batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cols = [c for c in panel.columns if c != 'target']\n",
    "\n",
    "def rank_normalize(group):\n",
    "    return group[feat_cols].rank(pct=True)\n",
    "\n",
    "panel[feat_cols] = panel.groupby(level='date', group_keys=False).apply(\n",
    "    rank_normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each feature is now rank-normalized cross-sectionally: at every date, every stock's feature value is mapped to its percentile rank among all stocks at that date. The stock with the highest momentum gets 1.0, the lowest gets approximately 0.0, and everything else is linearly spaced between. This removes the influence of outliers, makes features directly comparable, and ensures the model is robust to regime changes where the *levels* of features shift but the *ranks* remain meaningful.\n",
    "\n",
    "Our data pipeline is complete. We have a clean panel of 11 rank-normalized features and a forward excess return target. Time to train some models.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Linear Models — OLS, Ridge, Lasso, and Elastic Net\n",
    "\n",
    "In the Gu-Kelly-Xiu study, OLS — ordinary least squares, no regularization — had the *worst* out-of-sample performance of all eight model classes. Worse than random forests. Worse than neural nets. Even worse than partial least squares. OLS overfit spectacularly, fitting noise in 94 correlated features as if it were signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression, with a single hyperparameter controlling the penalty, nearly doubled OLS's out-of-sample R-squared. One hyperparameter. That's the lesson: in financial prediction, where signal-to-noise is extremely low, regularization isn't optional — it's the difference between a model that works and one that hallucinates.\n",
    "\n",
    "Here's the intuition. OLS finds the $\\boldsymbol{\\beta}$ that minimizes the sum of squared errors — period. If two features are correlated, OLS is free to give one a coefficient of +100 and the other -100, as long as the net prediction is good *in sample*. That's a terrible strategy when the correlations shift even slightly out of sample. Ridge adds a penalty for coefficient magnitude:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{Ridge}} = \\underbrace{\\sum_{i=1}^{N} (r_i - \\mathbf{x}_i^T \\boldsymbol{\\beta})^2}_{\\text{fit the data}} + \\underbrace{\\alpha \\|\\boldsymbol{\\beta}\\|_2^2}_{\\text{keep coefficients small}}$$\n",
    "\n",
    "The analytical solution:\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\beta}}_{\\text{Ridge}} = (\\mathbf{X}^T\\mathbf{X} + \\alpha \\mathbf{I})^{-1} \\mathbf{X}^T \\mathbf{r}$$\n",
    "\n",
    "Compare with OLS: $\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{r}$. The only difference is $\\alpha\\mathbf{I}$ — a small diagonal addition that stabilizes the matrix inversion. Same idea as Ledoit-Wolf shrinkage from Week 3: add structure to a noisy estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso replaces the L2 penalty with L1:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{Lasso}} = \\sum_{i=1}^{N} (r_i - \\mathbf{x}_i^T \\boldsymbol{\\beta})^2 + \\alpha \\|\\boldsymbol{\\beta}\\|_1$$\n",
    "\n",
    "There's no closed-form solution — Lasso requires coordinate descent. But the L1 penalty does something Ridge can't: it drives coefficients to *exactly zero*, performing automatic feature selection. In a world with 400+ published factors and a signal-to-noise ratio near 0.001, knowing which features to *ignore* is almost as valuable as knowing which to use.\n",
    "\n",
    "Elastic Net combines both: $\\alpha\\bigl[\\ell_1 \\|\\boldsymbol{\\beta}\\|_1 + (1 - \\ell_1) \\|\\boldsymbol{\\beta}\\|_2^2\\bigr]$, hedging between Ridge's smooth shrinkage and Lasso's sparse selection. Think of it as a Bayesian prior that says: \"most coefficients should be small, and some should be zero.\"\n",
    "\n",
    "Let's train all four models on the same data and compare their coefficients. This is where the differences become visceral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = panel.index.get_level_values('date').unique().sort_values()\n",
    "split = dates[int(len(dates) * 0.7)]\n",
    "\n",
    "train = panel.loc[panel.index.get_level_values('date') <= split]\n",
    "test = panel.loc[panel.index.get_level_values('date') > split]\n",
    "\n",
    "X_train, y_train = train[feat_cols].values, train['target'].values\n",
    "X_test, y_test = test[feat_cols].values, test['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've split the data at the 70% mark in time — everything before that date is training, everything after is test. This is a simple temporal split, not the full expanding-window CV we'll build in Section 4, but it's enough to compare models. Notice: we split by *date*, not randomly. Shuffling would contaminate the test set with future information. This is the first commandment of financial ML evaluation — violate it and your results are fiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'OLS': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=1.0),\n",
    "    'Lasso': Lasso(alpha=0.001, max_iter=5000),\n",
    "    'ElasticNet': ElasticNet(alpha=0.001, l1_ratio=0.5, max_iter=5000),\n",
    "}\n",
    "\n",
    "for name, m in models.items():\n",
    "    m.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All four models are trained on the same data. Now let's look at what they learned — the coefficient vectors. This is where the story gets interesting, because the coefficients reveal *how* each model interprets the features, and the differences between OLS and the regularized models are often dramatic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame({name: m.coef_ for name, m in models.items()},\n",
    "                        index=feat_cols)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 5), sharey=True)\n",
    "for ax, name in zip(axes, coef_df.columns):\n",
    "    coef_df[name].plot.barh(ax=ax, color='steelblue')\n",
    "    ax.set_title(name, fontsize=12)\n",
    "    ax.axvline(0, color='k', lw=0.5)\n",
    "plt.suptitle('Coefficient Comparison Across Models', y=1.02, fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study those coefficient bars carefully. OLS has wild swings — some features get large positive coefficients, others large negative ones, and the magnitudes are much larger than the regularized models. This is multicollinearity doing its damage: OLS is using one momentum feature to offset another, producing a fragile linear combination that works in sample but will blow up on new data.\n",
    "\n",
    "Ridge shrinks everything toward zero uniformly. The coefficients are smaller, smoother, and more stable. Momentum features (especially the 12-month skip-1 variant) tend to retain the largest positive coefficients, consistent with decades of academic evidence. Volatility features tend to be negative — consistent with the low-volatility anomaly.\n",
    "\n",
    "Lasso is more surgical: it zeros out several features entirely, keeping only the ones that carry the strongest independent signal. The features that survive Lasso's knife are a useful diagnostic — they tell you which characteristics the cross-section is actually rewarding right now.\n",
    "\n",
    "But coefficients don't tell you whether the model *works*. For that, we need out-of-sample evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for name, m in models.items():\n",
    "    preds = m.predict(X_test)\n",
    "    ic = spearmanr(preds, y_test).statistic\n",
    "    r2_is = m.score(X_train, y_train)\n",
    "    r2_oos = m.score(X_test, y_test)\n",
    "    results.append({'Model': name, 'IC (OOS)': round(ic, 4),\n",
    "                    'R-sq in-sample': round(r2_is, 5),\n",
    "                    'R-sq OOS': round(r2_oos, 5)})\n",
    "\n",
    "display(pd.DataFrame(results).set_index('Model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's the pattern that repeats in every cross-sectional study: OLS has the highest in-sample R-squared — it's the *definition* of the best fit to training data — but out of sample, it typically has the lowest IC and the worst (often negative) R-squared. The model memorized the noise and extrapolated it forward.\n",
    "\n",
    "Ridge, with its gentle shrinkage, tends to produce the best or near-best out-of-sample IC. Lasso is competitive but often slightly lower, because it zeros out features that carry small but genuine signal. With only 30 stocks, the differences between Ridge and Lasso are within sampling error — you need hundreds of stocks and dozens of years to see a reliable difference. That's what the homework is for.\n",
    "\n",
    "> **Did You Know?** At most quant funds, an IC of 0.05 is considered excellent. An IC of 0.10 would be world-class — and suspicious (your first reaction should be \"check for look-ahead bias\"). Most published factor ICs are between 0.02 and 0.06. Cliff Asness founded AQR (Applied Quantitative Research) in 1998 with \\$1 billion, and the firm now manages over \\$100 billion — primarily through systematic strategies based on exactly the factors we're implementing: momentum, value, quality, and low volatility. The strategy hasn't fundamentally changed in 25 years. The edge comes from discipline, scale, and continuous refinement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Expanding-Window Cross-Validation\n",
    "\n",
    "If you use standard 5-fold cross-validation on financial data, you will get a beautiful, impressive, and completely fake result.\n",
    "\n",
    "Here's why. Imagine your training set contains data from January 2020 and your test fold contains data from December 2019. Your model has seen the future — it knows the COVID crash is coming — and it's using that information to \"predict\" the past. The CV score looks great, but it's measuring your model's ability to time-travel, not to predict. In production, your model doesn't have a time machine.\n",
    "\n",
    "The fix is expanding-window cross-validation: at each month $t$, train on *everything* from the beginning of the sample up to month $t-1$, then predict month $t$. The training window grows over time (hence \"expanding\"), and the model never sees future data. This is the only correct evaluation method for temporal data, and it's the standard we'll use for the remaining 14 weeks of the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, for each month $t = T_0, T_0 + 1, \\dots, T$:\n",
    "\n",
    "$$\\text{Train: } \\{(\\mathbf{x}_{i,s},\\, r_{i,s+1})\\}_{s=1}^{t-1} \\qquad\\qquad \\text{Predict: } \\hat{r}_{i,t+1} = f(\\mathbf{x}_{i,t};\\, \\hat{\\theta}_{1:t-1})$$\n",
    "\n",
    "$$\\text{Evaluate: } IC_t = \\text{Spearman}(\\hat{r}_{i,t+1},\\, r_{i,t+1})$$\n",
    "\n",
    "The final metric is the average IC across all out-of-sample months:\n",
    "\n",
    "$$\\overline{IC} = \\frac{1}{T - T_0 + 1} \\sum_{t=T_0}^{T} IC_t$$\n",
    "\n",
    "The key parameter $T_0$ is the minimum training window — you need enough historical data to fit a stable model before you start predicting. We'll use 60 months (5 years) as the minimum, which is standard in the industry.\n",
    "\n",
    "Let's implement this from scratch and see what the rolling IC looks like for Ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_sorted = panel.index.get_level_values('date').unique().sort_values()\n",
    "min_train = 60\n",
    "ic_results = []\n",
    "\n",
    "for i in range(min_train, len(dates_sorted) - 1):\n",
    "    train_end = dates_sorted[i]\n",
    "    test_date = dates_sorted[i + 1]\n",
    "    tr = panel.loc[panel.index.get_level_values('date') <= train_end]\n",
    "    te = panel.loc[panel.index.get_level_values('date') == test_date]\n",
    "    if len(te) < 5:\n",
    "        continue\n",
    "    ridge = Ridge(alpha=1.0).fit(tr[feat_cols], tr['target'])\n",
    "    preds = ridge.predict(te[feat_cols])\n",
    "    ic_val = spearmanr(preds, te['target']).statistic\n",
    "    ic_results.append({'date': test_date, 'ic': ic_val})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That loop is the backbone of cross-sectional research — the exact same structure used at every quant fund, and the one you'll reuse for every model from here through Week 18. At each month, we train Ridge on *all prior data* and predict next month's cross-sectional returns. The training window starts at 60 months and grows to the full sample. No shuffling. No leakage. No time travel.\n",
    "\n",
    "Now let's visualize the results. What does the IC look like over time? Is it consistently positive, or does it swing wildly between positive and negative? The answer matters enormously, because a strategy built on this signal needs to survive the bad months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_df = pd.DataFrame(ic_results).set_index('date')\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 7), sharex=True)\n",
    "axes[0].bar(ic_df.index, ic_df['ic'], color='steelblue', alpha=0.7, width=20)\n",
    "axes[0].axhline(ic_df['ic'].mean(), color='red', ls='--',\n",
    "                label=f\"Mean IC: {ic_df['ic'].mean():.3f}\")\n",
    "axes[0].set_ylabel('Monthly IC'); axes[0].legend()\n",
    "axes[0].set_title('Rolling Out-of-Sample IC (Ridge, Expanding Window)')\n",
    "\n",
    "axes[1].plot(ic_df.index, ic_df['ic'].cumsum(), color='darkblue', lw=2)\n",
    "axes[1].set_ylabel('Cumulative IC')\n",
    "axes[1].set_title('Cumulative IC Over Time')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study that top panel. The monthly IC bounces around — positive in some months, negative in others. That's not a bug, it's the reality of financial prediction. Even the best quant models are wrong *almost as often as they're right*. What matters is whether the average is reliably positive and whether the cumulative IC drifts upward over time.\n",
    "\n",
    "The bottom panel shows the cumulative IC, which should trend upward with periodic drawdowns. Those drawdowns correspond to market regimes where the linear model's signal breaks down — typically during sudden crashes, sharp sector rotations, or regime changes. During the COVID crash of March 2020, for instance, momentum signals reversed violently: stocks that had been winning suddenly became the biggest losers. A model trained on historical momentum would have been on the wrong side of that reversal.\n",
    "\n",
    "The average IC across our 30-stock universe should be in the 0.02-0.05 range — exactly where the academic literature says it should be. Tiny, but economically meaningful when applied across hundreds of stocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how badly standard k-fold CV misleads you. We'll evaluate the same Ridge model two ways: our honest expanding-window method and shuffled 5-fold CV. This comparison is the single most important methodological lesson in this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X_all, y_all = panel[feat_cols].values, panel['target'].values\n",
    "kfold_ics = []\n",
    "for tr_idx, te_idx in KFold(5, shuffle=True, random_state=42).split(X_all):\n",
    "    m = Ridge(alpha=1.0).fit(X_all[tr_idx], y_all[tr_idx])\n",
    "    ic = spearmanr(m.predict(X_all[te_idx]), y_all[te_idx]).statistic\n",
    "    kfold_ics.append(ic)\n",
    "\n",
    "expanding_ic = ic_df['ic'].mean()\n",
    "kfold_ic = np.mean(kfold_ics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now run the same Ridge model through both evaluation pipelines. Let's see the numbers side by side — the gap is usually striking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denom = expanding_ic if abs(expanding_ic) > 0.001 else 0.001\n",
    "comparison = pd.DataFrame({\n",
    "    'Method': ['5-Fold CV (shuffled)', 'Expanding-Window CV'],\n",
    "    'Average IC': [round(kfold_ic, 4), round(expanding_ic, 4)],\n",
    "    'Inflation Factor': [round(kfold_ic / denom, 1), 1.0]\n",
    "}).set_index('Method')\n",
    "display(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shuffled 5-fold IC is systematically higher than the expanding-window IC — typically by a factor of 2-4x. That gap is pure temporal leakage: the shuffled folds mix past and future data, allowing the model to exploit autocorrelation in the target variable. A monthly return in January 2020 is correlated with returns in December 2019 and February 2020 — shuffle them into different folds and you've given the model a crystal ball.\n",
    "\n",
    "Every impressive financial ML result that used shuffled CV is lying — not maliciously, but structurally. This comparison should be permanent calibration for you: whenever someone shows you a financial ML result, your first question should be \"what CV method did you use?\" If the answer is k-fold with shuffling, divide their reported performance by 3 and that's a *generous* estimate of the truth.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. The Gu-Kelly-Xiu Framework — Where We Stand\n",
    "\n",
    "Let's place our work in context. We've built a small version of the cross-sectional prediction pipeline that Gu, Kelly, and Xiu scaled to industrial proportions. Their dataset: 30,000 stocks, 720 months, 94 characteristics. Ours: 30 stocks, ~150 months, 11 features. Same methodology, different scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Here are the key results from the GKX paper — the benchmark that defines the field:\n\n| Model | OOS Monthly $R^2$ | Relative Rank |\n|-------|-------------------|---------------|\n| OLS | ~0.08% | 8 (worst) |\n| Elastic Net | ~0.20% | 6 |\n| PLS / PCR | ~0.15% | 7 |\n| Random Forest | ~0.22% | 4 |\n| Gradient Boosted Trees | ~0.28% | 3 |\n| Neural Net (1-layer) | ~0.30% | 2 |\n| Neural Net (3-5 layer) | ~0.40% | 1 (best) |\n\nThese numbers look absurdly small. A monthly R-squared of 0.4% means the model explains less than half a percent of the variation in stock returns. In any other ML domain, you'd throw this model away. But in finance, you'd build a billion-dollar firm on it.\n\nThe most important finding in the paper isn't the model ranking — it's the *interaction effect*. GKX showed that the single most important nonlinear feature is the interaction between momentum and volatility. High-momentum, low-volatility stocks outperform by a wide margin. High-momentum, high-volatility stocks? Not so much — the momentum signal is unreliable when volatility is high. Trees and neural nets capture this interaction automatically. Linear models can't, because they model each feature independently. That's the specific gap we'll close in Week 5.\n\nLet's make the Fundamental Law connection concrete. If our Ridge model achieves an IC of 0.03 on a universe of $N$ stocks rebalanced monthly, what annualized information ratio does that imply? The answer depends entirely on breadth."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_values = [0.02, 0.03, 0.05, 0.10]\n",
    "breadth_values = [30, 100, 500, 3000]\n",
    "\n",
    "ir_table = pd.DataFrame(\n",
    "    {f'BR={br}': [ic * np.sqrt(br * 12) for ic in ic_values]\n",
    "     for br in breadth_values},\n",
    "    index=[f'IC={ic}' for ic in ic_values])\n",
    "ir_table = ir_table.round(2)\n",
    "display(Markdown('**Annualized Information Ratio = IC x sqrt(BR x 12)**'))\n",
    "display(ir_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read this table carefully — it's the economics of the entire cross-sectional prediction business. With 30 stocks (our lecture demo), even an IC of 0.05 gives an IR around 1.0 — decent but not spectacular. With 500 stocks and the same IC, the IR jumps above 3. With 3,000 stocks (the GKX scale), even IC = 0.02 gives an IR above 1.5.\n",
    "\n",
    "Breadth is the multiplier. This is why quant funds trade thousands of stocks — not because any single bet is confident, but because the law of large numbers turns a tiny edge into a reliable stream. Your Ridge model with IC = 0.03 is unprofitable at 30 stocks. It's a decent living at 200 stocks. It's a career at 3,000 stocks. Same model, same signal, same IC — the only difference is how many independent bets you can make.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. From Predictions to Portfolios — Quantile Analysis\n",
    "\n",
    "The acid test of a cross-sectional prediction model isn't its R-squared or even its IC. It's this: if you sort stocks into quintiles by predicted return, does the top quintile actually outperform the bottom? If the spread is monotonic — Q1 < Q2 < Q3 < Q4 < Q5 — your model has learned something real. If it's not monotonic, you might just have noise that correlates with the target by accident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The long-short portfolio return at time $t$:\n",
    "\n",
    "$$r_{LS,t} = \\frac{1}{|Q_5|} \\sum_{i \\in Q_5} r_{i,t} \\;-\\; \\frac{1}{|Q_1|} \\sum_{i \\in Q_1} r_{i,t}$$\n",
    "\n",
    "where $Q_5$ is the top quintile (highest predicted returns) and $Q_1$ is the bottom. This portfolio is approximately dollar-neutral (zero net investment) and market-neutral (roughly zero beta), so its return approximates pure alpha. At every quant fund, the quintile spread chart is the standard deliverable for evaluating a signal. You don't publish an R-squared — you show the spread.\n",
    "\n",
    "Let's build this using our expanding-window predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_results = []\n",
    "for i in range(min_train, len(dates_sorted) - 1):\n",
    "    train_end = dates_sorted[i]\n",
    "    test_date = dates_sorted[i + 1]\n",
    "    tr = panel.loc[panel.index.get_level_values('date') <= train_end]\n",
    "    te = panel.loc[panel.index.get_level_values('date') == test_date].copy()\n",
    "    if len(te) < 10:\n",
    "        continue\n",
    "    ridge = Ridge(alpha=1.0).fit(tr[feat_cols], tr['target'])\n",
    "    te['pred'] = ridge.predict(te[feat_cols])\n",
    "    te['quintile'] = pd.qcut(te['pred'], 5, labels=[1,2,3,4,5], duplicates='drop')\n",
    "    for q in range(1, 6):\n",
    "        mask = te['quintile'] == q\n",
    "        if mask.any():\n",
    "            quantile_results.append({'date': test_date, 'quintile': q,\n",
    "                                     'ret': te.loc[mask, 'target'].mean()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each out-of-sample month, we've sorted stocks into five buckets by predicted return and recorded the average realized return for each bucket. If the model is working, quintile 5 (the stocks the model likes most) should have the highest average return, and quintile 1 (the stocks the model likes least) should have the lowest. Let's see the quintile spread chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdf = pd.DataFrame(quantile_results)\n",
    "q_means = qdf.groupby('quintile')['ret'].mean() * 100\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "colors = ['#d62728', '#ff7f0e', '#cccccc', '#2ca02c', '#1f77b4']\n",
    "q_means.plot.bar(ax=ax, color=colors, edgecolor='black', linewidth=0.5)\n",
    "ax.set_xlabel('Quintile (1=Lowest Predicted, 5=Highest)')\n",
    "ax.set_ylabel('Mean Monthly Excess Return (%)')\n",
    "ax.set_title('Quintile Returns: Ridge Expanding-Window Predictions')\n",
    "ax.axhline(0, color='k', lw=0.5)\n",
    "plt.xticks(rotation=0); plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the quintile chart shows an upward slope from Q1 to Q5, the model is working. The spread between Q5 and Q1 — the long-short return — is the economic value of the prediction. With 30 stocks and 11 features, the spread will be modest. With 500 stocks and 20+ features (the homework scale), it should be more pronounced.\n",
    "\n",
    "The key diagnostic is *monotonicity*: do the returns increase steadily from Q1 to Q5? A perfectly monotonic spread means the model's ranking is meaningful at every level. A non-monotonic spread (say, Q3 outperforms Q5) suggests the model only reliably separates winners from losers at the extremes.\n",
    "\n",
    "Now let's trace out the long-short portfolio's cumulative performance — what a trader would actually experience living with this strategy day after day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q5 = qdf[qdf['quintile'] == 5].set_index('date')['ret']\n",
    "q1 = qdf[qdf['quintile'] == 1].set_index('date')['ret']\n",
    "ls_returns = q5.subtract(q1, fill_value=0).dropna()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "(1 + ls_returns).cumprod().plot(ax=ax, lw=2, color='darkblue')\n",
    "ax.axhline(1, color='k', ls='--', lw=0.5)\n",
    "ax.set_ylabel('Cumulative Return')\n",
    "ax.set_title('Ridge: Long Q5 / Short Q1 — Cumulative Performance')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That cumulative line tells the full story. If it drifts upward over time, the strategy is extracting genuine signal from the cross-section. If it's flat or declining, the model isn't working — or the universe is too small to generate reliable quintile sorts (a common issue with only 30 stocks, where each quintile contains just 6 names).\n",
    "\n",
    "Notice the drawdowns: there will be periods where the long-short portfolio loses money for several consecutive months. These drawdowns correspond to regime changes where the model's learned relationships break down. A model that \"explains\" 0.3% of return variation is still wrong on roughly 48% of its monthly bets. The edge is statistical, not certain. This is the psychological challenge of quantitative investing — you need to keep the system running through the bad months, trusting that the long-run average IC is positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the performance metrics that a portfolio manager would demand — including what happens after transaction costs. We introduced the cost framework in Week 3: 10 basis points per side is realistic for large-cap US stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_ret = ls_returns.mean() * 12\n",
    "ann_vol = ls_returns.std() * np.sqrt(12)\n",
    "sharpe = ann_ret / ann_vol if ann_vol > 0 else 0\n",
    "cum = (1 + ls_returns).cumprod()\n",
    "max_dd = (cum / cum.cummax() - 1).min()\n",
    "\n",
    "cost_per_rebal = 0.0010 * 2\n",
    "net_ret = ls_returns - cost_per_rebal\n",
    "sharpe_net = (net_ret.mean() * 12) / (net_ret.std() * np.sqrt(12)) if net_ret.std() > 0 else 0\n",
    "\n",
    "display(Markdown(\n",
    "    f\"| Metric | Value |\\n|--------|-------|\\n\"\n",
    "    f\"| Ann. Return (gross) | {ann_ret:.2%} |\\n\"\n",
    "    f\"| Ann. Volatility | {ann_vol:.2%} |\\n\"\n",
    "    f\"| Sharpe (gross) | {sharpe:.2f} |\\n\"\n",
    "    f\"| Sharpe (net 10bps) | {sharpe_net:.2f} |\\n\"\n",
    "    f\"| Max Drawdown | {max_dd:.2%} |\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the Sharpe ratio degradation from gross to net-of-costs. Even at 10 basis points per side — which is realistic for large-cap US equities — the monthly rebalancing cost eats into the strategy's edge. The degradation is proportional to turnover: if the model's predictions change dramatically month to month (high turnover), you're paying more in trading costs. If the predictions are stable (low turnover), costs are manageable.\n",
    "\n",
    "Ridge's smooth coefficients tend to produce more stable predictions than Lasso's sparse ones — another advantage of L2 regularization that doesn't show up in IC comparisons but matters for real money. This is the permanent tension in quantitative finance: signal decays with time (you want to trade frequently to capture fresh signal), but transaction costs accumulate with trading frequency (you want to trade rarely to minimize costs). Every strategy lives somewhere on this tradeoff curve.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. What Linear Models Miss — A Preview of Week 5\n",
    "\n",
    "We've built a working cross-sectional prediction pipeline with linear models. The IC is small but positive. The quintile spread is hopefully monotonic. The Fundamental Law tells us this should work at scale. So why bother with trees and neural nets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because linear models can't capture *interactions*. The Gu-Kelly-Xiu paper showed that the single most important nonlinear effect in stock prediction is the momentum-volatility interaction: high-momentum, low-volatility stocks massively outperform, while high-momentum, high-volatility stocks deliver unreliable returns. A linear model sees momentum and volatility as independent features and combines them additively. It can't learn that the *combination* matters more than either feature alone.\n",
    "\n",
    "Let's see this interaction in our data. We'll split stocks by both momentum tercile and volatility tercile, then look at the average forward return in each cell. If there's an interaction, the pattern won't be additive — the best cell won't simply be \"highest momentum + lowest volatility summed together.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata = panel.copy()\n",
    "idata['mom_q'] = idata.groupby(level='date')['mom_12m_skip1'].transform(\n",
    "    lambda x: pd.qcut(x, 3, labels=['Low','Mid','High'], duplicates='drop'))\n",
    "idata['vol_q'] = idata.groupby(level='date')['vol_20d'].transform(\n",
    "    lambda x: pd.qcut(x, 3, labels=['Low','Mid','High'], duplicates='drop'))\n",
    "\n",
    "interact_ret = idata.groupby(['mom_q','vol_q'])['target'].mean() * 100\n",
    "pivot = interact_ret.unstack('vol_q')\n",
    "display(Markdown('**Mean Monthly Excess Return (%) by Momentum x Volatility**'))\n",
    "display(pivot.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data reveals the interaction, you'll see that high-momentum, low-volatility stocks have substantially better returns than you'd predict by simply adding the momentum effect and the volatility effect. A linear model can't capture this — it would assign a fixed weight to momentum and a fixed weight to volatility and add them. A tree can learn that \"when momentum is high AND volatility is low, the prediction should be especially positive\" — because trees partition the feature space, they naturally capture interactions.\n",
    "\n",
    "This is the specific gap we'll close in Week 5 with XGBoost and LightGBM. The improvement over Ridge in R-squared terms is typically modest (from ~0.2% to ~0.3%), but it translates to real portfolio improvement because the interaction captures a genuine economic effect: momentum is more reliable when volatility is low, because low-volatility environments are less affected by noise, sentiment, and liquidity shocks.\n",
    "\n",
    "> **Did You Know?** Kenneth French at Dartmouth's Tuck School maintains a free online data library with factor returns going back to 1926 — the most-used free dataset in academic finance. Every asset pricing paper references it. The Fama-French factors (market, size, value, momentum, profitability, investment) are computed from CRSP data using a standardized methodology. Bookmark it at mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html — you'll use it repeatedly for benchmarking your models against decades of academic evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "| Concept | What You Need to Remember |\n",
    "|---------|---------------------------|\n",
    "| Cross-sectional prediction | Predict *relative* returns (Apple vs. Microsoft), not absolute. The IC measures your ranking accuracy. |\n",
    "| Feature engineering | Momentum, value, volatility, size, liquidity — each represents decades of research. Rank-normalize cross-sectionally at each date. |\n",
    "| OLS vs. Ridge vs. Lasso | OLS overfits. Ridge shrinks all coefficients smoothly (best for correlated features). Lasso zeros out weak features (interpretable but may discard weak signal). |\n",
    "| Expanding-window CV | The only correct evaluation for temporal data. Never shuffle. Train on past, predict future, period. |\n",
    "| IC calibration | IC of 0.05 is *excellent*. IC of 0.10 is suspicious. Breadth (number of stocks) is the multiplier that makes small IC economically meaningful. |\n",
    "| Quantile analysis | Sort stocks into quintiles by prediction. Monotonic spread = real signal. The Q5-Q1 long-short return is your signal's economic value. |\n",
    "| Transaction costs | At 10 bps round-trip, monthly rebalancing costs ~2.4% per year. Your Sharpe after costs is the only Sharpe that matters. |\n",
    "| The GKX benchmark | Linear models: OOS $R^2$ ~0.2%. Neural nets: ~0.4%. Most prediction power comes from features, not model architecture. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bridge to Week 5\n",
    "\n",
    "You've just built your first cross-sectional alpha model — the bread and butter of quantitative asset management. It uses simple linear models (Ridge, Lasso), standard features (momentum, volatility, size), and rigorous expanding-window evaluation. The IC is small — maybe 0.02-0.05 — but the Fundamental Law tells us that's enough when breadth is high.\n",
    "\n",
    "Next week, we replace the linear model with gradient-boosted trees — XGBoost and LightGBM. These models dominate production quant finance for a specific reason: they capture nonlinear interactions between features that linear models miss. The momentum-volatility interaction we glimpsed in Section 7 is just the beginning. Trees find it automatically. They also give us SHAP values — a way to see *exactly* which features and interactions your model is using, which is essential for trust (both your own trust in the model, and your PM's trust in your model). You'll discover that the improvement over Ridge is real but modest — and that a well-tuned Ridge with engineered interaction features can sometimes match a default XGBoost. The model matters, but the features and methodology matter more.\n",
    "\n",
    "In the homework this week, you'll scale the pipeline to 200+ stocks and 20+ features. That's where the Fundamental Law kicks in: breadth amplifies small signal into reliable returns. The code you build is modular by design — swap in a tree model next week, a neural net in Week 7, and the evaluation framework stays exactly the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Suggested Reading\n",
    "\n",
    "- **Gu, Kelly & Xiu (2020), \"Empirical Asset Pricing via Machine Learning\"** — The paper that defined the field. Focus on Tables 3-5 (model comparison) and Figure 3 (feature importance and interactions). Dense but essential — every quant you'll ever work with has read it.\n",
    "\n",
    "- **Jegadeesh & Titman (1993), \"Returns to Buying Winners and Selling Losers\"** — The paper that discovered momentum. Over 12,000 citations. The anomaly has been replicated in 40+ countries and remains the single most robust predictor. Read it to understand the foundation of your strongest feature.\n",
    "\n",
    "- **Harvey, Liu & Zhu (2016), \"...and the Cross-Section of Expected Returns\"** — The \"factor zoo\" paper that showed most published factors are probably false positives. Essential context for why rigorous methodology matters — connects directly to Week 6's discussion of backtest overfitting.\n",
    "\n",
    "- **Stefan Jansen, *Machine Learning for Algorithmic Trading*, Chapters 4-7** — The most practical treatment of alpha factor research with Python code. Working implementations of feature engineering and linear models with alphalens integration. Start here if you want production-grade code for the homework."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}