{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 Seminar: Cross-Sectional Return Prediction with Linear Models\n",
    "\n",
    "In the lecture, we framed the cross-sectional prediction problem and showed you the moving parts: features, regularized regression, expanding-window evaluation, and the Gu-Kelly-Xiu benchmark. Now you're going to build all of this yourself. You'll construct a real feature matrix for dozens of stocks, trace the regularization path to see exactly what Ridge and Lasso are doing under the hood, expose the information leakage trap that has fooled countless published papers, and rank individual features by their raw predictive power. By the end of this session, you'll have a working cross-sectional prediction pipeline -- and a healthy skepticism about any backtest result that doesn't specify its evaluation methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "def get_close(data):\n",
    "    \"\"\"Extract close prices, handling yfinance MultiIndex.\"\"\"\n",
    "    if isinstance(data.columns, pd.MultiIndex):\n",
    "        return data['Close']\n",
    "    return data[['Close']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need data for all four exercises, so we'll download everything once. We're pulling about 50 liquid US stocks with enough history to build a meaningful cross-section. In a production system, you'd use the full S&P 500 or broader -- but 50 stocks is enough to see the patterns without waiting 20 minutes for yfinance to respond. We'll grab daily data from 2010 through mid-2024 and also download SPY to compute excess returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = [\n",
    "    'AAPL', 'MSFT', 'AMZN', 'GOOGL', 'META', 'NVDA', 'TSLA', 'JPM',\n",
    "    'JNJ', 'V', 'PG', 'UNH', 'HD', 'MA', 'DIS', 'BAC', 'XOM',\n",
    "    'PFE', 'CSCO', 'VZ', 'INTC', 'KO', 'PEP', 'ABT', 'MRK',\n",
    "    'CVX', 'WMT', 'ABBV', 'COST', 'TMO', 'NKE', 'DHR', 'LLY',\n",
    "    'NEE', 'MDT', 'TXN', 'UNP', 'QCOM', 'LOW', 'BMY', 'AMGN',\n",
    "    'CAT', 'GS', 'BLK', 'SPGI', 'ADP', 'MMM', 'SYK', 'DE', 'CL'\n",
    "]\n",
    "\n",
    "raw = yf.download(tickers + ['SPY'], start='2010-01-01', end='2024-07-01',\n",
    "                  auto_adjust=True, progress=False)\n",
    "prices = get_close(raw)\n",
    "volume = raw['Volume'] if isinstance(raw.columns, pd.MultiIndex) else raw[['Volume']]\n",
    "prices.shape, volume.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That gives us roughly 14 years of daily data for 51 tickers (50 stocks plus SPY). A few tickers may have started trading after 2010 (META didn't IPO until 2012, for instance), so you'll see some NaNs in the early rows. That's fine -- it's exactly the kind of messiness you'd encounter in a real feature pipeline, and we'll handle it.\n",
    "\n",
    "## Exercise 1: Building the Feature Matrix\n",
    "\n",
    "**The question:** Can you construct a clean, complete cross-sectional feature matrix for 50 stocks using only free data -- and what does the resulting data actually look like?\n",
    "\n",
    "This is the foundation of everything we'll build for the remaining 14 weeks of the course. You're going to compute a dozen features for each stock at each month-end, handle the inevitable missing data, cross-sectionally normalize, and produce a panel dataset ready for ML. Think of this as building your feature store -- the asset that every model downstream depends on. Get it wrong here, and every backtest, every IC calculation, and every portfolio return you compute later is garbage.\n",
    "\n",
    "**Your tasks:**\n",
    "1. Compute daily returns and monthly returns for all stocks\n",
    "2. Compute the following features at each month-end: momentum (1m, 3m, 6m, 12m-skip-1), short-term reversal, realized volatility (20-day and 60-day), log dollar volume (size proxy), 50-day/200-day MA ratio, Amihud illiquidity\n",
    "3. Compute the market return from SPY and create excess returns as the prediction target\n",
    "4. Cross-sectionally rank-normalize all features to [0, 1] at each date\n",
    "5. Examine correlations and summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR EXPLORATION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ▶ Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_tickers = [t for t in tickers if t != 'SPY']\n",
    "stock_prices = prices[stock_tickers]\n",
    "spy_prices = prices['SPY']\n",
    "\n",
    "daily_returns = stock_prices.pct_change()\n",
    "monthly_prices = stock_prices.resample('ME').last()\n",
    "monthly_spy = spy_prices.resample('ME').last()\n",
    "monthly_returns = monthly_prices.pct_change()\n",
    "spy_monthly = monthly_spy.pct_change()\n",
    "excess_returns = monthly_returns.sub(spy_monthly, axis=0)\n",
    "\n",
    "# Momentum features\n",
    "mom_1m = monthly_prices.pct_change(1)\n",
    "mom_3m = monthly_prices.pct_change(3)\n",
    "mom_6m = monthly_prices.pct_change(6)\n",
    "mom_12m_skip1 = monthly_prices.shift(1).pct_change(12)\n",
    "reversal = monthly_returns.shift(1)\n",
    "\n",
    "# Volatility (from daily, sampled monthly)\n",
    "vol_20d_monthly = (daily_returns.rolling(20).std() * np.sqrt(252)).resample('ME').last()\n",
    "vol_60d_monthly = (daily_returns.rolling(60).std() * np.sqrt(252)).resample('ME').last()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the skip-1 trick in 12-month momentum: we shift the price series by 1 month before computing the 12-month return. Jegadeesh and Titman (1993) showed that stocks that won over the past 12 months tend to keep winning -- but stocks that won *last month specifically* tend to reverse. Including the most recent month contaminates momentum with reversal, muddying both signals. The skip is standard practice at every quant fund. Now let's add size, trend, and liquidity features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dollar_volume = stock_prices * volume[stock_tickers]\n",
    "log_dollar_vol = np.log(dollar_volume.rolling(21).mean()).resample('ME').last()\n",
    "ma_ratio = (stock_prices.rolling(50).mean() / stock_prices.rolling(200).mean()).resample('ME').last()\n",
    "amihud_monthly = (daily_returns.abs() / dollar_volume).rolling(21).mean().resample('ME').last()\n",
    "\n",
    "# Assemble panel DataFrame\n",
    "feature_dict = {\n",
    "    'mom_1m': mom_1m, 'mom_3m': mom_3m, 'mom_6m': mom_6m,\n",
    "    'mom_12m_skip1': mom_12m_skip1, 'reversal': reversal,\n",
    "    'vol_20d': vol_20d_monthly, 'vol_60d': vol_60d_monthly,\n",
    "    'log_dollar_vol': log_dollar_vol, 'ma_ratio': ma_ratio,\n",
    "    'amihud': amihud_monthly\n",
    "}\n",
    "panels = []\n",
    "for name, df in feature_dict.items():\n",
    "    s = df.stack(); s.name = name; panels.append(s)\n",
    "target = excess_returns.shift(-1).stack(); target.name = 'target'\n",
    "panels.append(target)\n",
    "panel = pd.concat(panels, axis=1).dropna()\n",
    "panel.index.names = ['date', 'ticker']\n",
    "panel.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see something like (5,000-8,000 rows, 11 columns) -- each row is a (month, stock) observation with 10 features and one target. The `shift(-1)` on the target is critical: we're aligning features at time t with the *next month's* excess return. Getting this alignment wrong is the most common source of look-ahead bias in financial ML -- and once it's in your pipeline, it's nearly invisible. Your backtest will look great and your live trading will be mediocre.\n",
    "\n",
    "Now for rank normalization -- the step that separates amateur from professional feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [c for c in panel.columns if c != 'target']\n",
    "\n",
    "def rank_normalize(group):\n",
    "    return group[feature_cols].rank(pct=True)\n",
    "\n",
    "panel[feature_cols] = panel.groupby(level='date', group_keys=False).apply(rank_normalize)\n",
    "\n",
    "# Correlation heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(panel[feature_cols].corr(), annot=True, fmt='.2f',\n",
    "            cmap='RdBu_r', center=0, vmin=-1, vmax=1, ax=ax)\n",
    "ax.set_title('Cross-Sectional Feature Correlations (After Rank Normalization)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After rank normalization, every feature at every date is uniformly distributed on [0, 1]. This is the trick that everyone at quant funds uses but rarely discusses openly. Raw features have wildly different scales -- log dollar volume ranges from 15 to 25, while momentum ranges from -0.5 to +1.0. More importantly, raw features have extreme outliers: a stock that tripled in a month will dominate the regression. Rank normalization eliminates all of this.\n",
    "\n",
    "Look at the correlation matrix. The momentum features (1m, 3m, 6m, 12m) should be substantially correlated -- typically 0.4 to 0.7. The two volatility measures should be highly correlated (0.8+). This multicollinearity is exactly why OLS struggles: when two features are correlated at 0.8, OLS can't decide how to split the credit between them and the coefficients become wildly unstable. This is the problem that Ridge was born to solve -- it says \"I don't care which momentum variant gets the credit, just shrink them all toward zero and let the ensemble do the work.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Regularization Path -- How Lambda Shapes the Cross-Section\n",
    "\n",
    "**The question:** What does the model \"see\" at different regularization strengths -- and does the optimal amount of regularization stay constant over time, or does it shift with market conditions?\n",
    "\n",
    "The lecture showed that Ridge beats OLS. But that's a binary comparison. In practice, the regularization parameter lambda (called `alpha` in scikit-learn, because naming conventions in ML are a controlled disaster) is a continuous dial from zero (pure OLS) to infinity (all coefficients crushed to zero). Somewhere on that dial is a sweet spot. You're going to trace the full regularization path and watch exactly how that dial changes what the model sees.\n",
    "\n",
    "**Your tasks:**\n",
    "1. Pick one expanding-window fold: train on all data up to Dec 2018, test on 2019\n",
    "2. Sweep Ridge alpha from 0.001 to 10,000 (50 values, log scale). Record coefficients and out-of-sample IC\n",
    "3. Plot the Ridge regularization path (coefficients vs log-alpha)\n",
    "4. Repeat for Lasso -- note which features get zeroed out first\n",
    "5. Time-stability check: find the best alpha for 2019-2023 separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR EXPLORATION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ▶ Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "train_mask = panel.index.get_level_values('date') <= '2018-12-31'\ntest_2019 = (panel.index.get_level_values('date') >= '2019-01-01') & \\\n            (panel.index.get_level_values('date') <= '2019-12-31')\nX_train = panel.loc[train_mask, feature_cols].values\ny_train = panel.loc[train_mask, 'target'].values\nX_test = panel.loc[test_2019, feature_cols].values\ny_test = panel.loc[test_2019, 'target'].values\n\nalphas = np.logspace(-3, 4, 50)\nridge_coefs, ridge_ics = [], []\nfor alpha in alphas:\n    model = Ridge(alpha=alpha).fit(X_train, y_train)\n    ridge_coefs.append(model.coef_)\n    ridge_ics.append(spearmanr(model.predict(X_test), y_test).statistic)\nridge_coefs = np.array(ridge_coefs)"
  },
  {
   "cell_type": "markdown",
   "source": "We've swept 50 alpha values from near-zero (essentially OLS) to 10,000 (everything crushed flat). Each entry in `ridge_coefs` records the 10 feature coefficients at that regularization strength, and `ridge_ics` records the out-of-sample Spearman IC on 2019 data. Let's visualize both -- the coefficient path and the IC curve -- side by side. This dual view is one of the most informative diagnostic plots in applied ML.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\nfor i, feat in enumerate(feature_cols):\n    axes[0].plot(np.log10(alphas), ridge_coefs[:, i], label=feat)\naxes[0].set_xlabel('log10(alpha)'); axes[0].set_ylabel('Coefficient')\naxes[0].set_title('Ridge Regularization Path')\naxes[0].legend(fontsize=7, loc='upper right')\naxes[0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\nbest_idx = np.argmax(ridge_ics)\naxes[1].plot(np.log10(alphas), ridge_ics, 'b-o', markersize=3)\naxes[1].axvline(x=np.log10(alphas[best_idx]), color='r', linestyle='--',\n                label=f'Best alpha={alphas[best_idx]:.1f}')\naxes[1].set_xlabel('log10(alpha)'); axes[1].set_ylabel('OOS IC (2019)')\naxes[1].set_title('Ridge: IC vs Regularization'); axes[1].legend()\nplt.tight_layout(); plt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two things to notice. In the left panel, at very low alpha the coefficients are scattered wildly -- some large and positive, some large and negative. This is OLS-like behavior: with correlated features, the model assigns huge positive weight to one momentum variant and huge negative weight to another, trying to exploit tiny differences. That's overfitting. As alpha increases, all coefficients smoothly converge toward zero. The features that shrink *last* are carrying the most genuine signal.\n",
    "\n",
    "In the right panel, the IC curve should show a clear inverted-U shape. Too little regularization: overfitting kills out-of-sample performance. Too much: the model can't predict anything useful. The peak is where the reduction in variance from shrinkage exactly offsets the increase in bias. Now let's see what Lasso does differently -- where Ridge shrinks smoothly, Lasso makes hard choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_alphas = np.logspace(-6, -2, 50)\n",
    "lasso_coefs, lasso_ics = [], []\n",
    "for alpha in lasso_alphas:\n",
    "    model = Lasso(alpha=alpha, max_iter=10000).fit(X_train, y_train)\n",
    "    lasso_coefs.append(model.coef_)\n",
    "    lasso_ics.append(spearmanr(model.predict(X_test), y_test).statistic)\n",
    "lasso_coefs = np.array(lasso_coefs)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "for i, feat in enumerate(feature_cols):\n",
    "    axes[0].plot(np.log10(lasso_alphas), lasso_coefs[:, i], label=feat)\n",
    "axes[0].set_xlabel('log10(alpha)'); axes[0].set_ylabel('Coefficient')\n",
    "axes[0].set_title('Lasso Regularization Path')\n",
    "axes[0].legend(fontsize=7, loc='upper left')\n",
    "axes[0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "n_nonzero = (np.abs(lasso_coefs) > 1e-8).sum(axis=1)\n",
    "axes[1].plot(np.log10(lasso_alphas), n_nonzero, 'g-o', markersize=3)\n",
    "axes[1].set_xlabel('log10(alpha)'); axes[1].set_ylabel('Non-zero features')\n",
    "axes[1].set_title('Lasso: Feature Selection via Regularization')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lasso path looks fundamentally different from Ridge. Instead of smooth shrinkage, features snap to exactly zero as alpha increases. The right panel shows it starkly: at low alpha, all 10 features are active; crank up regularization and features drop out one by one until only 2-3 survive. The *order* of elimination is itself a feature importance ranking -- complementary to SHAP values you'll see in Week 5.\n",
    "\n",
    "Ridge tells you \"all features contribute a little.\" Lasso tells you \"these 3 features matter, the rest are noise.\" In the low signal-to-noise environment of stock prediction, Ridge typically produces better IC because it retains small correlated signals that Lasso discards. But Lasso gives you a clearer story about *which* features do the heavy lifting. Now for the time-stability check: does the optimal alpha stay constant across market regimes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_years = [2019, 2020, 2021, 2022, 2023]\n",
    "best_alphas_ridge, best_ics_ridge = [], []\n",
    "for year in test_years:\n",
    "    tr = panel.index.get_level_values('date') < f'{year}-01-01'\n",
    "    te = (panel.index.get_level_values('date') >= f'{year}-01-01') & \\\n",
    "         (panel.index.get_level_values('date') <= f'{year}-12-31')\n",
    "    Xtr, ytr = panel.loc[tr, feature_cols].values, panel.loc[tr, 'target'].values\n",
    "    Xte, yte = panel.loc[te, feature_cols].values, panel.loc[te, 'target'].values\n",
    "    year_ics = [spearmanr(Ridge(alpha=a).fit(Xtr, ytr).predict(Xte), yte).statistic\n",
    "                for a in alphas]\n",
    "    best_alphas_ridge.append(alphas[np.argmax(year_ics)])\n",
    "    best_ics_ridge.append(max(year_ics))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].bar(test_years, np.log10(best_alphas_ridge), color='steelblue')\n",
    "axes[0].set_xlabel('Test Year'); axes[0].set_ylabel('log10(Best Alpha)')\n",
    "axes[0].set_title('Ridge: Optimal Alpha Over Time')\n",
    "axes[1].bar(test_years, best_ics_ridge, color='coral')\n",
    "axes[1].set_xlabel('Test Year'); axes[1].set_ylabel('Best OOS IC')\n",
    "axes[1].set_title('Ridge: Best IC Achieved Per Year')\n",
    "axes[1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two findings that matter for production. First, the optimal alpha is *not* constant -- it fluctuates across years. You should see higher regularization preferred during volatile years (2020 with COVID, 2022 with rate hikes) and lower regularization during calm trending markets (2021). When the world is noisy, you want the model to be more conservative. The signal-to-noise ratio itself is non-stationary.\n",
    "\n",
    "Second, the best achievable IC varies dramatically. Some years you might see IC of 0.05-0.08; others show IC near zero or negative. This is not a bug -- it's the reality of financial prediction. The cross-section is not equally predictable at all times. This is why AQR and Two Sigma re-tune their models regularly and monitor IC in real time. The regularization path isn't a one-time tuning exercise -- it's a diagnostic you run continuously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: The Leakage Trap -- 5-Fold vs. Expanding-Window vs. Purged\n",
    "\n",
    "**The question:** How dramatically does your choice of evaluation methodology change your assessment of the same model's skill?\n",
    "\n",
    "This is the exercise that should permanently change how you read ML papers. You're going to take the exact same Ridge model -- same features, same hyperparameters, same data -- and evaluate it three different ways. The results will differ by a factor of 2-5x. Only one of those results is honest. The other two are structural self-deception that fills journals with \"impressive\" results that evaporate in production.\n",
    "\n",
    "**Your tasks:**\n",
    "1. Take your Ridge model with a reasonable alpha (median of the best alphas from Exercise 2)\n",
    "2. Evaluate with standard 5-fold CV (random shuffling -- the Kaggle approach)\n",
    "3. Evaluate with expanding-window CV (train up to month t, predict month t+1)\n",
    "4. Evaluate with expanding-window CV + 1-month purge buffer\n",
    "5. Compare the ICs and plot all temporal IC series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR EXPLORATION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ▶ Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_alpha = np.median(best_alphas_ridge)\n",
    "X_all, y_all = panel[feature_cols].values, panel['target'].values\n",
    "\n",
    "# Method 1: 5-fold shuffled CV (WRONG for financial data)\n",
    "kfold_ics = []\n",
    "for train_idx, test_idx in KFold(5, shuffle=True, random_state=42).split(X_all):\n",
    "    m = Ridge(alpha=chosen_alpha).fit(X_all[train_idx], y_all[train_idx])\n",
    "    kfold_ics.append(spearmanr(m.predict(X_all[test_idx]), y_all[test_idx]).statistic)\n",
    "kfold_mean_ic = np.mean(kfold_ics)\n",
    "\n",
    "# Method 2: Expanding-window CV (correct temporal ordering)\n",
    "dates = panel.index.get_level_values('date').unique().sort_values()\n",
    "min_train = 60\n",
    "expanding_ics, expanding_dates = [], []\n",
    "for i in range(min_train, len(dates) - 1):\n",
    "    tr = panel.index.get_level_values('date') <= dates[i]\n",
    "    te = panel.index.get_level_values('date') == dates[i + 1]\n",
    "    if te.sum() < 10: continue\n",
    "    m = Ridge(alpha=chosen_alpha).fit(panel.loc[tr, feature_cols], panel.loc[tr, 'target'])\n",
    "    expanding_ics.append(spearmanr(m.predict(panel.loc[te, feature_cols]),\n",
    "                                   panel.loc[te, 'target']).statistic)\n",
    "    expanding_dates.append(dates[i + 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 5-fold shuffled IC should look suspiciously good -- probably 0.05-0.10 or higher. Shuffling randomly assigns observations to folds regardless of time, so your training set might include March 2020 data while the test fold contains February 2020. The model has effectively seen the future. Adjacent observations share information through autocorrelated returns, persistent volatility regimes, and overlapping feature windows. The result is a metric measuring time-travel ability, not predictive skill.\n",
    "\n",
    "The expanding-window IC is the honest evaluation: train on everything up to month t, predict month t+1, no future information. Now let's add the purge buffer -- the most conservative evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Expanding-window with 1-month purge buffer\n",
    "purged_ics, purged_dates = [], []\n",
    "for i in range(min_train + 1, len(dates) - 1):\n",
    "    tr = panel.index.get_level_values('date') <= dates[i - 1]\n",
    "    te = panel.index.get_level_values('date') == dates[i + 1]\n",
    "    if te.sum() < 10 or tr.sum() < 100: continue\n",
    "    m = Ridge(alpha=chosen_alpha).fit(panel.loc[tr, feature_cols], panel.loc[tr, 'target'])\n",
    "    purged_ics.append(spearmanr(m.predict(panel.loc[te, feature_cols]),\n",
    "                                panel.loc[te, 'target']).statistic)\n",
    "    purged_dates.append(dates[i + 1])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "methods = ['5-Fold\\n(Shuffled)', 'Expanding\\nWindow', 'Expanding\\n+ Purge']\n",
    "mean_ics = [kfold_mean_ic, np.mean(expanding_ics), np.mean(purged_ics)]\n",
    "colors = ['#e74c3c', '#f39c12', '#27ae60']\n",
    "axes[0].bar(methods, mean_ics, color=colors)\n",
    "axes[0].set_ylabel('Mean IC')\n",
    "axes[0].set_title('Same Model, Three Evaluation Methods')\n",
    "for j, v in enumerate(mean_ics):\n",
    "    axes[0].text(j, v + 0.002, f'{v:.4f}', ha='center', fontsize=11)\n",
    "axes[1].plot(expanding_dates, expanding_ics, alpha=0.7, label='Expanding')\n",
    "axes[1].plot(purged_dates, purged_ics, alpha=0.7, label='Purged')\n",
    "axes[1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[1].set_title('Rolling IC Over Time'); axes[1].legend()\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar chart on the left is the punchline of this entire seminar. The shuffled 5-fold IC (red) should be noticeably higher than both temporal methods -- typically 1.5x to 5x higher. The gap between the red bar and the green bar is the amount of self-deception in your evaluation. That gap is not sampling noise. It's structural information leakage.\n",
    "\n",
    "The time series on the right shows that IC fluctuates wildly month to month -- some months +0.10, others -0.10. The model goes through regimes of predictability and unpredictability. The expanding and purged series should track each other closely, confirming the purge is a conservative adjustment rather than a wholesale methodology change.\n",
    "\n",
    "Here's the lesson that should stick permanently: whenever someone shows you a financial ML result, your first question is \"what evaluation methodology did you use?\" If the answer is k-fold CV, or if they look confused by the question, the number is probably inflated by 2-5x. Every impressive backtest built on shuffled CV is a fairy tale. The honest numbers are smaller, uglier, and the only ones that matter when real capital is on the line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: IC Analysis by Feature\n",
    "\n",
    "**The question:** Which individual features actually predict returns on their own, how strong are they, and does combining them in a linear model beat the best single feature -- or is the model just an expensive way to rediscover momentum?\n",
    "\n",
    "Before you build complex models -- trees, neural nets, attention mechanisms -- you should understand what each input feature contributes on its own. Sometimes the best feature does 80% of the work and the other nine are passengers. Knowing which situation you're in determines whether you need a better model or better features.\n",
    "\n",
    "**Your tasks:**\n",
    "1. For each of the 10 features, compute the monthly cross-sectional Spearman IC with next-month excess returns\n",
    "2. Rank features by average absolute IC\n",
    "3. Compare: does the full Ridge model beat the best single feature?\n",
    "4. Plot a feature IC bar chart and rolling 12-month IC of the top 3 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR EXPLORATION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ▶ Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_ic_series = {feat: [] for feat in feature_cols}\n",
    "ic_dates = []\n",
    "for date in dates:\n",
    "    subset = panel.loc[panel.index.get_level_values('date') == date]\n",
    "    if len(subset) < 10: continue\n",
    "    ic_dates.append(date)\n",
    "    for feat in feature_cols:\n",
    "        feature_ic_series[feat].append(\n",
    "            spearmanr(subset[feat], subset['target']).statistic)\n",
    "\n",
    "avg_ics = {f: np.nanmean(v) for f, v in feature_ic_series.items()}\n",
    "ic_ranking = sorted(avg_ics.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "feats = [x[0] for x in ic_ranking]\n",
    "vals = [x[1] for x in ic_ranking]\n",
    "ax.barh(feats, vals, color=['#27ae60' if v > 0 else '#e74c3c' for v in vals])\n",
    "ax.set_xlabel('Average Cross-Sectional IC')\n",
    "ax.set_title('Feature Predictive Power: Average IC with Next-Month Excess Returns')\n",
    "ax.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ranking reveals which features carry genuine predictive power. You should see momentum features (especially `mom_12m_skip1`) near the top -- confirming Jegadeesh and Titman's 1993 finding, replicated in 40+ countries. Short-term `reversal` should show a meaningful negative IC (last month's winners underperform this month). And `vol_20d` should show a negative IC too: the low-volatility anomaly, where less volatile stocks outperform, is one of the most reliable effects in empirical finance.\n",
    "\n",
    "The absolute magnitudes are small -- probably 0.02-0.06 for the strongest features. An IC of 0.03 sounds like nothing in ML terms. But remember the Fundamental Law from Week 3: IR = IC * sqrt(BR). With 50 stocks rebalanced monthly, breadth = 600/year. IC of 0.03 gives IR = 0.03 * sqrt(600) = 0.73. That's a respectable Sharpe. AQR manages over $100 billion on signals of roughly this magnitude. Now let's check whether these signals are consistent or regime-dependent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3 = [x[0] for x in ic_ranking[:3]]\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "for feat in top_3:\n",
    "    rolling = pd.Series(feature_ic_series[feat], index=ic_dates).rolling(12).mean()\n",
    "    ax.plot(rolling, label=feat, alpha=0.8)\n",
    "ax.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax.set_title('Rolling 12-Month Average IC: Top 3 Features')\n",
    "ax.set_ylabel('Rolling IC (12-month avg)'); ax.legend()\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the \"features are non-stationary\" lesson becomes visceral. Even the best features oscillate -- sometimes positive, sometimes negative, sometimes wildly so. Momentum tends to work beautifully in trending markets but gets destroyed during sharp reversals. The \"momentum crash\" of March 2009 is the most famous example: stocks that had been falling for months suddenly reversed violently, and momentum portfolios lost 40% in a single month.\n",
    "\n",
    "The practical implication: any single feature is unreliable over short horizons. The value of a multi-feature model isn't dramatic IC improvement -- it's *diversification* across signals. When momentum stops working, maybe volatility or reversal picks up the slack. Let's test whether the Ridge model actually beats the best single feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mean_ic = np.mean(expanding_ics)\n",
    "best_feat_name, best_feat_ic = ic_ranking[0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "labels = [f'Best Feature\\n({best_feat_name})', 'Ridge Model\\n(all 10 features)']\n",
    "bars = ax.bar(labels, [best_feat_ic, model_mean_ic], color=['#3498db', '#e67e22'])\n",
    "ax.set_ylabel('Mean IC')\n",
    "ax.set_title('Single Feature vs. Multi-Feature Model')\n",
    "for bar, val in zip(bars, [best_feat_ic, model_mean_ic]):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "            f'{val:.4f}', ha='center', fontsize=12)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ridge model's IC should be modestly higher than the best single feature -- but the improvement is probably smaller than you'd expect. This is the honest reality of linear models in cross-sectional prediction: when the model is a weighted average of features, it can't exceed what the individual features collectively offer. Ridge diversifies across features and stabilizes the signal, but it can't *create* signal that doesn't exist in the raw inputs. The ceiling is fundamentally low.\n",
    "\n",
    "This is precisely why Gu, Kelly, and Xiu found that gradient-boosted trees and neural nets outperform linear models. Those models capture *interactions* between features -- for example, momentum behaving differently for high-vol versus low-vol stocks. A linear model can't represent \"momentum works better when volatility is low\" without you manually engineering that interaction. A tree finds it automatically by splitting on volatility first. That interaction -- momentum crossed with volatility -- is the single most important nonlinear effect in the GKX paper, and it's Week 5's story.\n",
    "\n",
    "But don't dismiss the linear baseline. It's interpretable, stable, fast to train, and surprisingly hard to beat. Many quant funds still use linear models as their primary signal -- not because they're ignorant of neural nets, but because the marginal improvement often doesn't justify the complexity, overfitting risk, and the fact that you can no longer explain to your risk manager what the model is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Here's what you built and discovered in this session:\n",
    "\n",
    "- **Feature engineering is mostly data plumbing.** Computing 10 features for 50 stocks across 14 years required careful temporal alignment (the `shift(-1)` for targets, the skip-1 for momentum), missing data handling, and cross-sectional rank normalization. Getting the time alignment wrong by a single period is the difference between a genuine prediction and a look-ahead bias that inflates everything downstream.\n",
    "\n",
    "- **The regularization path is a diagnostic tool, not just a tuning method.** The Lasso path reveals feature importance through elimination order; the Ridge path shows how aggressively the data wants to be shrunk. The optimal alpha shifts with market regimes -- higher regularization during volatile periods when signal-to-noise deteriorates.\n",
    "\n",
    "- **Evaluation methodology is more important than model architecture.** The same Ridge model evaluated three ways produced ICs differing by 2-5x. Shuffled 5-fold CV is structurally dishonest for temporal data. Expanding-window CV with purging is the minimum honest standard. Always ask \"how was this evaluated?\" before trusting any financial ML result.\n",
    "\n",
    "- **Individual feature ICs are small but real, and non-stationary.** The strongest features have average ICs of 0.02-0.06. Rolling IC plots reveal that even the most robust signals go through extended weakness. The value of combining features is diversification, not amplification.\n",
    "\n",
    "- **Linear models provide a hard-to-beat baseline, but have a low ceiling.** The Ridge model modestly outperforms the best single feature. But the improvement is bounded by the linear assumption -- no feature interactions. Trees and neural nets lift this ceiling.\n",
    "\n",
    "In the homework, you'll scale this pipeline to 200+ stocks, compare all four linear models (OLS, Ridge, Lasso, Elastic Net), build a long-short portfolio, and measure performance net of transaction costs. You'll discover that transaction costs change the model comparison in surprising ways -- Lasso's sparser predictions may produce lower turnover, partially offsetting its lower gross IC."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}