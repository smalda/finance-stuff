{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-00",
   "metadata": {},
   "source": "# Week 1 Seminar — Getting Your Hands Dirty with Market Data\n\nYou've heard the theory. Now let's see if it holds up when you actually touch the data. The lecture told you that fat tails exist, that survivorship bias is bad, that dollar bars are better, and that transaction costs matter. Fine — but how do fat tails compare across *asset classes* — equities vs. bonds vs. commodities vs. volatility products? Do dollar bars help every ETF equally, or does the benefit depend on liquidity? Can you actually put a dollar figure on survivorship bias? And at what cost level does a decent-looking strategy die?\n\nThese are the questions we'll answer today. Four exercises, each one designed to produce a number that surprises you. By the end, you'll have hard evidence — not just intuitions — for why these issues matter. And you'll have a much better sense of which asset classes behave nicely and which ones make your distributional assumptions look absurd."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-01",
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np, pandas as pd, matplotlib.pyplot as plt, yfinance as yf\nfrom scipy import stats\nplt.rcParams.update({'figure.figsize': (12, 6), 'font.size': 12, 'axes.grid': True, 'grid.alpha': 0.3})\nget_close = lambda d: d['Close'] if isinstance(d.columns, pd.MultiIndex) else d[['Close']]\n# ── Download ALL data for ALL four exercises ──\n# 10 ETFs spanning equities, bonds, commodities, volatility\ntickers = ['SPY','QQQ','TLT','GLD','USO','EEM','IWM','HYG','XLU','VIXY']\nraw = yf.download(tickers, start='2010-01-01', end='2024-01-01', auto_adjust=True)\nprices = get_close(raw)\n# Survivorship bias data (Exercise 3)\nsurvivors = ['AAPL','MSFT','JNJ','JPM','PG','UNH','HD','V','DIS','MRK','KO','PEP','CSCO','ABT','WMT','CMCSA','AMGN','LOW']\nremoved = ['GE','XRX','GPS','FLR','HRB','HP','LEG','NWSA','IPG','AIZ']\nbias_raw = yf.download(survivors + removed, start='2010-01-01', end='2024-01-01', auto_adjust=True)\nbias_prices = get_close(bias_raw)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-02",
   "metadata": {},
   "source": "## Exercise 1: Fat-Tail Safari Across Asset Classes\n\nThe lecture showed you that SPY has fat tails — excess kurtosis well above zero, a QQ-plot that curls away from the diagonal. But SPY is just one data point from one asset class. Here's the question worth asking: is non-Gaussianity a *universal* market phenomenon, or does it vary dramatically depending on what you're trading?\n\nWe've downloaded 10 ETFs that span very different corners of the financial universe: SPY and QQQ (US equities), IWM (small caps), EEM (emerging markets), TLT (long-term treasuries), HYG (high-yield bonds), GLD (gold), USO (oil), XLU (utilities), and VIXY (VIX futures — pure volatility). These aren't just different tickers — they represent fundamentally different return-generating processes. Oil prices are driven by OPEC decisions and inventory reports. Treasury prices respond to Fed announcements. And VIXY is a product designed to spike when everything else crashes.\n\nIf fat tails are the same everywhere, a single distributional model covers everything. If they're different — different magnitudes, different *directions* of skewness — then your risk model needs to know which asset it's looking at. Let's find out."
  },
  {
   "cell_type": "markdown",
   "id": "cell-03",
   "metadata": {},
   "source": "### Tasks\n\n1. Compute daily log returns for all 10 ETFs.\n2. Build a statistics table: mean, standard deviation, skewness, excess kurtosis, and Jarque-Bera statistic for each ETF. Include the asset class label.\n3. Produce QQ-plots for all 10 ETFs on a single 2×5 multi-panel figure, ordered by kurtosis.\n4. Answer: does kurtosis cluster by asset class? By how much do the fattest and tamest tails differ? Does skewness tell a different story than kurtosis?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-04",
   "metadata": {},
   "outputs": [],
   "source": "# ── YOUR WORKSPACE ──\n# Compute log returns for all 10 ETFs\n# Build a comparison table: mean, std, skewness, kurtosis, JB stat\n# Produce a 2x5 grid of QQ-plots ordered by kurtosis"
  },
  {
   "cell_type": "markdown",
   "id": "cell-05",
   "metadata": {},
   "source": [
    "---\n",
    "### ▶ Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-06",
   "metadata": {},
   "outputs": [],
   "source": "log_rets = np.log(prices / prices.shift(1))\n\nasset_class = {'SPY': 'US Eq', 'QQQ': 'US Eq', 'IWM': 'US SmCap',\n               'EEM': 'EM Eq', 'TLT': 'Bonds', 'HYG': 'HY Bonds',\n               'GLD': 'Gold', 'USO': 'Oil', 'XLU': 'Util', 'VIXY': 'Vol'}\nrows = []\nfor ticker in log_rets.columns:\n    r = log_rets[ticker].dropna()\n    jb_stat, _ = stats.jarque_bera(r)\n    rows.append({'ETF': ticker, 'Class': asset_class.get(ticker, '?'),\n                 'Mean%': r.mean() * 100, 'Std%': r.std() * 100,\n                 'Skew': r.skew(), 'Kurtosis': r.kurtosis(), 'JB': jb_stat})\n\nstat_df = pd.DataFrame(rows).sort_values('Kurtosis', ascending=False).set_index('ETF')\nprint(stat_df.to_string(float_format=lambda x: f'{x:.2f}'))"
  },
  {
   "cell_type": "markdown",
   "id": "cell-07",
   "metadata": {},
   "source": "Scan the kurtosis column from top to bottom. The pattern jumps out immediately. VIXY — pure volatility — sits at the extreme, with excess kurtosis that dwarfs everything else. Oil (USO) is likely next, driven by OPEC shocks and supply disruptions that produce days when crude moves 8-10% on a single headline. The equity ETFs (SPY, QQQ, IWM, EEM) cluster in the middle with broadly similar kurtosis, though emerging markets tend to run a notch wilder than US large caps. And TLT and XLU — the \"boring\" names — sit at the bottom, still non-Gaussian, but dramatically less fat-tailed.\n\nNow look at the skewness column. This is where asset classes diverge in a way kurtosis alone can't capture. Equity ETFs tend to be *left-skewed* — their tails are fatter on the downside, because crashes are fast and violent while recoveries are slow. VIXY is likely *right-skewed* — it spikes upward when fear hits, producing extreme positive returns. GLD often has near-zero skewness but still fat tails — symmetric heavy tails, the kind of distribution Nassim Taleb builds portfolios around. These are qualitatively different risk profiles masquerading under the same \"fat tails\" label.\n\nThe QQ-plots below will make these differences visceral — watch which curves break away from the diagonal at the 2-sigma level and which ones hold steady until 4-sigma territory."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-08",
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\nfor i, ticker in enumerate(stat_df.index[:10]):\n    ax = axes.ravel()[i]\n    r = log_rets[ticker].dropna()\n    stats.probplot(r, dist='norm', plot=ax)\n    ax.set_title(f'{ticker} (κ={r.kurtosis():.1f}, s={r.skew():.2f})')\n    ax.get_lines()[0].set(markersize=2, alpha=0.4)\n\nfig.suptitle('QQ-Plots Ranked by Excess Kurtosis — Watch the Tails',\n             fontsize=14, y=1.02)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-09",
   "metadata": {},
   "source": "The QQ-plots tell the full story. The highest-kurtosis ETFs — VIXY and USO — break away from the Gaussian diagonal almost immediately, curving outward in dramatic fashion. For VIXY, the right tail dominates: the top 1% of returns are far larger than a Gaussian predicts, while the left tail is comparatively tame. For equity ETFs, the opposite pattern: the left tail (crashes) is where the real deviation lives. TLT and XLU stay closer to the diagonal longer, bending only at the extremes — they're non-Gaussian, but politely so.\n\nHere's why this matters for your ML models. A risk model that treats all 10 ETFs as having \"the same kind of fat tails\" is making at least three different mistakes. For volatility products, it underestimates upside risk. For equities, it underestimates downside risk. For bonds, it overestimates both. The variation across asset classes isn't noise — it's a structural feature of how different markets process information and absorb shocks. Any feature engineering or distribution-fitting you do later in this course needs to be *asset-aware*, not one-size-fits-all.\n\nThe lecture told you fat tails exist. Now you know they vary by an order of magnitude across asset classes, they have different *shapes* (left-skewed vs. right-skewed vs. symmetric), and the pattern is predictable from the type of asset. In the homework, you'll compute this for 200 stocks and discover that even within equities, the kurtosis distribution is bimodal."
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": "## Exercise 2: Dollar Bars at Scale — 10 ETFs\n\nThe lecture showed a 5-line \"napkin version\" of dollar bars on SPY and noted that they bring returns closer to Gaussian. Lopez de Prado makes a stronger claim: dollar bars are *strictly better* than time bars as inputs for ML models. But SPY is the most liquid security on the planet — over $30 billion in daily volume. It's the easiest possible test case.\n\nHere's the real question: does the dollar-bar advantage generalize? When you try dollar bars on a thinly-traded utilities ETF, or a volatile commodity fund, or a volatility product with bizarre volume patterns, does the kurtosis reduction hold up? Does it get better or worse? And does adding volume bars to the comparison change the story?\n\nWe'll build time bars, volume bars, *and* dollar bars for all 10 ETFs, choose per-ETF thresholds based on each asset's own trading patterns, and produce a comprehensive comparison table. By the end, you'll know exactly when dollar bars earn their keep and when they're more trouble than they're worth."
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": "### Tasks\n\n1. Write `make_dollar_bars` and `make_volume_bars` functions that take a DataFrame and a threshold and return OHLCV bars.\n2. For each of the 10 ETFs, compute the median daily dollar volume and median daily share volume. Use these as per-ETF thresholds.\n3. Build time bars (daily), volume bars, and dollar bars for all 10 ETFs. Compute log returns for each bar type.\n4. Build a comparison table: ETF, bar type, number of bars, excess kurtosis, Jarque-Bera statistic.\n5. Answer: does the kurtosis reduction from dollar bars depend on the ETF's liquidity? Which ETFs benefit most, and which least?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "# ── YOUR WORKSPACE ──\n# Build make_dollar_bars and make_volume_bars functions\n# Apply to all 10 ETFs with per-ETF thresholds\n# Compute returns for each bar type, build comparison table"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "### ▶ Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": "def make_dollar_bars(df, dollar_threshold):\n    \"\"\"Build OHLCV bars, one per fixed dollar volume traded.\"\"\"\n    dollar_vol = df['Close'] * df['Volume']\n    cum_dollars = dollar_vol.cumsum()\n    bar_ids = (cum_dollars // dollar_threshold).astype(int)\n    return df.groupby(bar_ids).agg(\n        {'Open': 'first', 'High': 'max',\n         'Low': 'min', 'Close': 'last', 'Volume': 'sum'})\n\ndef make_volume_bars(df, volume_threshold):\n    \"\"\"Build OHLCV bars, one per fixed number of shares traded.\"\"\"\n    cum_vol = df['Volume'].cumsum()\n    bar_ids = (cum_vol // volume_threshold).astype(int)\n    return df.groupby(bar_ids).agg(\n        {'Open': 'first', 'High': 'max',\n         'Low': 'min', 'Close': 'last', 'Volume': 'sum'})"
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": "Both functions follow the same logic: accumulate a running total (dollar volume or share volume), and cut a new bar every time the total crosses the threshold. On a high-activity day you get multiple bars; on a quiet day you get zero or one. That's the entire insight — we're sampling by *information flow*, not by the clock.\n\nThe critical design choice is the threshold. Too low and you get noisy micro-bars dominated by bid-ask bounce. Too high and you get so few bars that you're back to weekly data. Using each ETF's median daily value as the threshold gives roughly the same number of bars as trading days — an apples-to-apples comparison. But that \"roughly\" is doing real work: a calm August week produces fewer bars, while an FOMC announcement week produces many more.\n\nNow let's apply both functions to all 10 ETFs and see which ones benefit most from the resampling."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": "rows = []\nfor ticker in tickers:\n    df = raw.xs(ticker, axis=1, level=1).dropna(subset=['Close'])\n    time_ret = np.log(df['Close'] / df['Close'].shift(1)).dropna()\n\n    daily_dollar = df['Close'] * df['Volume']\n    d_bars = make_dollar_bars(df, daily_dollar.median())\n    v_bars = make_volume_bars(df, df['Volume'].median())\n\n    d_ret = np.log(d_bars['Close'] / d_bars['Close'].shift(1)).dropna()\n    v_ret = np.log(v_bars['Close'] / v_bars['Close'].shift(1)).dropna()\n\n    for label, ret in [('Time', time_ret), ('Volume', v_ret), ('Dollar', d_ret)]:\n        jb = stats.jarque_bera(ret)\n        rows.append({'ETF': ticker, 'Bar': label, 'N': len(ret),\n                     'Kurt': ret.kurtosis(), 'JB': jb.statistic})\n\ncomp = pd.DataFrame(rows)\npivot_k = comp.pivot(index='ETF', columns='Bar', values='Kurt')[['Time','Volume','Dollar']]\npivot_k['%Δ'] = ((pivot_k['Dollar'] - pivot_k['Time']) / pivot_k['Time'] * 100)\npivot_k = pivot_k.sort_values('Time', ascending=False)\nprint(pivot_k.to_string(float_format=lambda x: f'{x:.1f}'))"
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": "Read the %Δ column — it tells you how much kurtosis *changed* when switching from time bars to dollar bars (negative means improvement). The pattern is striking. The liquid, high-volume ETFs — SPY, QQQ, IWM — show substantial kurtosis reductions, likely in the 20-50% range. These are assets where daily volume swings dramatically between quiet days and event days, so resampling by dollar volume genuinely normalizes the information content per bar.\n\nNow look at the lower-volume ETFs — XLU, USO, possibly HYG. The improvement is much smaller, perhaps only 5-15%. For very illiquid names, you might even see kurtosis *increase* with dollar bars, because the threshold produces bars that span multiple days, introducing stale-price effects. This is the honest answer about dollar bars: they're not a magic bullet.\n\nThe volume bars (middle column) tell an intermediate story. They help, but not as much as dollar bars for the liquid names — because volume bars don't account for price changes. A million shares of SPY at $450 carries different information content than a million shares at $350. Dollar bars capture that; volume bars don't. Let's see this as a chart."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "fig, ax = plt.subplots(figsize=(14, 6))\nx = np.arange(len(pivot_k))\nw = 0.25\n\nax.bar(x - w, pivot_k['Time'], w, label='Time Bars', color='#e74c3c', alpha=0.8)\nax.bar(x, pivot_k['Volume'], w, label='Volume Bars', color='#f39c12', alpha=0.8)\nax.bar(x + w, pivot_k['Dollar'], w, label='Dollar Bars', color='#2ecc71', alpha=0.8)\nax.set_xticks(x)\nax.set_xticklabels(pivot_k.index, rotation=45)\nax.set_ylabel('Excess Kurtosis')\nax.set_title('Kurtosis by Bar Type — Dollar Bars Help Most Where Needed Most')\nax.legend()\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": "The chart makes the pattern visceral. For the fattiest-tailed ETFs on the left — the ones where non-Gaussianity is most extreme — the green bars (dollar) sit noticeably lower than the red bars (time). Dollar bars are doing real work where it matters most. For the tamest ETFs on the right — where time-bar kurtosis is already relatively low — all three bar types produce similar results. The free lunch has a variable price tag.\n\nThis has a practical consequence for your ML pipeline. If you're building a multi-asset model that takes positions across equities, bonds, commodities, and volatility products, a blanket switch to dollar bars helps some assets dramatically and others barely at all. For liquid assets with large intraday volume variation (SPY on earnings day vs. SPY in August), dollar bars are a genuine improvement. For thinly-traded assets with sparse, lumpy volume, the threshold selection becomes a new source of researcher discretion — and researcher discretion is a polite name for overfitting.\n\nThe lecture told you dollar bars are better. Now you know *how much* better depends on the asset — and the pattern is predictable from liquidity and volume variability. In the homework, you'll integrate dollar bars into a `DataLoader` class and discover that threshold selection at scale introduces its own set of design decisions."
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Exercise 3: Putting a Price on Survivorship Bias\n",
    "\n",
    "The lecture told the story of Enron and WorldCom — companies that vanished and took their data with them. It told you survivorship bias is bad. But *how* bad? Two percent per year? Five? Is it enough to matter, or is it a theoretical concern that doesn't move the needle in practice?\n",
    "\n",
    "Let's put a number on it. We'll take a list of S&P 500 constituents from around 2010, compare it to the current list, and measure the return gap between the stocks that survived and the stocks that were removed. The survivors are the ones you'd see if you downloaded \"S&P 500 data\" today and backtested to 2010. The removed companies are the ghosts your model never trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "1. Using the pre-defined lists of survivors and removed stocks (downloaded in the imports cell), compute the annualized return of each stock from 2010 to 2024 (or to removal date, whichever comes first).\n",
    "2. Compute the average annualized return for each group.\n",
    "3. Calculate the survivorship bias premium (survivors minus removed).\n",
    "4. Compound that annual premium over 14 years. Answer: if your backtest shows 15% annualized, how many of those percentage points might be ghosts?\n",
    "\n",
    "**Removed stocks provided** (these were in or near the S&P 500 around 2010 and have since been removed):\n",
    "\n",
    "| Ticker | Company | Removed | Reason |\n",
    "|--------|---------|---------|--------|\n",
    "| GE | General Electric | 2018 | Underperformance (was in index 122 years) |\n",
    "| XRX | Xerox | 2003/2017 | Decline |\n",
    "| GPS | Gap Inc | 2020 | Retail decline |\n",
    "| FLR | Fluor Corp | 2019 | Earnings collapse |\n",
    "| HRB | H&R Block | 2013 | Shrinking market cap |\n",
    "| HP | Helmerich & Payne | 2020 | Energy downturn |\n",
    "| LEG | Leggett & Platt | 2019 | Market cap decline |\n",
    "| NWSA | News Corp | various | Corporate restructuring |\n",
    "| IPG | Interpublic Group | various | Removed/readded |\n",
    "| AIZ | Assurant | 2016 | Dropped below threshold |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── YOUR WORKSPACE ──\n",
    "# Compute annualized return for each survivor and removed stock\n",
    "# Compare group averages\n",
    "# Calculate the compound bias over 14 years\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "---\n",
    "### ▶ Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": "def annualized_return(series):\n    \"\"\"Annualized return from a price series.\"\"\"\n    clean = series.dropna()\n    if len(clean) < 252:\n        return np.nan\n    n_yr = (clean.index[-1] - clean.index[0]).days / 365.25\n    total = clean.iloc[-1] / clean.iloc[0]\n    return total ** (1 / n_yr) - 1 if (total > 0 and n_yr > 0) else np.nan\n\nresults = []\nfor ticker in survivors + removed:\n    if ticker in bias_prices.columns:\n        ann = annualized_return(bias_prices[ticker])\n        grp = 'Survivor' if ticker in survivors else 'Removed'\n        if not np.isnan(ann):\n            results.append({'Ticker': ticker, 'Group': grp, 'Ann': ann})"
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "We now have annualized returns for both groups. Before we look at the averages, it's worth scanning the individual stocks. The survivors should look like a greatest-hits album — many of them are companies that rode the post-2010 bull market to multi-bagger returns (think AAPL, AMGN, UNH). The removed stocks tell a different story: GE, once the most valuable company on Earth, has underperformed badly. GPS (Gap) was gutted by the retail apocalypse. FLR collapsed when its earnings evaporated.\n",
    "\n",
    "The key insight is that you wouldn't see the removed group at all in a typical backtest. They've been silently replaced by companies like Tesla, which entered the S&P 500 in 2020. Your backtest gets Tesla's 2020 return and misses whatever stock Tesla replaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(results)\n",
    "\n",
    "surv_avg = res_df[res_df['Group'] == 'Survivor']['Ann'].mean()\n",
    "remv_avg = res_df[res_df['Group'] == 'Removed']['Ann'].mean()\n",
    "bias = surv_avg - remv_avg\n",
    "n_years = 14\n",
    "\n",
    "surv_cum = (1 + surv_avg) ** n_years - 1\n",
    "remv_cum = (1 + remv_avg) ** n_years - 1\n",
    "\n",
    "print(f'Survivor avg ann. return:  {surv_avg:+.2%}')\n",
    "print(f'Removed avg ann. return:   {remv_avg:+.2%}')\n",
    "print(f'Survivorship bias premium: {bias:+.2%} / year')\n",
    "print(f'\\nOver {n_years} years, compounded:')\n",
    "print(f'  Survivors cumulative:  {surv_cum:+.0%}')\n",
    "print(f'  Removed cumulative:    {remv_cum:+.0%}')\n",
    "print(f'  Cumulative overstate:  {surv_cum - remv_cum:+.0%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "The numbers speak for themselves. The survivorship bias premium is likely in the range of 2-5% per year — and that's a *lower bound*, because we can only measure it for removed stocks that still have downloadable data. Enron returned -100%. Lehman Brothers returned -100%. WorldCom returned -100%. Those zeros don't show up in our removed-stock average because `yfinance` can't download them. The true bias is worse than what we measured.\n",
    "\n",
    "Compound that annual premium over a 14-year backtest horizon and the overstatement becomes staggering. If the true annual bias is, say, 3.5%, that compounds to roughly a 60-70% overstatement of cumulative returns. Concretely: if your backtest shows 15% annualized returns, somewhere around 3-4 of those percentage points might be ghosts — returns that came from stocks being silently replaced by better-performing ones. Your model didn't earn those returns. The index reconstitution gave them to you for free.\n",
    "\n",
    "This is why professional quants pay thousands of dollars a year for survivorship-bias-free databases like CRSP. With free data from `yfinance`, you can document the bias and acknowledge it, but you cannot eliminate it. Every backtest you run with free data carries this invisible inflation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## Exercise 4: Death by a Thousand Cuts — Transaction Costs\n",
    "\n",
    "Most academic papers show you a beautiful equity curve and then bury the words \"transaction costs are not included\" in a footnote on page 23. Let's see what happens when you drag those costs out of the footnote and into the P&L.\n",
    "\n",
    "We'll implement the simplest possible timing strategy — the 50/200-day moving average crossover that every intro-to-trading textbook loves. At zero cost, it looks decent. The question is: at what cost level does it die? Is it 5 basis points per side? 10? 20? And once you know the answer, you'll understand why every model we build in this course includes a transaction cost estimate — because without one, you're running a fantasy backtest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "1. Compute the 50-day and 200-day simple moving averages of SPY's close price.\n",
    "2. Generate a signal: long SPY when MA(50) > MA(200), else flat (cash). Shift the signal by one day to avoid look-ahead bias.\n",
    "3. Compute daily strategy returns at four cost levels: 0, 5, 10, and 20 basis points per side. Costs are incurred on each day the signal changes.\n",
    "4. Plot cumulative returns for each cost level alongside buy-and-hold.\n",
    "5. Answer: at what cost level does the strategy underperform buy-and-hold? What does that imply for any strategy you build?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── YOUR WORKSPACE ──\n",
    "# Build the moving average crossover strategy\n",
    "# Apply costs at 0, 5, 10, 20 bps per side\n",
    "# Plot cumulative returns for each cost level vs. buy-and-hold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "---\n",
    "### ▶ Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_close = prices['SPY'].dropna()\n",
    "spy_ret = np.log(spy_close / spy_close.shift(1)).dropna()\n",
    "\n",
    "ma50 = spy_close.rolling(50).mean()\n",
    "ma200 = spy_close.rolling(200).mean()\n",
    "signal = (ma50 > ma200).astype(int)\n",
    "signal = signal.shift(1).reindex(spy_ret.index).dropna()\n",
    "spy_ret = spy_ret.reindex(signal.index)\n",
    "\n",
    "trades = signal.diff().abs().fillna(0)\n",
    "n_trades = int(trades.sum())\n",
    "\n",
    "print(f'Period: {signal.index[0].strftime(\"%Y-%m-%d\")} to '\n",
    "      f'{signal.index[-1].strftime(\"%Y-%m-%d\")}')\n",
    "print(f'Signal changes (trades): {n_trades}')\n",
    "print(f'Avg holding period: ~{len(signal) // max(n_trades, 1)} days')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "The moving average crossover is intentionally a low-frequency strategy — it trades only when a slow trend reverses, so the number of round trips over 14 years is modest. That's actually the *best case* for surviving transaction costs. A daily-rebalancing strategy would trade 252 times per year; this one trades maybe a dozen times. If even this strategy gets killed by costs, imagine what happens to something that trades every day.\n",
    "\n",
    "Now let's compute returns at each cost level and see where the equity curves diverge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": "cost_bps_list = [0, 5, 10, 20]\nstrat_curves = {}\nbh_curve = spy_ret.cumsum().apply(np.exp)\nbh_ann = spy_ret.mean() * 252\n\nfor bps in cost_bps_list:\n    cost = bps / 10_000\n    net = signal * spy_ret - trades * cost\n    strat_curves[bps] = {'curve': net.cumsum().apply(np.exp),\n                          'ann': net.mean() * 252}\n\nfor bps in cost_bps_list:\n    r = strat_curves[bps]\n    print(f\"  {bps:2d} bps/side  ->  ann. return: {r['ann']:+.2%}\")\nprint(f\"  Buy & Hold ->  ann. return: {bh_ann:+.2%}\")"
  },
  {
   "cell_type": "markdown",
   "id": "lonxbta01u",
   "source": "Look at how the annualized return decays as costs increase. The gap between 0 bps and 20 bps might not sound like much in percentage terms, but compounded over 14 years it's the difference between a strategy worth running and one that's worse than doing nothing. Let's see this as an equity curve, where the divergence over time makes the damage visceral.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "l46zt21fir",
   "source": "fig, ax = plt.subplots(figsize=(14, 7))\ncolors = ['#2ecc71', '#f39c12', '#e67e22', '#e74c3c']\nfor bps, c in zip(cost_bps_list, colors):\n    r = strat_curves[bps]\n    ax.plot(r['curve'], color=c, lw=1.5,\n            label=f'{bps} bps/side (ann: {r[\"ann\"]:.1%})')\nax.plot(bh_curve, color='steelblue', lw=2, ls='--',\n        label=f'Buy & Hold (ann: {bh_ann:.1%})')\nax.set_yscale('log')\nax.set_ylabel('Cumulative Return (log scale)')\nax.set_title('Transaction Costs: The Silent Strategy Killer')\nax.legend(loc='upper left')\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "The chart tells a clear story. At zero cost, the moving average crossover looks like a reasonable strategy — it avoids some drawdowns and captures most of the upside. But watch what happens as you add costs. At 5 bps per side, the curve dips noticeably. At 10 bps, the gap with buy-and-hold narrows to almost nothing. At 20 bps, the strategy likely underperforms buy-and-hold over the full period.\n",
    "\n",
    "And remember: this is a *low-turnover* strategy. It trades maybe 10-20 times over 14 years. A typical quant strategy trades weekly or daily. The damage from costs scales linearly with turnover, so a daily-rebalancing model at 10 bps round-trip would lose roughly 25% of its gross return to costs per year. For reference, the average hedge fund's gross return is about 10-15%. You'd be donating most of your edge to market makers.\n",
    "\n",
    "This is the most important practical lesson from today's seminar. A strategy that looks brilliant on paper can be worthless in practice if it trades too frequently or if the assets it trades have wide spreads. Every model in this course will include a transaction cost estimate — because without one, you're living in fantasy land. The breakeven cost level for your strategy is the single most important number in your backtest, and most academic papers never report it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": "---\n\n## Summary\n\nFour exercises, four numbers that should change how you think about financial data:\n\n- **Fat tails vary by an order of magnitude across asset classes — and they have different *shapes*.** Volatility products (VIXY) can exceed kurtosis of 30, oil (USO) pushes 15-20, while bonds and utilities sit in the 5-8 range. Skewness diverges too: equities are left-skewed (crash risk), volatility is right-skewed (spike risk), gold is nearly symmetric. A risk model that treats all assets the same is making different mistakes in different directions.\n\n- **Dollar bars help most where they're needed most — on liquid ETFs.** SPY and QQQ see 20-50% kurtosis reduction; thinly-traded ETFs see 5-15% at best. The dollar-bar advantage is proportional to each asset's volume variability. For illiquid assets, threshold selection becomes a new source of researcher discretion — which is a polite name for overfitting.\n\n- **Survivorship bias inflates backtest returns by an estimated 2-5% per year.** Compounded over 14 years, that's a 30-70% overstatement of cumulative returns. And our estimate is a lower bound — the true disasters are unmeasurable because the data no longer exists.\n\n- **Even a low-turnover strategy dies at moderate cost levels.** The moving average crossover underperforms buy-and-hold somewhere around 10-20 bps per side. A daily-rebalancing strategy would need far lower costs to survive.\n\nYou've done this for 10 ETFs. In the homework, you'll scale to 200 stocks — and discover that the patterns get more interesting. The kurtosis distribution across the full universe is bimodal. The data quality issues multiply. And you'll build a `DataLoader` class that handles these pathologies systematically, so every model you build for the rest of the course starts from clean, honest data."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}