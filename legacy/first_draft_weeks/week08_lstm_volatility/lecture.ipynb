{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8 --- Sequence Models: LSTM/GRU for Volatility\n",
    "\n",
    "**Quantitative Finance ML Course**\n",
    "\n",
    "---\n",
    "\n",
    "## Roadmap\n",
    "\n",
    "1. Why sequence models for finance\n",
    "2. RNN to LSTM to GRU: vanishing gradients and gating\n",
    "3. Volatility forecasting as supervised learning\n",
    "4. Realized volatility estimators\n",
    "5. Classical baselines: GARCH(1,1) and HAR\n",
    "6. LSTM architecture choices\n",
    "7. Loss functions: MSE and QLIKE\n",
    "8. Demo: LSTM vs GARCH for SPY volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f'PyTorch version: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Why Sequence Models for Finance?\n",
    "\n",
    "Financial time series have **temporal dependencies**:\n",
    "\n",
    "- **Volatility clustering**: high-vol days follow high-vol days (ARCH effects)\n",
    "- **Regime persistence**: bull/bear markets persist for months\n",
    "- **Autocorrelation in volatility**: RV is strongly autocorrelated at multiple horizons\n",
    "- **Asymmetric response**: negative returns increase vol more than positive returns (leverage effect)\n",
    "\n",
    "Feedforward nets (Week 7) treat each observation independently. Sequence models (RNN/LSTM/GRU) explicitly model the **history** of each time series.\n",
    "\n",
    "### Key Application: Volatility Forecasting\n",
    "\n",
    "Volatility forecasting is a natural fit for sequence models because:\n",
    "1. The target (realized vol) is strongly autocorrelated\n",
    "2. The features are sequential (lagged returns, lagged vol)\n",
    "3. Classical models (GARCH) already exploit this temporal structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. RNN to LSTM to GRU\n",
    "\n",
    "### Vanilla RNN\n",
    "\n",
    "$$h_t = \\tanh(W_{xh} x_t + W_{hh} h_{t-1} + b)$$\n",
    "\n",
    "**Problem**: Vanishing gradients. After ~20 time steps, the gradient decays exponentially. The network \"forgets\" early inputs.\n",
    "\n",
    "### LSTM (Long Short-Term Memory)\n",
    "\n",
    "Adds a **cell state** $c_t$ and three **gates**:\n",
    "\n",
    "| Gate | Purpose |\n",
    "|------|----------|\n",
    "| Forget gate $f_t$ | How much of the previous cell state to keep |\n",
    "| Input gate $i_t$ | How much of the new candidate to add |\n",
    "| Output gate $o_t$ | How much of the cell state to expose as output |\n",
    "\n",
    "$$f_t = \\sigma(W_f [h_{t-1}, x_t] + b_f)$$\n",
    "$$i_t = \\sigma(W_i [h_{t-1}, x_t] + b_i)$$\n",
    "$$\\tilde{c}_t = \\tanh(W_c [h_{t-1}, x_t] + b_c)$$\n",
    "$$c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$$\n",
    "$$o_t = \\sigma(W_o [h_{t-1}, x_t] + b_o)$$\n",
    "$$h_t = o_t \\odot \\tanh(c_t)$$\n",
    "\n",
    "### GRU (Gated Recurrent Unit)\n",
    "\n",
    "Simplified LSTM with 2 gates (reset and update). Fewer parameters, similar performance.\n",
    "\n",
    "$$z_t = \\sigma(W_z [h_{t-1}, x_t])$$\n",
    "$$r_t = \\sigma(W_r [h_{t-1}, x_t])$$\n",
    "$$\\tilde{h}_t = \\tanh(W [r_t \\odot h_{t-1}, x_t])$$\n",
    "$$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$$\n",
    "\n",
    "**For volatility forecasting, GRU often works as well as LSTM with less computation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Volatility Forecasting as Supervised Learning\n",
    "\n",
    "### Problem Setup\n",
    "\n",
    "Given a history of daily data up to day $t$, predict realized volatility for day $t+1$ (or the next 5 days, etc.).\n",
    "\n",
    "**Features** (sequence of length $L$):\n",
    "- Lagged realized volatility: $RV_{t}, RV_{t-1}, \\ldots, RV_{t-L+1}$\n",
    "- Daily returns: $r_t, r_{t-1}, \\ldots$\n",
    "- Volume (optional)\n",
    "\n",
    "**Target**: $RV_{t+1}$ (next-day realized volatility)\n",
    "\n",
    "This maps to a standard sequence-to-one regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Realized Volatility Estimators\n",
    "\n",
    "### Close-to-Close (Standard)\n",
    "\n",
    "$$\\hat{\\sigma}^2_{CC} = \\sum_{i=1}^{n} r_i^2$$\n",
    "\n",
    "where $r_i$ are intraday returns (or simply $r^2$ for daily).\n",
    "\n",
    "### Parkinson (Range-Based)\n",
    "\n",
    "$$\\hat{\\sigma}^2_P = \\frac{1}{4 \\ln 2} (\\ln H - \\ln L)^2$$\n",
    "\n",
    "Uses high and low prices. More efficient than close-to-close.\n",
    "\n",
    "### Garman-Klass\n",
    "\n",
    "$$\\hat{\\sigma}^2_{GK} = 0.5 (\\ln H - \\ln L)^2 - (2\\ln 2 - 1)(\\ln C - \\ln O)^2$$\n",
    "\n",
    "Uses open, high, low, close. Most efficient single-day estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Realized volatility estimators ---\n",
    "\n",
    "def rv_close_to_close(returns, window=5):\n",
    "    \"\"\"Close-to-close realized volatility (rolling sum of squared returns).\"\"\"\n",
    "    return returns.pow(2).rolling(window).sum().apply(np.sqrt)\n",
    "\n",
    "\n",
    "def rv_parkinson(high, low, window=5):\n",
    "    \"\"\"Parkinson range-based estimator.\"\"\"\n",
    "    log_hl = (np.log(high) - np.log(low)) ** 2\n",
    "    return (log_hl / (4 * np.log(2))).rolling(window).mean().apply(np.sqrt)\n",
    "\n",
    "\n",
    "def rv_garman_klass(open_p, high, low, close, window=5):\n",
    "    \"\"\"Garman-Klass estimator.\"\"\"\n",
    "    log_hl = (np.log(high) - np.log(low)) ** 2\n",
    "    log_co = (np.log(close) - np.log(open_p)) ** 2\n",
    "    gk = 0.5 * log_hl - (2 * np.log(2) - 1) * log_co\n",
    "    return gk.rolling(window).mean().apply(np.sqrt)\n",
    "\n",
    "\n",
    "print('RV estimators defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate synthetic daily data mimicking SPY ---\n",
    "\n",
    "np.random.seed(42)\n",
    "n_days = 2520  # ~10 years of trading days\n",
    "\n",
    "# Simulate GARCH(1,1) process for returns\n",
    "mu = 0.0003  # daily drift\n",
    "omega = 1e-6\n",
    "alpha = 0.08\n",
    "beta = 0.90\n",
    "\n",
    "returns = np.zeros(n_days)\n",
    "sigma2 = np.zeros(n_days)\n",
    "sigma2[0] = omega / (1 - alpha - beta)  # unconditional variance\n",
    "\n",
    "for t in range(1, n_days):\n",
    "    sigma2[t] = omega + alpha * returns[t-1]**2 + beta * sigma2[t-1]\n",
    "    returns[t] = mu + np.sqrt(sigma2[t]) * np.random.randn()\n",
    "\n",
    "# Generate OHLC from returns\n",
    "close = 100 * np.exp(np.cumsum(returns))\n",
    "# Simulate intraday range\n",
    "daily_vol = np.sqrt(sigma2)\n",
    "high = close * np.exp(np.abs(np.random.randn(n_days)) * daily_vol * 0.5)\n",
    "low = close * np.exp(-np.abs(np.random.randn(n_days)) * daily_vol * 0.5)\n",
    "open_p = close * np.exp(np.random.randn(n_days) * daily_vol * 0.2)\n",
    "\n",
    "# Ensure high >= close >= low and high >= open >= low\n",
    "high = np.maximum(high, np.maximum(close, open_p)) * 1.001\n",
    "low = np.minimum(low, np.minimum(close, open_p)) * 0.999\n",
    "\n",
    "# Create DataFrame\n",
    "dates = pd.bdate_range('2015-01-02', periods=n_days)\n",
    "spy = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'open': open_p, 'high': high, 'low': low, 'close': close,\n",
    "    'return': returns,\n",
    "    'volume': np.random.lognormal(18, 0.5, n_days)  # synthetic volume\n",
    "}).set_index('date')\n",
    "\n",
    "# Compute realized volatility (5-day, annualized)\n",
    "spy['rv_cc'] = rv_close_to_close(spy['return'], window=5) * np.sqrt(252/5)\n",
    "spy['rv_park'] = rv_parkinson(spy['high'], spy['low'], window=5) * np.sqrt(252)\n",
    "spy['rv_gk'] = rv_garman_klass(spy['open'], spy['high'], spy['low'], spy['close'], window=5) * np.sqrt(252)\n",
    "\n",
    "spy = spy.dropna()\n",
    "print(f'SPY data: {len(spy)} days')\n",
    "spy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot RV estimators ---\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "axes[0].plot(spy.index, spy['return'], alpha=0.5, linewidth=0.5)\n",
    "axes[0].set_ylabel('Daily Return')\n",
    "axes[0].set_title('SPY Daily Returns (Simulated)')\n",
    "\n",
    "axes[1].plot(spy.index, spy['rv_cc'], label='Close-to-Close', alpha=0.8)\n",
    "axes[1].plot(spy.index, spy['rv_park'], label='Parkinson', alpha=0.8)\n",
    "axes[1].plot(spy.index, spy['rv_gk'], label='Garman-Klass', alpha=0.8)\n",
    "axes[1].set_ylabel('Annualized RV')\n",
    "axes[1].set_title('Realized Volatility Estimators (5-day window)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Classical Baselines: GARCH(1,1) and HAR\n",
    "\n",
    "### GARCH(1,1)\n",
    "\n",
    "$$\\sigma_t^2 = \\omega + \\alpha \\epsilon_{t-1}^2 + \\beta \\sigma_{t-1}^2$$\n",
    "\n",
    "The workhorse of volatility modeling. Captures volatility clustering with 3 parameters.\n",
    "\n",
    "### HAR (Heterogeneous Autoregressive) Model\n",
    "\n",
    "$$RV_{t+1} = \\beta_0 + \\beta_D RV_t^{(D)} + \\beta_W RV_t^{(W)} + \\beta_M RV_t^{(M)} + \\epsilon_t$$\n",
    "\n",
    "where:\n",
    "- $RV^{(D)}$ = daily RV (1 day)\n",
    "- $RV^{(W)}$ = weekly RV (average of last 5 days)\n",
    "- $RV^{(M)}$ = monthly RV (average of last 22 days)\n",
    "\n",
    "Simple, linear, and surprisingly hard to beat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GARCH(1,1) baseline ---\n",
    "# Simple implementation: fit on training data, forecast one-step-ahead\n",
    "\n",
    "from arch import arch_model\n",
    "\n",
    "# Use close-to-close RV as our target\n",
    "# For GARCH, we fit on returns directly\n",
    "rv_col = 'rv_cc'\n",
    "\n",
    "# Temporal split\n",
    "train_end = int(len(spy) * 0.6)\n",
    "val_end = int(len(spy) * 0.8)\n",
    "\n",
    "train = spy.iloc[:train_end]\n",
    "val = spy.iloc[train_end:val_end]\n",
    "test = spy.iloc[val_end:]\n",
    "\n",
    "print(f'Train: {len(train)} days, Val: {len(val)} days, Test: {len(test)} days')\n",
    "\n",
    "# Fit GARCH on training returns\n",
    "garch = arch_model(train['return'] * 100, vol='GARCH', p=1, q=1, mean='Constant')\n",
    "garch_result = garch.fit(disp='off')\n",
    "print(f'\\nGARCH(1,1) parameters:')\n",
    "print(garch_result.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GARCH one-step-ahead forecasts on test set ---\n",
    "# Re-fit on train+val, forecast on test\n",
    "all_returns = spy['return'] * 100\n",
    "\n",
    "garch_full = arch_model(all_returns.iloc[:val_end], vol='GARCH', p=1, q=1, mean='Constant')\n",
    "garch_full_result = garch_full.fit(disp='off')\n",
    "\n",
    "# Rolling one-step forecast\n",
    "garch_forecasts = []\n",
    "for t in range(val_end, len(spy)):\n",
    "    # Use data up to t to forecast t+1 conditional volatility\n",
    "    am = arch_model(all_returns.iloc[:t], vol='GARCH', p=1, q=1, mean='Constant')\n",
    "    res = am.fit(disp='off', last_obs=t)\n",
    "    fc = res.forecast(horizon=1)\n",
    "    # Convert variance to annualized vol\n",
    "    garch_vol = np.sqrt(fc.variance.values[-1, 0]) / 100 * np.sqrt(252)\n",
    "    garch_forecasts.append(garch_vol)\n",
    "\n",
    "test_copy = test.copy()\n",
    "test_copy['garch_pred'] = garch_forecasts\n",
    "print(f'GARCH forecasts: {len(garch_forecasts)} days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- HAR baseline ---\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Compute HAR features\n",
    "spy['rv_d'] = spy[rv_col]  # daily\n",
    "spy['rv_w'] = spy[rv_col].rolling(5).mean()  # weekly\n",
    "spy['rv_m'] = spy[rv_col].rolling(22).mean()  # monthly\n",
    "spy['rv_target'] = spy[rv_col].shift(-1)  # next-day RV\n",
    "\n",
    "har_features = ['rv_d', 'rv_w', 'rv_m']\n",
    "spy_har = spy.dropna(subset=har_features + ['rv_target'])\n",
    "\n",
    "# Split\n",
    "train_end_har = int(len(spy_har) * 0.6)\n",
    "val_end_har = int(len(spy_har) * 0.8)\n",
    "\n",
    "har_train = spy_har.iloc[:train_end_har]\n",
    "har_test = spy_har.iloc[val_end_har:]\n",
    "\n",
    "# Fit HAR\n",
    "har_model = LinearRegression()\n",
    "har_model.fit(har_train[har_features], har_train['rv_target'])\n",
    "\n",
    "# Predict\n",
    "har_pred = har_model.predict(har_test[har_features])\n",
    "print(f'HAR coefficients: D={har_model.coef_[0]:.3f}, W={har_model.coef_[1]:.3f}, M={har_model.coef_[2]:.3f}')\n",
    "print(f'HAR intercept: {har_model.intercept_:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. LSTM Architecture for Volatility Forecasting\n",
    "\n",
    "### Design Choices\n",
    "\n",
    "| Choice | Recommendation | Reason |\n",
    "|--------|----------------|--------|\n",
    "| RNN type | LSTM or GRU | Both work; GRU is faster |\n",
    "| Layers | 1-2 | More layers rarely help for vol |\n",
    "| Hidden size | 32-64 | Vol is a \"simple\" signal |\n",
    "| Sequence length | 20-60 days | Captures weekly + monthly patterns |\n",
    "| Dropout | 0.2-0.3 | Less than cross-sectional models |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LSTM for volatility forecasting ---\n",
    "\n",
    "class VolSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that creates sequences of (features, target) for vol forecasting.\n",
    "    Features: [lagged_rv, returns, log_volume] over a window.\n",
    "    Target: next-day RV.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, feature_cols, target_col, seq_len=20):\n",
    "        self.seq_len = seq_len\n",
    "        self.features = data[feature_cols].values.astype(np.float32)\n",
    "        self.targets = data[target_col].values.astype(np.float32)\n",
    "\n",
    "        # Normalize features (z-score using training stats)\n",
    "        self.feat_mean = self.features.mean(axis=0)\n",
    "        self.feat_std = self.features.std(axis=0) + 1e-8\n",
    "        self.features = (self.features - self.feat_mean) / self.feat_std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.features[idx:idx + self.seq_len]  # (seq_len, n_features)\n",
    "        y = self.targets[idx + self.seq_len]        # scalar\n",
    "        return torch.from_numpy(X), torch.tensor(y)\n",
    "\n",
    "\n",
    "print('VolSequenceDataset ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMVolForecaster(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM for volatility forecasting.\n",
    "    Takes a sequence of features and outputs a single vol prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=32, n_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if n_layers > 1 else 0.0\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, input_dim)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        # Use the last hidden state\n",
    "        last_hidden = lstm_out[:, -1, :]  # (batch, hidden_dim)\n",
    "        out = self.head(last_hidden).squeeze(-1)  # (batch,)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Test\n",
    "model = LSTMVolForecaster(input_dim=3, hidden_dim=32, n_layers=2)\n",
    "print(model)\n",
    "x_test = torch.randn(16, 20, 3)  # batch=16, seq_len=20, features=3\n",
    "print(f'Output shape: {model(x_test).shape}')  # (16,)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Parameters: {n_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Loss Functions for Volatility\n",
    "\n",
    "### MSE\n",
    "\n",
    "$$\\mathcal{L}_{MSE} = \\frac{1}{N} \\sum_{t=1}^{N} (\\sigma_t - \\hat{\\sigma}_t)^2$$\n",
    "\n",
    "Treats all errors equally. Problem: errors on high-vol days dominate.\n",
    "\n",
    "### QLIKE\n",
    "\n",
    "$$\\mathcal{L}_{QLIKE} = \\frac{1}{N} \\sum_{t=1}^{N} \\left( \\frac{\\sigma_t^2}{\\hat{\\sigma}_t^2} - \\ln \\frac{\\sigma_t^2}{\\hat{\\sigma}_t^2} - 1 \\right)$$\n",
    "\n",
    "The quasi-likelihood loss. Better calibrated for volatility because:\n",
    "- It penalizes underestimation more than overestimation\n",
    "- It's robust to the scale of volatility\n",
    "- It's the natural loss for Gaussian MLE with heteroskedasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_pred, y_true):\n",
    "    return ((y_pred - y_true) ** 2).mean()\n",
    "\n",
    "\n",
    "def qlike_loss(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    QLIKE loss for volatility forecasting.\n",
    "    y_pred, y_true are volatilities (not variances).\n",
    "    \"\"\"\n",
    "    # Convert to variances\n",
    "    var_pred = y_pred ** 2 + 1e-8  # avoid division by zero\n",
    "    var_true = y_true ** 2 + 1e-8\n",
    "    ratio = var_true / var_pred\n",
    "    return (ratio - torch.log(ratio) - 1).mean()\n",
    "\n",
    "\n",
    "# Demo: QLIKE penalizes underestimation more\n",
    "true_vol = torch.tensor([0.20])\n",
    "over_pred = torch.tensor([0.30])   # overestimate by 10pp\n",
    "under_pred = torch.tensor([0.10])  # underestimate by 10pp\n",
    "\n",
    "print(f'QLIKE(overestimate):  {qlike_loss(over_pred, true_vol):.4f}')\n",
    "print(f'QLIKE(underestimate): {qlike_loss(under_pred, true_vol):.4f}')\n",
    "print('(Underestimation penalized more --- good for risk management!)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Demo: LSTM vs GARCH for SPY Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare data for LSTM ---\n",
    "spy['log_volume'] = np.log(spy['volume'])\n",
    "lstm_features = ['rv_cc', 'return', 'log_volume']\n",
    "target_col = 'rv_target'\n",
    "\n",
    "# Filter to valid rows\n",
    "spy_lstm = spy.dropna(subset=lstm_features + [target_col]).copy()\n",
    "\n",
    "SEQ_LEN = 20\n",
    "\n",
    "# Temporal split\n",
    "n = len(spy_lstm)\n",
    "train_end_idx = int(n * 0.6)\n",
    "val_end_idx = int(n * 0.8)\n",
    "\n",
    "train_data = spy_lstm.iloc[:train_end_idx]\n",
    "val_data = spy_lstm.iloc[train_end_idx - SEQ_LEN:val_end_idx]  # overlap for sequence\n",
    "test_data = spy_lstm.iloc[val_end_idx - SEQ_LEN:]  # overlap for sequence\n",
    "\n",
    "# Create datasets\n",
    "train_ds = VolSequenceDataset(train_data, lstm_features, target_col, seq_len=SEQ_LEN)\n",
    "val_ds = VolSequenceDataset(val_data, lstm_features, target_col, seq_len=SEQ_LEN)\n",
    "test_ds = VolSequenceDataset(test_data, lstm_features, target_col, seq_len=SEQ_LEN)\n",
    "\n",
    "# Use training stats to normalize val/test\n",
    "val_ds.feat_mean = train_ds.feat_mean\n",
    "val_ds.feat_std = train_ds.feat_std\n",
    "val_ds.features = (spy_lstm.iloc[train_end_idx - SEQ_LEN:val_end_idx][lstm_features].values.astype(np.float32) - train_ds.feat_mean) / train_ds.feat_std\n",
    "\n",
    "test_ds.feat_mean = train_ds.feat_mean\n",
    "test_ds.feat_std = train_ds.feat_std\n",
    "test_ds.features = (spy_lstm.iloc[val_end_idx - SEQ_LEN:][lstm_features].values.astype(np.float32) - train_ds.feat_mean) / train_ds.feat_std\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\n",
    "\n",
    "print(f'Train sequences: {len(train_ds)}')\n",
    "print(f'Val sequences: {len(val_ds)}')\n",
    "print(f'Test sequences: {len(test_ds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train LSTM ---\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=15):\n",
    "        self.patience = patience\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.best_state = None\n",
    "\n",
    "    def step(self, val_loss, model):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            return False\n",
    "        self.counter += 1\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "    def restore_best(self, model):\n",
    "        if self.best_state:\n",
    "            model.load_state_dict(self.best_state)\n",
    "\n",
    "\n",
    "lstm_model = LSTMVolForecaster(input_dim=len(lstm_features), hidden_dim=32, n_layers=2, dropout=0.2)\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "stopper = EarlyStopping(patience=15)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(200):\n",
    "    # Train\n",
    "    lstm_model.train()\n",
    "    epoch_loss = []\n",
    "    for X_b, y_b in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = lstm_model(X_b)\n",
    "        loss = mse_loss(pred, y_b)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(lstm_model.parameters(), 1.0)  # gradient clipping\n",
    "        optimizer.step()\n",
    "        epoch_loss.append(loss.item())\n",
    "    train_losses.append(np.mean(epoch_loss))\n",
    "\n",
    "    # Validate\n",
    "    lstm_model.eval()\n",
    "    v_losses = []\n",
    "    with torch.no_grad():\n",
    "        for X_b, y_b in val_loader:\n",
    "            v_losses.append(mse_loss(lstm_model(X_b), y_b).item())\n",
    "    val_losses.append(np.mean(v_losses))\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch {epoch:3d}: train_loss={train_losses[-1]:.6f}, val_loss={val_losses[-1]:.6f}')\n",
    "\n",
    "    if stopper.step(val_losses[-1], lstm_model):\n",
    "        print(f'Early stopping at epoch {epoch}')\n",
    "        break\n",
    "\n",
    "stopper.restore_best(lstm_model)\n",
    "print(f'Best val loss: {stopper.best_loss:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training curves ---\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(train_losses, label='Train', alpha=0.8)\n",
    "ax.plot(val_losses, label='Validation', alpha=0.8)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('MSE Loss')\n",
    "ax.set_title('LSTM Training Curves')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluate on test set ---\n",
    "lstm_model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_b, y_b in test_loader:\n",
    "        preds = lstm_model(X_b)\n",
    "        all_preds.append(preds.numpy())\n",
    "        all_targets.append(y_b.numpy())\n",
    "\n",
    "lstm_preds = np.concatenate(all_preds)\n",
    "actual_rv = np.concatenate(all_targets)\n",
    "\n",
    "# Align with HAR and GARCH predictions\n",
    "n_test = min(len(lstm_preds), len(har_pred), len(garch_forecasts))\n",
    "lstm_preds_aligned = lstm_preds[:n_test]\n",
    "har_pred_aligned = har_pred[:n_test]\n",
    "garch_pred_aligned = np.array(garch_forecasts[:n_test])\n",
    "actual_aligned = actual_rv[:n_test]\n",
    "\n",
    "# MSE comparison\n",
    "mse_lstm = np.mean((lstm_preds_aligned - actual_aligned) ** 2)\n",
    "mse_har = np.mean((har_pred_aligned - actual_aligned) ** 2)\n",
    "mse_garch = np.mean((garch_pred_aligned - actual_aligned) ** 2)\n",
    "\n",
    "# Correlation\n",
    "corr_lstm = np.corrcoef(lstm_preds_aligned, actual_aligned)[0, 1]\n",
    "corr_har = np.corrcoef(har_pred_aligned, actual_aligned)[0, 1]\n",
    "corr_garch = np.corrcoef(garch_pred_aligned, actual_aligned)[0, 1]\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['GARCH(1,1)', 'HAR', 'LSTM'],\n",
    "    'MSE': [mse_garch, mse_har, mse_lstm],\n",
    "    'Correlation': [corr_garch, corr_har, corr_lstm]\n",
    "}).set_index('Model')\n",
    "\n",
    "print('Volatility Forecasting Results (Test Set):')\n",
    "print(results.round(6).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot predictions vs actual ---\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "t = np.arange(n_test)\n",
    "\n",
    "# Actual vs predicted\n",
    "axes[0].plot(t, actual_aligned, label='Actual RV', color='black', alpha=0.6, linewidth=0.8)\n",
    "axes[0].plot(t, lstm_preds_aligned, label='LSTM', alpha=0.8, linewidth=1.2)\n",
    "axes[0].plot(t, har_pred_aligned, label='HAR', alpha=0.8, linewidth=1.2)\n",
    "axes[0].plot(t, garch_pred_aligned, label='GARCH', alpha=0.8, linewidth=1.2)\n",
    "axes[0].set_ylabel('Annualized Volatility')\n",
    "axes[0].set_title('Volatility Forecasts vs Actual (Test Set)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Forecast errors\n",
    "axes[1].plot(t, (lstm_preds_aligned - actual_aligned), label='LSTM error', alpha=0.6)\n",
    "axes[1].plot(t, (har_pred_aligned - actual_aligned), label='HAR error', alpha=0.6)\n",
    "axes[1].plot(t, (garch_pred_aligned - actual_aligned), label='GARCH error', alpha=0.6)\n",
    "axes[1].axhline(y=0, color='black', linewidth=0.5)\n",
    "axes[1].set_ylabel('Forecast Error')\n",
    "axes[1].set_xlabel('Test Day')\n",
    "axes[1].set_title('Forecast Errors')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Volatility clustering** makes forecasting possible --- vol is highly persistent.\n",
    "2. **RV estimators**: Parkinson and Garman-Klass use OHLC data and are more efficient than close-to-close.\n",
    "3. **GARCH(1,1)**: The classic baseline. Hard to beat for one-step-ahead forecasting.\n",
    "4. **HAR model**: Simple linear model using daily/weekly/monthly RV. Surprisingly competitive.\n",
    "5. **LSTM**: Can capture nonlinear patterns, but needs enough data and careful tuning.\n",
    "6. **QLIKE loss**: Better than MSE for volatility --- penalizes underestimation more.\n",
    "7. **Gradient clipping**: Essential for training LSTM on financial data.\n",
    "\n",
    "### When Does LSTM Beat Classical?\n",
    "\n",
    "- When there are **regime changes** that GARCH's exponential smoothing handles poorly\n",
    "- When you have **many assets** (cross-learning)\n",
    "- When **additional features** (volume, order flow) carry information\n",
    "- For **multi-step** forecasts where GARCH reverts to the unconditional mean too fast\n",
    "\n",
    "### Next Week\n",
    "\n",
    "Week 9: Transformers and attention for financial time series."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}