{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8 Homework --- Volatility Forecasting Showdown\n",
    "\n",
    "**Quantitative Finance ML Course**\n",
    "\n",
    "**Total: 100 points**\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this homework, you will build a comprehensive volatility forecasting pipeline:\n",
    "1. Compute realized volatility for 30 stocks\n",
    "2. Implement GARCH and HAR baselines\n",
    "3. Build an LSTM vol forecaster with proper temporal splitting\n",
    "4. Compare all models\n",
    "5. Extend with attention\n",
    "\n",
    "### Grading\n",
    "\n",
    "| Part | Points | Topic |\n",
    "|------|--------|-------|\n",
    "| 1 | 15 | Compute RV for 30 stocks |\n",
    "| 2 | 20 | GARCH + HAR baselines |\n",
    "| 3 | 30 | LSTM vol forecaster |\n",
    "| 4 | 20 | Model comparison (QLIKE + MSE) |\n",
    "| 5 | 15 | TFT or attention extension |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate synthetic data for 30 stocks ---\n",
    "\n",
    "np.random.seed(42)\n",
    "n_days = 2520  # ~10 years\n",
    "n_stocks = 30\n",
    "tickers = [f'STOCK_{i:02d}' for i in range(n_stocks)]\n",
    "\n",
    "all_data = {}\n",
    "for ticker in tickers:\n",
    "    omega = np.random.uniform(5e-7, 5e-6)\n",
    "    alpha = np.random.uniform(0.04, 0.12)\n",
    "    beta = np.random.uniform(0.82, 0.94)\n",
    "    # Ensure stationarity\n",
    "    if alpha + beta >= 0.999:\n",
    "        beta = 0.999 - alpha\n",
    "\n",
    "    returns = np.zeros(n_days)\n",
    "    sigma2 = np.zeros(n_days)\n",
    "    sigma2[0] = omega / (1 - alpha - beta)\n",
    "\n",
    "    for t in range(1, n_days):\n",
    "        sigma2[t] = omega + alpha * returns[t-1]**2 + beta * sigma2[t-1]\n",
    "        returns[t] = np.sqrt(sigma2[t]) * np.random.randn()\n",
    "\n",
    "    close = 100 * np.exp(np.cumsum(returns))\n",
    "    dv = np.sqrt(sigma2)\n",
    "    high = close * np.exp(np.abs(np.random.randn(n_days)) * dv * 0.5)\n",
    "    low = close * np.exp(-np.abs(np.random.randn(n_days)) * dv * 0.5)\n",
    "    open_p = close * np.exp(np.random.randn(n_days) * dv * 0.2)\n",
    "    high = np.maximum(high, np.maximum(close, open_p)) * 1.001\n",
    "    low = np.minimum(low, np.minimum(close, open_p)) * 0.999\n",
    "\n",
    "    all_data[ticker] = pd.DataFrame({\n",
    "        'open': open_p, 'high': high, 'low': low, 'close': close,\n",
    "        'return': returns,\n",
    "        'volume': np.random.lognormal(18, 0.5, n_days)\n",
    "    }, index=pd.bdate_range('2015-01-02', periods=n_days))\n",
    "\n",
    "print(f'Generated data for {n_stocks} stocks, {n_days} days each')\n",
    "print(f'Tickers: {tickers[:5]} ... {tickers[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Compute RV for 30 Stocks (15 pts)\n",
    "\n",
    "For each of the 30 stocks:\n",
    "1. Compute close-to-close RV (5-day window)\n",
    "2. Compute Parkinson RV (5-day window)\n",
    "3. Compute Garman-Klass RV (5-day window)\n",
    "4. Annualize all estimators\n",
    "5. Create the target: next-day RV (using close-to-close)\n",
    "\n",
    "Show summary statistics and a plot for 3 representative stocks.\n",
    "\n",
    "**Grading**: Correct estimators (5), annualization (5), plots/stats (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rv_close_to_close(returns, window=5):\n",
    "    \"\"\"TODO: Close-to-close realized volatility.\"\"\"\n",
    "    pass\n",
    "\n",
    "def rv_parkinson(high, low, window=5):\n",
    "    \"\"\"TODO: Parkinson range-based RV.\"\"\"\n",
    "    pass\n",
    "\n",
    "def rv_garman_klass(open_p, high, low, close, window=5):\n",
    "    \"\"\"TODO: Garman-Klass RV.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# TODO: Apply to all 30 stocks, add rv_cc, rv_park, rv_gk, rv_target columns\n",
    "# for ticker, stock_df in all_data.items():\n",
    "#     ...\n",
    "\n",
    "print('Part 1: implement above')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Summary statistics table and plots for 3 stocks\n",
    "\n",
    "print('Stats and plots: implement above')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: GARCH + HAR Baselines (20 pts)\n",
    "\n",
    "For each stock, implement:\n",
    "\n",
    "### GARCH(1,1)\n",
    "- Fit on training data (first 60%)\n",
    "- One-step-ahead forecasts on test data (last 20%)\n",
    "\n",
    "### HAR Model\n",
    "- Features: daily RV, 5-day mean RV, 22-day mean RV\n",
    "- Fit on training data, predict on test data\n",
    "\n",
    "**Grading**: GARCH implementation (8), HAR implementation (7), proper splitting (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: GARCH(1,1) for all 30 stocks\n",
    "# from arch import arch_model\n",
    "#\n",
    "# For each stock:\n",
    "#   1. Fit GARCH on training returns\n",
    "#   2. Forecast on test set\n",
    "#   3. Store predictions\n",
    "\n",
    "print('GARCH baselines: implement above')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: HAR model for all 30 stocks\n",
    "# For each stock:\n",
    "#   1. Create HAR features (rv_d, rv_w, rv_m)\n",
    "#   2. Fit on training data\n",
    "#   3. Predict on test data\n",
    "#   4. Store predictions and coefficients\n",
    "\n",
    "print('HAR baselines: implement above')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: LSTM Vol Forecaster (30 pts)\n",
    "\n",
    "Build an LSTM that forecasts next-day RV for all 30 stocks.\n",
    "\n",
    "**Architecture**:\n",
    "- Input features: [rv_cc, return, log_volume] (3 features)\n",
    "- Sequence length: 20 days\n",
    "- LSTM: 2 layers, hidden_dim=32\n",
    "- Head: Linear(32, 16) -> ReLU -> Linear(16, 1)\n",
    "\n",
    "**Training**:\n",
    "- Temporal split: train (60%), val (20%), test (20%)\n",
    "- MSE loss, Adam optimizer (lr=1e-3)\n",
    "- Gradient clipping (max_norm=1.0)\n",
    "- Early stopping (patience=15)\n",
    "\n",
    "You can train one model per stock or one model across all stocks (your choice, but justify it).\n",
    "\n",
    "**Grading**: Correct architecture (10), proper temporal split (8), training loop (7), predictions (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VolSequenceDataset(Dataset):\n",
    "    \"\"\"TODO: Implement sequence dataset for vol forecasting.\"\"\"\n",
    "    def __init__(self, data, feature_cols, target_col, seq_len=20):\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pass\n",
    "\n",
    "\n",
    "class LSTMVolForecaster(nn.Module):\n",
    "    \"\"\"TODO: Implement LSTM for vol forecasting.\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=32, n_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Training loop with early stopping\n",
    "# For each stock (or pooled):\n",
    "#   1. Create datasets and loaders\n",
    "#   2. Train with gradient clipping\n",
    "#   3. Early stopping on validation loss\n",
    "#   4. Predict on test set\n",
    "\n",
    "print('LSTM training: implement above')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Compare All Models (20 pts)\n",
    "\n",
    "Compare GARCH, HAR, and LSTM on the test set for all 30 stocks.\n",
    "\n",
    "**Metrics**:\n",
    "1. **MSE**: $\\frac{1}{N}\\sum(\\sigma - \\hat{\\sigma})^2$\n",
    "2. **QLIKE**: $\\frac{1}{N}\\sum\\left(\\frac{\\sigma^2}{\\hat{\\sigma}^2} - \\ln\\frac{\\sigma^2}{\\hat{\\sigma}^2} - 1\\right)$\n",
    "3. **Correlation**: between predicted and actual RV\n",
    "\n",
    "**Required outputs**:\n",
    "- Table: average metrics across all 30 stocks\n",
    "- Plot: model comparison bar chart\n",
    "- Per-stock analysis: for which stocks does LSTM win?\n",
    "\n",
    "**Grading**: Correct QLIKE (5), comprehensive table (5), plots (5), per-stock analysis (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_qlike(y_true, y_pred):\n",
    "    \"\"\"TODO: Compute QLIKE loss.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# TODO: Compute MSE, QLIKE, Correlation for each stock x model\n",
    "# Aggregate across stocks\n",
    "\n",
    "print('Model comparison: implement above')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Comparison table and bar chart\n",
    "\n",
    "print('Comparison plots: implement above')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Per-stock analysis\n",
    "# For how many stocks does LSTM beat GARCH? Beat HAR?\n",
    "# Are there patterns? (e.g., LSTM wins for high-vol stocks?)\n",
    "\n",
    "print('Per-stock analysis: implement above')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Attention Extension (15 pts)\n",
    "\n",
    "Extend the LSTM with a simple attention mechanism or implement a TFT-style (Temporal Fusion Transformer) layer.\n",
    "\n",
    "**Option A: LSTM + Attention**\n",
    "- Add a self-attention layer over LSTM hidden states\n",
    "- Instead of using only the last hidden state, compute attention-weighted average\n",
    "\n",
    "**Option B: Simple Temporal Transformer**\n",
    "- Replace LSTM with a TransformerEncoder\n",
    "- Use positional encoding for time steps\n",
    "\n",
    "Compare against the vanilla LSTM from Part 3.\n",
    "\n",
    "**Grading**: Correct implementation (8), comparison with vanilla LSTM (4), discussion (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMWithAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    TODO: LSTM with attention over hidden states.\n",
    "    Instead of using only h_T, compute:\n",
    "        alpha_t = softmax(v^T tanh(W h_t))\n",
    "        context = sum(alpha_t * h_t)\n",
    "    Then feed context to the prediction head.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=32, n_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train the attention model and compare with vanilla LSTM\n",
    "# - Use same data, same training procedure\n",
    "# - Report MSE, QLIKE, Correlation\n",
    "# - Visualize attention weights for a few sample sequences\n",
    "\n",
    "print('Attention model: implement above')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "*Write 1-2 paragraphs on:*\n",
    "\n",
    "1. Does attention help? Why or why not?\n",
    "2. What do the attention weights focus on? (recent days? high-vol days?)\n",
    "3. Is the improvement worth the added complexity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Submission Checklist\n",
    "\n",
    "- [ ] Part 1: RV computed for all 30 stocks with 3 estimators\n",
    "- [ ] Part 2: GARCH and HAR baselines produce predictions\n",
    "- [ ] Part 3: LSTM trained with proper temporal splitting\n",
    "- [ ] Part 4: Comparison table with MSE, QLIKE, Correlation\n",
    "- [ ] Part 5: Attention model implemented and compared\n",
    "- [ ] All cells run without errors\n",
    "- [ ] Notebook is clean and well-organized"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}