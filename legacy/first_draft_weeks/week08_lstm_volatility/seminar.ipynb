{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8 Seminar --- LSTM/GRU for Volatility Forecasting\n",
    "\n",
    "**Quantitative Finance ML Course**\n",
    "\n",
    "---\n",
    "\n",
    "## Today's Plan (90 min)\n",
    "\n",
    "| Time | Activity |\n",
    "|------|----------|\n",
    "| 25 min | Exercise 1: Compute RV using 3 estimators |\n",
    "| 25 min | Exercise 2: Implement GARCH + HAR baselines |\n",
    "| 20 min | Exercise 3: Build LSTM vol forecaster |\n",
    "| 20 min | Discussion: When does classical beat DL? |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate synthetic daily OHLCV data ---\n",
    "\n",
    "np.random.seed(42)\n",
    "n_days = 2520\n",
    "\n",
    "# GARCH(1,1) DGP\n",
    "omega, alpha, beta = 1e-6, 0.08, 0.90\n",
    "returns = np.zeros(n_days)\n",
    "sigma2 = np.zeros(n_days)\n",
    "sigma2[0] = omega / (1 - alpha - beta)\n",
    "\n",
    "for t in range(1, n_days):\n",
    "    sigma2[t] = omega + alpha * returns[t-1]**2 + beta * sigma2[t-1]\n",
    "    returns[t] = 0.0003 + np.sqrt(sigma2[t]) * np.random.randn()\n",
    "\n",
    "close = 100 * np.exp(np.cumsum(returns))\n",
    "daily_vol = np.sqrt(sigma2)\n",
    "high = close * np.exp(np.abs(np.random.randn(n_days)) * daily_vol * 0.5)\n",
    "low = close * np.exp(-np.abs(np.random.randn(n_days)) * daily_vol * 0.5)\n",
    "open_p = close * np.exp(np.random.randn(n_days) * daily_vol * 0.2)\n",
    "high = np.maximum(high, np.maximum(close, open_p)) * 1.001\n",
    "low = np.minimum(low, np.minimum(close, open_p)) * 0.999\n",
    "\n",
    "spy = pd.DataFrame({\n",
    "    'open': open_p, 'high': high, 'low': low, 'close': close,\n",
    "    'return': returns,\n",
    "    'volume': np.random.lognormal(18, 0.5, n_days)\n",
    "}, index=pd.bdate_range('2015-01-02', periods=n_days))\n",
    "\n",
    "print(f'Data: {len(spy)} days')\n",
    "spy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Compute RV Using 3 Estimators (25 min)\n",
    "\n",
    "Implement three realized volatility estimators:\n",
    "\n",
    "1. **Close-to-Close**: $\\hat{\\sigma}_{CC} = \\sqrt{\\sum_{i=1}^{n} r_i^2}$ (sum of squared returns over window)\n",
    "2. **Parkinson**: $\\hat{\\sigma}_P = \\sqrt{\\frac{1}{4 \\ln 2} (\\ln H - \\ln L)^2}$\n",
    "3. **Garman-Klass**: $\\hat{\\sigma}_{GK} = \\sqrt{0.5(\\ln H - \\ln L)^2 - (2\\ln 2 - 1)(\\ln C - \\ln O)^2}$\n",
    "\n",
    "Use a 5-day rolling window and annualize (multiply by $\\sqrt{252}$ or $\\sqrt{252/5}$ as appropriate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rv_close_to_close(returns, window=5):\n",
    "    \"\"\"\n",
    "    TODO: Compute close-to-close realized volatility.\n",
    "    - Rolling sum of squared returns over `window` days\n",
    "    - Take square root\n",
    "    - Annualize: multiply by sqrt(252/window)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def rv_parkinson(high, low, window=5):\n",
    "    \"\"\"\n",
    "    TODO: Compute Parkinson range-based RV.\n",
    "    - log_hl_sq = (ln(H) - ln(L))^2\n",
    "    - Rolling mean over window\n",
    "    - Divide by (4 * ln(2))\n",
    "    - Take square root, annualize\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def rv_garman_klass(open_p, high, low, close, window=5):\n",
    "    \"\"\"\n",
    "    TODO: Compute Garman-Klass RV.\n",
    "    - gk = 0.5 * (ln(H) - ln(L))^2 - (2*ln(2) - 1) * (ln(C) - ln(O))^2\n",
    "    - Rolling mean, sqrt, annualize\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute all three estimators\n",
    "# spy['rv_cc'] = rv_close_to_close(spy['return'])\n",
    "# spy['rv_park'] = rv_parkinson(spy['high'], spy['low'])\n",
    "# spy['rv_gk'] = rv_garman_klass(spy['open'], spy['high'], spy['low'], spy['close'])\n",
    "\n",
    "print('RV estimators: implement above')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot all three RV estimators on the same chart\n",
    "# Also compute summary statistics: mean, std, min, max, autocorrelation(1)\n",
    "\n",
    "print('RV plots and stats: implement above')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions to answer:**\n",
    "1. Which estimator is smoothest? Why?\n",
    "2. What is the autocorrelation of each RV estimator at lag 1? What does this tell you?\n",
    "3. Why is Parkinson typically lower variance than Close-to-Close?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Implement GARCH + HAR Baselines (25 min)\n",
    "\n",
    "### Part A: GARCH(1,1)\n",
    "\n",
    "Fit a GARCH(1,1) on training returns and produce one-step-ahead volatility forecasts.\n",
    "\n",
    "### Part B: HAR Model\n",
    "\n",
    "Implement the HAR model:\n",
    "$$RV_{t+1} = \\beta_0 + \\beta_D RV_t + \\beta_W \\overline{RV}_{t,5} + \\beta_M \\overline{RV}_{t,22} + \\epsilon_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part A: GARCH(1,1) ---\n",
    "# from arch import arch_model\n",
    "\n",
    "# TODO:\n",
    "# 1. Split data into train (60%) and test (40%)\n",
    "# 2. Fit GARCH(1,1) on train returns * 100 (arch library convention)\n",
    "# 3. Produce one-step-ahead forecasts on test set\n",
    "# 4. Convert forecasted variance to annualized vol\n",
    "\n",
    "print('GARCH baseline: implement above')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part B: HAR Model ---\n",
    "\n",
    "# TODO:\n",
    "# 1. Create HAR features: rv_d (daily), rv_w (5-day mean), rv_m (22-day mean)\n",
    "# 2. Create target: next-day RV (shift by -1)\n",
    "# 3. Split train/test temporally\n",
    "# 4. Fit LinearRegression on train\n",
    "# 5. Predict on test\n",
    "# 6. Report coefficients and R^2\n",
    "\n",
    "print('HAR baseline: implement above')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare GARCH vs HAR\n",
    "# - Compute MSE and correlation for both on test set\n",
    "# - Plot actual vs predicted for both\n",
    "\n",
    "print('GARCH vs HAR comparison: implement above')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Build LSTM Vol Forecaster (20 min)\n",
    "\n",
    "Build a simple LSTM that takes sequences of (RV, returns, log_volume) and predicts next-day RV.\n",
    "\n",
    "Architecture:\n",
    "- LSTM: 2 layers, hidden_dim=32\n",
    "- Sequence length: 20 days\n",
    "- Head: Linear(32, 16) -> ReLU -> Linear(16, 1)\n",
    "- Train with MSE loss and early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VolSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    TODO: Create sequences for vol forecasting.\n",
    "    Each sample: (seq_len, n_features) -> scalar target.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, feature_cols, target_col, seq_len=20):\n",
    "        # TODO: Store features and targets, normalize features\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # TODO: Return (X, y) where X has shape (seq_len, n_features)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMVolForecaster(nn.Module):\n",
    "    \"\"\"\n",
    "    TODO: LSTM for vol forecasting.\n",
    "    - LSTM: input_dim -> hidden_dim, n_layers, batch_first=True\n",
    "    - Head: hidden_dim -> 16 -> 1\n",
    "    - Use last hidden state from LSTM\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=32, n_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train the LSTM\n",
    "# 1. Create VolSequenceDataset for train/val/test\n",
    "# 2. Use DataLoader with shuffle=True for train only\n",
    "# 3. Train with MSE loss, Adam optimizer, gradient clipping\n",
    "# 4. Use early stopping on validation loss\n",
    "# 5. Evaluate on test set\n",
    "\n",
    "print('LSTM training: implement above')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Final comparison table\n",
    "# Compare GARCH, HAR, LSTM on test set\n",
    "# Metrics: MSE, QLIKE, Correlation\n",
    "\n",
    "print('Final comparison: implement above')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Discussion: When Does Classical Beat Deep Learning? (20 min)\n",
    "\n",
    "### Question 1\n",
    "GARCH(1,1) has only 3 parameters. The LSTM has thousands. Under what conditions would you expect the LSTM to overfit and underperform?\n",
    "\n",
    "**Think about**: sample size, signal-to-noise ratio, structural breaks.\n",
    "\n",
    "### Question 2\n",
    "The HAR model is just a linear regression with 3 features. Why is it so hard to beat?\n",
    "\n",
    "**Think about**: what the HAR features capture, the smoothness of the vol process.\n",
    "\n",
    "### Question 3\n",
    "For which assets or conditions would you expect LSTM to have the biggest advantage?\n",
    "\n",
    "**Think about**: crypto, FX, individual stocks, exogenous events, high-frequency data.\n",
    "\n",
    "### Question 4\n",
    "How would you combine GARCH and LSTM predictions? Is simple averaging optimal?\n",
    "\n",
    "**Think about**: complementary strengths, regime-dependent weighting, stacking.\n",
    "\n",
    "### Question 5\n",
    "The QLIKE loss penalizes underestimation more than overestimation. Is this always desirable? When might MSE be preferred?\n",
    "\n",
    "**Think about**: risk management vs trading, long vs short vol positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion Notes\n",
    "\n",
    "*Write your group's key takeaways here:*\n",
    "\n",
    "- Q1: ...\n",
    "- Q2: ...\n",
    "- Q3: ...\n",
    "- Q4: ...\n",
    "- Q5: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Today you practiced:\n",
    "1. Computing realized volatility with 3 different estimators\n",
    "2. Implementing GARCH(1,1) and HAR baselines\n",
    "3. Building an LSTM vol forecaster in PyTorch\n",
    "\n",
    "**For the homework**: you'll extend this to 30 stocks, add the QLIKE loss, and experiment with attention mechanisms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}