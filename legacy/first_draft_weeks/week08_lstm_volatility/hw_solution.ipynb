{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8 Homework SOLUTION --- Volatility Forecasting Showdown\n",
    "\n",
    "## SOLUTION --- do not distribute\n",
    "\n",
    "**Quantitative Finance ML Course**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from arch import arch_model\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate synthetic data for 30 stocks ---\n",
    "np.random.seed(42)\n",
    "n_days = 2520\n",
    "n_stocks = 30\n",
    "tickers = [f'STOCK_{i:02d}' for i in range(n_stocks)]\n",
    "\n",
    "all_data = {}\n",
    "for ticker in tickers:\n",
    "    omega = np.random.uniform(5e-7, 5e-6)\n",
    "    alpha = np.random.uniform(0.04, 0.12)\n",
    "    beta = np.random.uniform(0.82, 0.94)\n",
    "    if alpha + beta >= 0.999:\n",
    "        beta = 0.999 - alpha\n",
    "\n",
    "    returns = np.zeros(n_days)\n",
    "    sigma2 = np.zeros(n_days)\n",
    "    sigma2[0] = omega / (1 - alpha - beta)\n",
    "\n",
    "    for t in range(1, n_days):\n",
    "        sigma2[t] = omega + alpha * returns[t-1]**2 + beta * sigma2[t-1]\n",
    "        returns[t] = np.sqrt(sigma2[t]) * np.random.randn()\n",
    "\n",
    "    close = 100 * np.exp(np.cumsum(returns))\n",
    "    dv = np.sqrt(sigma2)\n",
    "    high = close * np.exp(np.abs(np.random.randn(n_days)) * dv * 0.5)\n",
    "    low = close * np.exp(-np.abs(np.random.randn(n_days)) * dv * 0.5)\n",
    "    open_p = close * np.exp(np.random.randn(n_days) * dv * 0.2)\n",
    "    high = np.maximum(high, np.maximum(close, open_p)) * 1.001\n",
    "    low = np.minimum(low, np.minimum(close, open_p)) * 0.999\n",
    "\n",
    "    all_data[ticker] = pd.DataFrame({\n",
    "        'open': open_p, 'high': high, 'low': low, 'close': close,\n",
    "        'return': returns,\n",
    "        'volume': np.random.lognormal(18, 0.5, n_days)\n",
    "    }, index=pd.bdate_range('2015-01-02', periods=n_days))\n",
    "\n",
    "print(f'Generated {n_stocks} stocks, {n_days} days each')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Compute RV for 30 Stocks (15 pts) --- SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rv_close_to_close(returns, window=5):\n",
    "    \"\"\"Close-to-close realized volatility.\"\"\"\n",
    "    return returns.pow(2).rolling(window).sum().apply(np.sqrt) * np.sqrt(252 / window)\n",
    "\n",
    "\n",
    "def rv_parkinson(high, low, window=5):\n",
    "    \"\"\"Parkinson range-based RV.\"\"\"\n",
    "    log_hl = (np.log(high) - np.log(low)) ** 2\n",
    "    return (log_hl / (4 * np.log(2))).rolling(window).mean().apply(np.sqrt) * np.sqrt(252)\n",
    "\n",
    "\n",
    "def rv_garman_klass(open_p, high, low, close, window=5):\n",
    "    \"\"\"Garman-Klass RV.\"\"\"\n",
    "    log_hl = (np.log(high) - np.log(low)) ** 2\n",
    "    log_co = (np.log(close) - np.log(open_p)) ** 2\n",
    "    gk = 0.5 * log_hl - (2 * np.log(2) - 1) * log_co\n",
    "    # Clamp negative values to small positive before sqrt\n",
    "    gk_mean = gk.rolling(window).mean().clip(lower=1e-12)\n",
    "    return gk_mean.apply(np.sqrt) * np.sqrt(252)\n",
    "\n",
    "\n",
    "# Apply to all stocks\n",
    "for ticker, stock_df in all_data.items():\n",
    "    stock_df['rv_cc'] = rv_close_to_close(stock_df['return'])\n",
    "    stock_df['rv_park'] = rv_parkinson(stock_df['high'], stock_df['low'])\n",
    "    stock_df['rv_gk'] = rv_garman_klass(stock_df['open'], stock_df['high'],\n",
    "                                         stock_df['low'], stock_df['close'])\n",
    "    stock_df['rv_target'] = stock_df['rv_cc'].shift(-1)  # next-day target\n",
    "    stock_df['log_volume'] = np.log(stock_df['volume'])\n",
    "    stock_df.dropna(inplace=True)\n",
    "\n",
    "print(f'RV computed for {n_stocks} stocks')\n",
    "print(f'Sample stock has {len(all_data[tickers[0]])} valid rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Summary statistics ---\n",
    "stats_list = []\n",
    "for ticker in tickers[:5]:  # show first 5\n",
    "    d = all_data[ticker]\n",
    "    stats_list.append({\n",
    "        'Ticker': ticker,\n",
    "        'Mean RV(CC)': d['rv_cc'].mean(),\n",
    "        'Std RV(CC)': d['rv_cc'].std(),\n",
    "        'AC(1) RV(CC)': d['rv_cc'].autocorr(1),\n",
    "        'Mean RV(Park)': d['rv_park'].mean(),\n",
    "        'Mean RV(GK)': d['rv_gk'].mean()\n",
    "    })\n",
    "\n",
    "stats_df = pd.DataFrame(stats_list).set_index('Ticker')\n",
    "print(stats_df.round(4).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot RV for 3 representative stocks ---\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "for i, ticker in enumerate([tickers[0], tickers[10], tickers[20]]):\n",
    "    d = all_data[ticker]\n",
    "    axes[i].plot(d.index, d['rv_cc'], label='Close-to-Close', alpha=0.8)\n",
    "    axes[i].plot(d.index, d['rv_park'], label='Parkinson', alpha=0.8)\n",
    "    axes[i].plot(d.index, d['rv_gk'], label='Garman-Klass', alpha=0.8)\n",
    "    axes[i].set_ylabel('Ann. RV')\n",
    "    axes[i].set_title(f'{ticker}')\n",
    "    axes[i].legend(loc='upper right')\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.suptitle('Realized Volatility Estimators', fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: GARCH + HAR Baselines (20 pts) --- SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GARCH(1,1) for all 30 stocks ---\n",
    "\n",
    "garch_results = {}  # ticker -> test predictions\n",
    "\n",
    "for ticker in tickers:\n",
    "    d = all_data[ticker]\n",
    "    n = len(d)\n",
    "    train_end = int(n * 0.6)\n",
    "    val_end = int(n * 0.8)\n",
    "\n",
    "    # Fit on train+val, forecast on test\n",
    "    rets_scaled = d['return'].values * 100\n",
    "    am = arch_model(rets_scaled[:val_end], vol='GARCH', p=1, q=1, mean='Constant')\n",
    "    res = am.fit(disp='off')\n",
    "\n",
    "    # Simple approach: use fitted model to get conditional variances for test\n",
    "    # Re-fit on all data up to val_end, then forecast\n",
    "    forecasts = []\n",
    "    for t in range(val_end, n):\n",
    "        # Expanding window (keep model params fixed for speed)\n",
    "        am_full = arch_model(rets_scaled[:t], vol='GARCH', p=1, q=1, mean='Constant')\n",
    "        res_full = am_full.fit(disp='off', last_obs=t)\n",
    "        fc = res_full.forecast(horizon=1)\n",
    "        garch_vol = np.sqrt(fc.variance.values[-1, 0]) / 100 * np.sqrt(252)\n",
    "        forecasts.append(garch_vol)\n",
    "\n",
    "    garch_results[ticker] = {\n",
    "        'pred': np.array(forecasts),\n",
    "        'actual': d['rv_target'].values[val_end:val_end + len(forecasts)]\n",
    "    }\n",
    "\n",
    "print(f'GARCH fitted for {len(garch_results)} stocks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- HAR for all 30 stocks ---\n",
    "\n",
    "har_results = {}\n",
    "\n",
    "for ticker in tickers:\n",
    "    d = all_data[ticker].copy()\n",
    "    d['rv_d'] = d['rv_cc']\n",
    "    d['rv_w'] = d['rv_cc'].rolling(5).mean()\n",
    "    d['rv_m'] = d['rv_cc'].rolling(22).mean()\n",
    "    d = d.dropna()\n",
    "\n",
    "    n = len(d)\n",
    "    train_end = int(n * 0.6)\n",
    "    val_end = int(n * 0.8)\n",
    "\n",
    "    har_feats = ['rv_d', 'rv_w', 'rv_m']\n",
    "    train = d.iloc[:train_end]\n",
    "    test = d.iloc[val_end:]\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(train[har_feats], train['rv_target'])\n",
    "\n",
    "    pred = model.predict(test[har_feats])\n",
    "\n",
    "    har_results[ticker] = {\n",
    "        'pred': pred,\n",
    "        'actual': test['rv_target'].values,\n",
    "        'coefs': dict(zip(har_feats, model.coef_))\n",
    "    }\n",
    "\n",
    "print(f'HAR fitted for {len(har_results)} stocks')\n",
    "print(f'\\nSample HAR coefficients ({tickers[0]}):')\n",
    "print(har_results[tickers[0]]['coefs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: LSTM Vol Forecaster (30 pts) --- SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VolSequenceDataset(Dataset):\n",
    "    \"\"\"Sequence dataset for volatility forecasting.\"\"\"\n",
    "    def __init__(self, features, targets, seq_len=20):\n",
    "        self.features = features.astype(np.float32)\n",
    "        self.targets = targets.astype(np.float32)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = torch.from_numpy(self.features[idx:idx + self.seq_len])\n",
    "        y = torch.tensor(self.targets[idx + self.seq_len])\n",
    "        return X, y\n",
    "\n",
    "\n",
    "class LSTMVolForecaster(nn.Module):\n",
    "    \"\"\"LSTM for volatility forecasting.\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=32, n_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if n_layers > 1 else 0.0\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_hidden = lstm_out[:, -1, :]\n",
    "        return self.head(last_hidden).squeeze(-1)\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=15):\n",
    "        self.patience = patience\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.best_state = None\n",
    "\n",
    "    def step(self, val_loss, model):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            return False\n",
    "        self.counter += 1\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "    def restore_best(self, model):\n",
    "        if self.best_state:\n",
    "            model.load_state_dict(self.best_state)\n",
    "\n",
    "\n",
    "print('Classes defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train LSTM for each stock ---\n",
    "\n",
    "SEQ_LEN = 20\n",
    "lstm_features_cols = ['rv_cc', 'return', 'log_volume']\n",
    "lstm_results = {}\n",
    "\n",
    "for idx, ticker in enumerate(tickers):\n",
    "    d = all_data[ticker].copy()\n",
    "    n = len(d)\n",
    "    train_end = int(n * 0.6)\n",
    "    val_end = int(n * 0.8)\n",
    "\n",
    "    # Extract arrays\n",
    "    feats = d[lstm_features_cols].values\n",
    "    targets = d['rv_target'].values\n",
    "\n",
    "    # Normalize using train stats\n",
    "    train_feats = feats[:train_end]\n",
    "    feat_mean = train_feats.mean(axis=0)\n",
    "    feat_std = train_feats.std(axis=0) + 1e-8\n",
    "    feats_norm = (feats - feat_mean) / feat_std\n",
    "\n",
    "    # Create datasets (with overlap for sequence start)\n",
    "    train_ds = VolSequenceDataset(feats_norm[:train_end], targets[:train_end], SEQ_LEN)\n",
    "    val_ds = VolSequenceDataset(feats_norm[train_end - SEQ_LEN:val_end],\n",
    "                                 targets[train_end - SEQ_LEN:val_end], SEQ_LEN)\n",
    "    test_ds = VolSequenceDataset(feats_norm[val_end - SEQ_LEN:],\n",
    "                                  targets[val_end - SEQ_LEN:], SEQ_LEN)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
    "    test_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\n",
    "\n",
    "    # Train\n",
    "    torch.manual_seed(42)\n",
    "    model = LSTMVolForecaster(input_dim=len(lstm_features_cols))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    stopper = EarlyStopping(patience=15)\n",
    "\n",
    "    for epoch in range(150):\n",
    "        model.train()\n",
    "        for X_b, y_b in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = ((model(X_b) - y_b) ** 2).mean()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        v_losses = []\n",
    "        with torch.no_grad():\n",
    "            for X_b, y_b in val_loader:\n",
    "                v_losses.append(((model(X_b) - y_b) ** 2).mean().item())\n",
    "        if stopper.step(np.mean(v_losses), model):\n",
    "            break\n",
    "\n",
    "    stopper.restore_best(model)\n",
    "\n",
    "    # Predict on test\n",
    "    model.eval()\n",
    "    preds, actuals = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_b, y_b in test_loader:\n",
    "            preds.append(model(X_b).numpy())\n",
    "            actuals.append(y_b.numpy())\n",
    "\n",
    "    lstm_results[ticker] = {\n",
    "        'pred': np.concatenate(preds),\n",
    "        'actual': np.concatenate(actuals)\n",
    "    }\n",
    "\n",
    "    if (idx + 1) % 10 == 0:\n",
    "        print(f'  {idx+1}/{n_stocks} stocks done')\n",
    "\n",
    "print(f'\\nLSTM trained for {len(lstm_results)} stocks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Compare All Models (20 pts) --- SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(actual, pred):\n",
    "    return np.mean((actual - pred) ** 2)\n",
    "\n",
    "\n",
    "def compute_qlike(actual, pred):\n",
    "    \"\"\"QLIKE loss for volatility.\"\"\"\n",
    "    var_true = actual ** 2 + 1e-10\n",
    "    var_pred = pred ** 2 + 1e-10\n",
    "    ratio = var_true / var_pred\n",
    "    return np.mean(ratio - np.log(ratio) - 1)\n",
    "\n",
    "\n",
    "def compute_corr(actual, pred):\n",
    "    return np.corrcoef(actual, pred)[0, 1]\n",
    "\n",
    "\n",
    "# Compute metrics for each stock x model\n",
    "per_stock_metrics = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    # Align lengths\n",
    "    n_test = min(\n",
    "        len(garch_results[ticker]['pred']),\n",
    "        len(har_results[ticker]['pred']),\n",
    "        len(lstm_results[ticker]['pred'])\n",
    "    )\n",
    "\n",
    "    actual = har_results[ticker]['actual'][:n_test]  # common actual\n",
    "    garch_p = garch_results[ticker]['pred'][:n_test]\n",
    "    har_p = har_results[ticker]['pred'][:n_test]\n",
    "    lstm_p = lstm_results[ticker]['pred'][:n_test]\n",
    "\n",
    "    for model_name, pred in [('GARCH', garch_p), ('HAR', har_p), ('LSTM', lstm_p)]:\n",
    "        per_stock_metrics.append({\n",
    "            'Ticker': ticker,\n",
    "            'Model': model_name,\n",
    "            'MSE': compute_mse(actual, pred),\n",
    "            'QLIKE': compute_qlike(actual, pred),\n",
    "            'Correlation': compute_corr(actual, pred)\n",
    "        })\n",
    "\n",
    "metrics_df = pd.DataFrame(per_stock_metrics)\n",
    "\n",
    "# Aggregate across stocks\n",
    "agg_metrics = metrics_df.groupby('Model')[['MSE', 'QLIKE', 'Correlation']].mean()\n",
    "print('Average Metrics Across 30 Stocks:')\n",
    "print(agg_metrics.round(6).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Comparison bar chart ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, metric in enumerate(['MSE', 'QLIKE', 'Correlation']):\n",
    "    vals = agg_metrics[metric]\n",
    "    colors = ['#e74c3c', '#3498db', '#2ecc71']\n",
    "    axes[i].bar(vals.index, vals.values, color=colors)\n",
    "    axes[i].set_title(metric)\n",
    "    axes[i].set_ylabel(metric)\n",
    "    for j, v in enumerate(vals.values):\n",
    "        axes[i].text(j, v, f'{v:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.suptitle('Model Comparison (Average Across 30 Stocks)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Per-stock analysis: where does LSTM win? ---\n",
    "\n",
    "# For each stock, find the best model by QLIKE\n",
    "best_model_per_stock = metrics_df.loc[\n",
    "    metrics_df.groupby('Ticker')['QLIKE'].idxmin()\n",
    "][['Ticker', 'Model', 'QLIKE']]\n",
    "\n",
    "win_counts = best_model_per_stock['Model'].value_counts()\n",
    "print('Best Model by QLIKE (per stock):')\n",
    "print(win_counts)\n",
    "print(f'\\nLSTM wins for {win_counts.get(\"LSTM\", 0)}/{n_stocks} stocks')\n",
    "print(f'HAR wins for {win_counts.get(\"HAR\", 0)}/{n_stocks} stocks')\n",
    "print(f'GARCH wins for {win_counts.get(\"GARCH\", 0)}/{n_stocks} stocks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Full comparison table (top 10 stocks) ---\n",
    "pivot = metrics_df.pivot_table(\n",
    "    index='Ticker', columns='Model', values=['MSE', 'QLIKE', 'Correlation']\n",
    ")\n",
    "print('Per-Stock Metrics (first 10 stocks):')\n",
    "print(pivot.head(10).round(4).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Attention Extension (15 pts) --- SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMWithAttention(nn.Module):\n",
    "    \"\"\"LSTM with attention over hidden states.\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=32, n_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if n_layers > 1 else 0.0\n",
    "        )\n",
    "        # Attention: project hidden states, then dot with learned query\n",
    "        self.attn_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.attn_query = nn.Parameter(torch.randn(hidden_dim))\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, input_dim)\n",
    "        lstm_out, _ = self.lstm(x)  # (batch, seq_len, hidden_dim)\n",
    "\n",
    "        # Attention scores\n",
    "        proj = torch.tanh(self.attn_proj(lstm_out))  # (batch, seq_len, hidden_dim)\n",
    "        scores = (proj * self.attn_query).sum(dim=-1)  # (batch, seq_len)\n",
    "        weights = torch.softmax(scores, dim=-1)  # (batch, seq_len)\n",
    "\n",
    "        # Weighted sum of hidden states\n",
    "        context = (lstm_out * weights.unsqueeze(-1)).sum(dim=1)  # (batch, hidden_dim)\n",
    "\n",
    "        return self.head(context).squeeze(-1)\n",
    "\n",
    "    def get_attention_weights(self, x):\n",
    "        \"\"\"Return attention weights for visualization.\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            lstm_out, _ = self.lstm(x)\n",
    "            proj = torch.tanh(self.attn_proj(lstm_out))\n",
    "            scores = (proj * self.attn_query).sum(dim=-1)\n",
    "            return torch.softmax(scores, dim=-1)\n",
    "\n",
    "\n",
    "# Test\n",
    "attn_model = LSTMWithAttention(input_dim=3)\n",
    "x_test = torch.randn(8, 20, 3)\n",
    "print(f'Output shape: {attn_model(x_test).shape}')\n",
    "print(f'Attention weights shape: {attn_model.get_attention_weights(x_test).shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train attention model for a subset of stocks ---\n",
    "# (For demonstration, train on 10 stocks to save time)\n",
    "\n",
    "attn_results = {}\n",
    "sample_tickers = tickers[:10]\n",
    "\n",
    "for idx, ticker in enumerate(sample_tickers):\n",
    "    d = all_data[ticker].copy()\n",
    "    n = len(d)\n",
    "    train_end = int(n * 0.6)\n",
    "    val_end = int(n * 0.8)\n",
    "\n",
    "    feats = d[lstm_features_cols].values\n",
    "    targets = d['rv_target'].values\n",
    "\n",
    "    train_feats = feats[:train_end]\n",
    "    feat_mean = train_feats.mean(axis=0)\n",
    "    feat_std = train_feats.std(axis=0) + 1e-8\n",
    "    feats_norm = (feats - feat_mean) / feat_std\n",
    "\n",
    "    train_ds = VolSequenceDataset(feats_norm[:train_end], targets[:train_end], SEQ_LEN)\n",
    "    val_ds = VolSequenceDataset(feats_norm[train_end - SEQ_LEN:val_end],\n",
    "                                 targets[train_end - SEQ_LEN:val_end], SEQ_LEN)\n",
    "    test_ds = VolSequenceDataset(feats_norm[val_end - SEQ_LEN:],\n",
    "                                  targets[val_end - SEQ_LEN:], SEQ_LEN)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
    "    test_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    model = LSTMWithAttention(input_dim=len(lstm_features_cols))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    stopper = EarlyStopping(patience=15)\n",
    "\n",
    "    for epoch in range(150):\n",
    "        model.train()\n",
    "        for X_b, y_b in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = ((model(X_b) - y_b) ** 2).mean()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        v_losses = []\n",
    "        with torch.no_grad():\n",
    "            for X_b, y_b in val_loader:\n",
    "                v_losses.append(((model(X_b) - y_b) ** 2).mean().item())\n",
    "        if stopper.step(np.mean(v_losses), model):\n",
    "            break\n",
    "\n",
    "    stopper.restore_best(model)\n",
    "\n",
    "    model.eval()\n",
    "    preds, actuals = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_b, y_b in test_loader:\n",
    "            preds.append(model(X_b).numpy())\n",
    "            actuals.append(y_b.numpy())\n",
    "\n",
    "    attn_results[ticker] = {\n",
    "        'pred': np.concatenate(preds),\n",
    "        'actual': np.concatenate(actuals),\n",
    "        'model': model,\n",
    "        'test_loader': test_loader\n",
    "    }\n",
    "\n",
    "print(f'Attention LSTM trained for {len(attn_results)} stocks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compare vanilla LSTM vs attention LSTM ---\n",
    "comparison_rows = []\n",
    "for ticker in sample_tickers:\n",
    "    n_t = min(len(lstm_results[ticker]['pred']), len(attn_results[ticker]['pred']))\n",
    "    actual = lstm_results[ticker]['actual'][:n_t]\n",
    "    lstm_p = lstm_results[ticker]['pred'][:n_t]\n",
    "    attn_p = attn_results[ticker]['pred'][:n_t]\n",
    "\n",
    "    comparison_rows.append({\n",
    "        'Ticker': ticker,\n",
    "        'LSTM MSE': compute_mse(actual, lstm_p),\n",
    "        'Attn MSE': compute_mse(actual, attn_p),\n",
    "        'LSTM QLIKE': compute_qlike(actual, lstm_p),\n",
    "        'Attn QLIKE': compute_qlike(actual, attn_p),\n",
    "        'LSTM Corr': compute_corr(actual, lstm_p),\n",
    "        'Attn Corr': compute_corr(actual, attn_p)\n",
    "    })\n",
    "\n",
    "comp_df = pd.DataFrame(comparison_rows).set_index('Ticker')\n",
    "print('Vanilla LSTM vs Attention LSTM:')\n",
    "print(comp_df.round(6).to_string())\n",
    "\n",
    "print(f'\\nAverage improvement in QLIKE: '\n",
    "      f'{(comp_df[\"LSTM QLIKE\"] - comp_df[\"Attn QLIKE\"]).mean():.6f}')\n",
    "print(f'Average improvement in Correlation: '\n",
    "      f'{(comp_df[\"Attn Corr\"] - comp_df[\"LSTM Corr\"]).mean():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize attention weights ---\n",
    "# Show attention for a few test sequences from the first stock\n",
    "\n",
    "ticker = sample_tickers[0]\n",
    "model = attn_results[ticker]['model']\n",
    "test_loader = attn_results[ticker]['test_loader']\n",
    "\n",
    "# Get first batch\n",
    "X_sample, y_sample = next(iter(test_loader))\n",
    "weights = model.get_attention_weights(X_sample[:5])  # first 5 sequences\n",
    "\n",
    "fig, axes = plt.subplots(5, 1, figsize=(12, 10), sharex=True)\n",
    "for i in range(5):\n",
    "    axes[i].bar(range(SEQ_LEN), weights[i].numpy(), color='steelblue')\n",
    "    axes[i].set_ylabel(f'Sample {i+1}')\n",
    "    axes[i].set_ylim(0, weights[i].max().item() * 1.3)\n",
    "\n",
    "axes[-1].set_xlabel('Time Step (days before prediction)')\n",
    "plt.suptitle(f'Attention Weights ({ticker})', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Average attention profile\n",
    "all_weights = []\n",
    "for X_b, _ in test_loader:\n",
    "    w = model.get_attention_weights(X_b)\n",
    "    all_weights.append(w.numpy())\n",
    "avg_weights = np.concatenate(all_weights).mean(axis=0)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(range(SEQ_LEN), avg_weights, color='steelblue')\n",
    "plt.xlabel('Time Step (days before prediction)')\n",
    "plt.ylabel('Average Attention Weight')\n",
    "plt.title(f'Average Attention Profile ({ticker})')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "**1. Does attention help?**\n",
    "\n",
    "The attention mechanism provides a modest improvement for some stocks. The benefit is most apparent for stocks with strong regime-dependent dynamics, where different historical time steps carry different amounts of information. For stocks with simple GARCH-like dynamics, the vanilla LSTM (which relies on the final hidden state) already captures the signal well, and attention adds little.\n",
    "\n",
    "**2. What do the attention weights focus on?**\n",
    "\n",
    "The average attention profile shows that the model tends to put more weight on recent time steps (especially the last 3-5 days), which is consistent with the strong short-term autocorrelation of realized volatility. However, there is also non-trivial weight on more distant time steps, particularly around the weekly lag (5 days back), suggesting the model has learned something akin to the HAR structure.\n",
    "\n",
    "**3. Is the improvement worth the complexity?**\n",
    "\n",
    "For this dataset, the improvement is marginal. The attention mechanism adds interpretability (we can see what the model focuses on), but the practical forecasting gains are small. In production, the simplicity of the HAR model or vanilla LSTM might be preferred unless working with richer feature sets where attention can selectively combine different information sources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}