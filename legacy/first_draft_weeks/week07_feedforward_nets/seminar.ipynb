{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7 Seminar --- Feedforward Neural Networks for Asset Pricing\n",
    "\n",
    "**Quantitative Finance ML Course**\n",
    "\n",
    "---\n",
    "\n",
    "## Today's Plan (90 min)\n",
    "\n",
    "| Time | Activity |\n",
    "|------|----------|\n",
    "| 25 min | Exercise 1: Build the Gu-Kelly-Xiu 3-layer net |\n",
    "| 25 min | Exercise 2: Temporal train/val/test splitting in DataLoader |\n",
    "| 20 min | Exercise 3: Compare MSE vs weighted MSE vs IC loss |\n",
    "| 20 min | Discussion: When do NNs beat trees? |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.stats import spearmanr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Shared data generation (same as lecture) ---\n",
    "\n",
    "np.random.seed(42)\n",
    "n_stocks = 500\n",
    "n_months = 180\n",
    "\n",
    "records = []\n",
    "for t in range(n_months):\n",
    "    for i in range(n_stocks):\n",
    "        mom_1m = np.random.randn() * 0.08\n",
    "        mom_12m = np.random.randn() * 0.20\n",
    "        vol_20d = np.abs(np.random.randn()) * 0.02 + 0.01\n",
    "        size = np.random.randn() * 2 + 15\n",
    "        bm = np.random.randn() * 0.5\n",
    "        turnover = np.abs(np.random.randn()) * 0.01\n",
    "        rev_1m = np.random.randn() * 0.05\n",
    "\n",
    "        ret_next = (\n",
    "            -0.002 * mom_1m + 0.003 * mom_12m - 0.005 * vol_20d\n",
    "            + 0.001 * bm + 0.002 * np.sin(mom_12m * size)\n",
    "            + np.random.randn() * 0.08\n",
    "        )\n",
    "\n",
    "        records.append({\n",
    "            'date_idx': t, 'stock_id': i,\n",
    "            'mom_1m': mom_1m, 'mom_12m': mom_12m, 'vol_20d': vol_20d,\n",
    "            'size': size, 'bm': bm, 'turnover': turnover, 'rev_1m': rev_1m,\n",
    "            'ret_next': ret_next\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "feature_cols = ['mom_1m', 'mom_12m', 'vol_20d', 'size', 'bm', 'turnover', 'rev_1m']\n",
    "\n",
    "# Cross-sectional rank normalization\n",
    "for col in feature_cols:\n",
    "    df[col] = df.groupby('date_idx')[col].transform(\n",
    "        lambda x: (x.rank() - 1) / (len(x) - 1) - 0.5\n",
    "    )\n",
    "\n",
    "print(f'Data ready: {df.shape[0]:,} observations, {len(feature_cols)} features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Build the Gu-Kelly-Xiu 3-Layer Net (25 min)\n",
    "\n",
    "Implement the NN3 architecture from the lecture:\n",
    "- 3 hidden layers with 32, 16, 8 neurons\n",
    "- ReLU activation after each hidden layer\n",
    "- Batch normalization after each linear layer\n",
    "- Dropout after each activation\n",
    "- Single linear output\n",
    "\n",
    "**Tasks**:\n",
    "1. Fill in the `__init__` method\n",
    "2. Fill in the `forward` method\n",
    "3. Verify the parameter count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuKellyXiuNet(nn.Module):\n",
    "    \"\"\"\n",
    "    TODO: Implement the Gu-Kelly-Xiu NN3 architecture.\n",
    "    \n",
    "    Architecture: input -> Linear(32) -> BN -> ReLU -> Dropout\n",
    "                       -> Linear(16) -> BN -> ReLU -> Dropout\n",
    "                       -> Linear(8)  -> BN -> ReLU -> Dropout\n",
    "                       -> Linear(1)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_sizes=(32, 16, 8), dropout=0.5):\n",
    "        super().__init__()\n",
    "        # TODO: Build the network\n",
    "        # Hint: use nn.Sequential with nn.Linear, nn.BatchNorm1d, nn.ReLU, nn.Dropout\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Forward pass\n",
    "        # Hint: don't forget to squeeze the output from (batch, 1) to (batch,)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test your implementation ---\n",
    "model = GuKellyXiuNet(input_dim=7)\n",
    "print(model)\n",
    "\n",
    "# Test forward pass\n",
    "x_test = torch.randn(64, 7)  # batch of 64, 7 features\n",
    "y_test = model(x_test)\n",
    "print(f'\\nInput shape:  {x_test.shape}')\n",
    "print(f'Output shape: {y_test.shape}')  # should be (64,)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Parameters:   {n_params}')  # should be around 1,000-1,200\n",
    "\n",
    "assert y_test.shape == (64,), f'Expected shape (64,), got {y_test.shape}'\n",
    "print('\\nAll checks passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Temporal Train/Val/Test Splitting (25 min)\n",
    "\n",
    "Implement proper temporal splitting for financial panel data.\n",
    "\n",
    "**Requirements**:\n",
    "1. Create a `CrossSectionalDataset` class\n",
    "2. Split data into train (months 0-107), val (108-143), test (144-179)\n",
    "3. Create DataLoaders with `shuffle=True` for train only\n",
    "4. Write a training loop with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossSectionalDataset(Dataset):\n",
    "    \"\"\"Dataset for cross-sectional stock data.\"\"\"\n",
    "    def __init__(self, features, targets):\n",
    "        # TODO: Convert to tensors and store\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # TODO: Return (features, target) tuple\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split the data temporally\n",
    "train_end = 108\n",
    "val_end = 144\n",
    "\n",
    "# TODO: Create train_df, val_df, test_df\n",
    "# TODO: Extract X_train, y_train, X_val, y_val, X_test, y_test\n",
    "# TODO: Create Datasets and DataLoaders\n",
    "\n",
    "# train_loader = DataLoader(..., batch_size=2048, shuffle=True)\n",
    "# val_loader = DataLoader(..., batch_size=4096, shuffle=False)\n",
    "# test_loader = DataLoader(..., batch_size=4096, shuffle=False)\n",
    "\n",
    "# Verify:\n",
    "# print(f'Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"TODO: Implement early stopping.\"\"\"\n",
    "    def __init__(self, patience=10):\n",
    "        self.patience = patience\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.best_state = None\n",
    "\n",
    "    def step(self, val_loss, model):\n",
    "        \"\"\"Returns True if we should stop training.\"\"\"\n",
    "        # TODO: Implement the logic\n",
    "        # - If val_loss improved, save model state, reset counter\n",
    "        # - If not, increment counter\n",
    "        # - Return True if counter >= patience\n",
    "        pass\n",
    "\n",
    "    def restore_best(self, model):\n",
    "        \"\"\"Restore the best model state.\"\"\"\n",
    "        # TODO\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write the training loop\n",
    "# 1. Initialize model, optimizer (Adam, lr=1e-3, weight_decay=1e-5)\n",
    "# 2. For each epoch:\n",
    "#    a. Train: model.train(), iterate train_loader, compute MSE, backprop\n",
    "#    b. Validate: model.eval(), iterate val_loader, compute MSE\n",
    "#    c. Check early stopping\n",
    "# 3. Restore best model\n",
    "# 4. Evaluate on test set with IC\n",
    "\n",
    "# model = GuKellyXiuNet(input_dim=7)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "# stopper = EarlyStopping(patience=10)\n",
    "# ...\n",
    "\n",
    "print('Training loop: implement above')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Compare Loss Functions (20 min)\n",
    "\n",
    "Train the same architecture with 3 different loss functions and compare test IC:\n",
    "\n",
    "1. **MSE**: standard mean squared error\n",
    "2. **Weighted MSE**: weight by absolute return (focus on big movers)\n",
    "3. **IC loss**: maximize cross-sectional correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_pred, y_true):\n",
    "    return ((y_pred - y_true) ** 2).mean()\n",
    "\n",
    "\n",
    "def weighted_mse_loss(y_pred, y_true):\n",
    "    \"\"\"TODO: Implement weighted MSE, weighting by |y_true|.\"\"\"\n",
    "    # Hint: normalize weights so they sum to 1\n",
    "    pass\n",
    "\n",
    "\n",
    "def ic_loss(y_pred, y_true):\n",
    "    \"\"\"TODO: Implement negative Pearson correlation loss.\"\"\"\n",
    "    # Hint: de-mean both, compute correlation, return negative\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train 3 models with different loss functions\n",
    "# For each:\n",
    "#   1. Train with early stopping (same hyperparameters)\n",
    "#   2. Compute test IC\n",
    "#   3. Store results\n",
    "\n",
    "# loss_functions = {\n",
    "#     'MSE': mse_loss,\n",
    "#     'Weighted MSE': weighted_mse_loss,\n",
    "#     'IC Loss': ic_loss\n",
    "# }\n",
    "\n",
    "# results = {}\n",
    "# for name, loss_fn in loss_functions.items():\n",
    "#     ... train model ...\n",
    "#     ... compute IC ...\n",
    "#     results[name] = {'mean_ic': ..., 'ic_ir': ...}\n",
    "\n",
    "print('Loss comparison: implement above')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a comparison bar chart of Mean IC for each loss function\n",
    "# Also create a table showing Mean IC, IC IR, and IC > 0 percentage\n",
    "\n",
    "print('Comparison plot: implement above')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Discussion: When Do NNs Beat Trees? (20 min)\n",
    "\n",
    "Consider the following questions with your group:\n",
    "\n",
    "### Question 1\n",
    "The Gu-Kelly-Xiu paper shows NNs outperform tree-based models in cross-sectional return prediction. But many practitioners still prefer XGBoost/LightGBM. Why?\n",
    "\n",
    "**Think about**: training speed, interpretability, hyperparameter sensitivity, data requirements.\n",
    "\n",
    "### Question 2\n",
    "In what scenarios would you expect neural nets to have a clear advantage over trees?\n",
    "\n",
    "**Think about**: nonlinear interactions between features, smoothness of the signal, dataset size.\n",
    "\n",
    "### Question 3\n",
    "Why do we use a small architecture (32-16-8) instead of a large one (256-128-64)?\n",
    "\n",
    "**Think about**: signal-to-noise ratio in financial data, overfitting risk, sample size.\n",
    "\n",
    "### Question 4\n",
    "Why is ensembling over random seeds so effective for neural nets but less critical for tree-based models?\n",
    "\n",
    "**Think about**: loss landscape, initialization sensitivity, bagging in tree ensembles.\n",
    "\n",
    "### Question 5\n",
    "Should we use the same loss function (e.g., MSE) for NNs and trees, or should we tailor the loss to the model type?\n",
    "\n",
    "**Think about**: differentiability requirements, what each model is good at learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion Notes\n",
    "\n",
    "*Write your group's key takeaways here:*\n",
    "\n",
    "- Q1: ...\n",
    "- Q2: ...\n",
    "- Q3: ...\n",
    "- Q4: ...\n",
    "- Q5: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Today you practiced:\n",
    "1. Building the Gu-Kelly-Xiu feedforward architecture in PyTorch\n",
    "2. Implementing proper temporal data splitting for financial panels\n",
    "3. Comparing different financial loss functions (MSE, weighted MSE, IC)\n",
    "\n",
    "**For the homework**: you'll extend this to expanding-window CV, ensemble with tree models, and analyze where NNs win vs lose."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}