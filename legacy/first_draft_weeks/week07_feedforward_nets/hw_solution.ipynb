{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7 Homework SOLUTION --- Deep Cross-Sectional Model\n",
    "\n",
    "## SOLUTION --- do not distribute\n",
    "\n",
    "**Quantitative Finance ML Course**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.stats import spearmanr, rankdata\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "\n",
    "# Device selection\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data generation ---\n",
    "np.random.seed(42)\n",
    "n_stocks = 500\n",
    "n_months = 240\n",
    "\n",
    "records = []\n",
    "for t in range(n_months):\n",
    "    for i in range(n_stocks):\n",
    "        mom_1m = np.random.randn() * 0.08\n",
    "        mom_12m = np.random.randn() * 0.20\n",
    "        vol_20d = np.abs(np.random.randn()) * 0.02 + 0.01\n",
    "        size = np.random.randn() * 2 + 15\n",
    "        bm = np.random.randn() * 0.5\n",
    "        turnover = np.abs(np.random.randn()) * 0.01\n",
    "        rev_1m = np.random.randn() * 0.05\n",
    "\n",
    "        ret_next = (\n",
    "            -0.002 * mom_1m + 0.003 * mom_12m - 0.005 * vol_20d\n",
    "            + 0.001 * bm + 0.002 * np.sin(mom_12m * size)\n",
    "            + np.random.randn() * 0.08\n",
    "        )\n",
    "\n",
    "        records.append({\n",
    "            'date_idx': t, 'stock_id': i,\n",
    "            'mom_1m': mom_1m, 'mom_12m': mom_12m, 'vol_20d': vol_20d,\n",
    "            'size': size, 'bm': bm, 'turnover': turnover, 'rev_1m': rev_1m,\n",
    "            'ret_next': ret_next\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "feature_cols = ['mom_1m', 'mom_12m', 'vol_20d', 'size', 'bm', 'turnover', 'rev_1m']\n",
    "\n",
    "# Keep raw features for tree models (they handle ranks internally)\n",
    "df_raw = df.copy()\n",
    "\n",
    "# Cross-sectional rank normalization for NN\n",
    "for col in feature_cols:\n",
    "    df[col] = df.groupby('date_idx')[col].transform(\n",
    "        lambda x: (x.rank() - 1) / (len(x) - 1) - 0.5\n",
    "    )\n",
    "\n",
    "print(f'Panel: {df.shape[0]:,} obs, {n_months} months, {n_stocks} stocks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Gu-Kelly-Xiu Architecture (20 pts) --- SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossSectionalDataset(Dataset):\n",
    "    \"\"\"Dataset for cross-sectional stock data.\"\"\"\n",
    "    def __init__(self, features, targets):\n",
    "        self.X = torch.tensor(features, dtype=torch.float32)\n",
    "        self.y = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting.\"\"\"\n",
    "    def __init__(self, patience=10):\n",
    "        self.patience = patience\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.best_state = None\n",
    "\n",
    "    def step(self, val_loss, model):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            return self.counter >= self.patience\n",
    "\n",
    "    def restore_best(self, model):\n",
    "        if self.best_state is not None:\n",
    "            model.load_state_dict(self.best_state)\n",
    "\n",
    "\n",
    "class GuKellyXiuNet(nn.Module):\n",
    "    \"\"\"Gu-Kelly-Xiu NN3: 3 hidden layers (32-16-8) with BN, ReLU, Dropout.\"\"\"\n",
    "    def __init__(self, input_dim, hidden_sizes=(32, 16, 8), dropout=0.5):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, h),\n",
    "                nn.BatchNorm1d(h),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = h\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "\n",
    "# Verify\n",
    "model = GuKellyXiuNet(input_dim=len(feature_cols))\n",
    "print(model)\n",
    "x_test = torch.randn(32, len(feature_cols))\n",
    "out = model(x_test)\n",
    "print(f'\\nOutput shape: {out.shape}')\n",
    "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Parameters: {n_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Expanding-Window CV with MPS (25 pts) --- SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expanding_window_folds(n_months, initial_train=120, val_size=24, test_size=24):\n",
    "    \"\"\"Generate expanding-window fold definitions.\"\"\"\n",
    "    folds = []\n",
    "    train_end = initial_train\n",
    "    while train_end + val_size + test_size <= n_months:\n",
    "        val_end = train_end + val_size\n",
    "        test_end = val_end + test_size\n",
    "        folds.append((train_end, val_end, test_end))\n",
    "        train_end += test_size  # shift by test_size\n",
    "    return folds\n",
    "\n",
    "\n",
    "folds = expanding_window_folds(n_months)\n",
    "for i, (tr, va, te) in enumerate(folds):\n",
    "    print(f'Fold {i+1}: Train [0, {tr})  Val [{tr}, {va})  Test [{va}, {te})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_nn(model, train_loader, val_loader, n_epochs=100,\n",
    "                    lr=1e-3, device='cpu'):\n",
    "    \"\"\"Train one NN model with early stopping.\"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    stopper = EarlyStopping(patience=10)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        for X_b, y_b in train_loader:\n",
    "            X_b, y_b = X_b.to(device), y_b.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = ((model(X_b) - y_b) ** 2).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for X_b, y_b in val_loader:\n",
    "                X_b, y_b = X_b.to(device), y_b.to(device)\n",
    "                val_losses.append(((model(X_b) - y_b) ** 2).mean().item())\n",
    "\n",
    "        if stopper.step(np.mean(val_losses), model):\n",
    "            break\n",
    "\n",
    "    stopper.restore_best(model)\n",
    "    model = model.to('cpu')\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_nn_ensemble(models, X, device='cpu'):\n",
    "    \"\"\"Average predictions from an ensemble of NN models.\"\"\"\n",
    "    X_t = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "    preds = []\n",
    "    for m in models:\n",
    "        m.eval()\n",
    "        m.to(device)\n",
    "        with torch.no_grad():\n",
    "            preds.append(m(X_t).cpu().numpy())\n",
    "        m.to('cpu')\n",
    "    return np.mean(preds, axis=0)\n",
    "\n",
    "\n",
    "print('Training functions ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_monthly_ic(df_sub, pred_col='pred', ret_col='ret_next'):\n",
    "    \"\"\"Compute monthly rank IC.\"\"\"\n",
    "    return df_sub.groupby('date_idx').apply(\n",
    "        lambda g: spearmanr(g[pred_col], g[ret_col])[0]\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_ls_returns(df_sub, pred_col='pred', ret_col='ret_next', n_q=5):\n",
    "    \"\"\"Compute long-short quintile returns.\"\"\"\n",
    "    def _ls(g):\n",
    "        g = g.copy()\n",
    "        g['q'] = pd.qcut(g[pred_col], n_q, labels=False, duplicates='drop')\n",
    "        return g[g['q'] == n_q - 1][ret_col].mean() - g[g['q'] == 0][ret_col].mean()\n",
    "    return df_sub.groupby('date_idx').apply(_ls)\n",
    "\n",
    "\n",
    "print('Evaluation functions ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run expanding-window CV for NN ---\n",
    "use_device = str(device)\n",
    "nn_test_dfs = []\n",
    "\n",
    "for fold_idx, (train_end, val_end, test_end) in enumerate(folds):\n",
    "    print(f'\\n--- Fold {fold_idx+1}: Train [0,{train_end}) Val [{train_end},{val_end}) Test [{val_end},{test_end}) ---')\n",
    "\n",
    "    # Split\n",
    "    tr = df[df.date_idx < train_end]\n",
    "    va = df[(df.date_idx >= train_end) & (df.date_idx < val_end)]\n",
    "    te = df[(df.date_idx >= val_end) & (df.date_idx < test_end)].copy()\n",
    "\n",
    "    train_ds = CrossSectionalDataset(tr[feature_cols].values, tr['ret_next'].values)\n",
    "    val_ds = CrossSectionalDataset(va[feature_cols].values, va['ret_next'].values)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=2048, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=4096, shuffle=False)\n",
    "\n",
    "    # Train ensemble of 3 seeds\n",
    "    models = []\n",
    "    for seed in range(3):\n",
    "        torch.manual_seed(seed)\n",
    "        m = GuKellyXiuNet(input_dim=len(feature_cols))\n",
    "        m = train_single_nn(m, train_loader, val_loader, device=use_device)\n",
    "        models.append(m)\n",
    "        print(f'  Seed {seed} done')\n",
    "\n",
    "    # Predict\n",
    "    te['pred_nn'] = predict_nn_ensemble(models, te[feature_cols].values)\n",
    "    ic = compute_monthly_ic(te, pred_col='pred_nn')\n",
    "    print(f'  Test IC: {ic.mean():.4f}')\n",
    "    nn_test_dfs.append(te)\n",
    "\n",
    "nn_test = pd.concat(nn_test_dfs)\n",
    "nn_ic = compute_monthly_ic(nn_test, pred_col='pred_nn')\n",
    "print(f'\\nOverall NN Test IC: {nn_ic.mean():.4f} (IR: {nn_ic.mean()/nn_ic.std():.3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Compare Against Week 5 Best Model (25 pts) --- SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- XGBoost with same expanding-window splits ---\n",
    "xgb_test_dfs = []\n",
    "\n",
    "for fold_idx, (train_end, val_end, test_end) in enumerate(folds):\n",
    "    print(f'Fold {fold_idx+1}...', end=' ')\n",
    "\n",
    "    tr = df_raw[df_raw.date_idx < train_end]\n",
    "    va = df_raw[(df_raw.date_idx >= train_end) & (df_raw.date_idx < val_end)]\n",
    "    te = df_raw[(df_raw.date_idx >= val_end) & (df_raw.date_idx < test_end)].copy()\n",
    "\n",
    "    xgb_model = XGBRegressor(\n",
    "        n_estimators=300, max_depth=4, learning_rate=0.05,\n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        early_stopping_rounds=20, verbosity=0\n",
    "    )\n",
    "    xgb_model.fit(\n",
    "        tr[feature_cols], tr['ret_next'],\n",
    "        eval_set=[(va[feature_cols], va['ret_next'])],\n",
    "        verbose=False\n",
    "    )\n",
    "    te['pred_xgb'] = xgb_model.predict(te[feature_cols])\n",
    "    ic = compute_monthly_ic(te, pred_col='pred_xgb')\n",
    "    print(f'IC: {ic.mean():.4f}')\n",
    "    xgb_test_dfs.append(te)\n",
    "\n",
    "xgb_test = pd.concat(xgb_test_dfs)\n",
    "xgb_ic = compute_monthly_ic(xgb_test, pred_col='pred_xgb')\n",
    "print(f'\\nOverall XGBoost Test IC: {xgb_ic.mean():.4f} (IR: {xgb_ic.mean()/xgb_ic.std():.3f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Comparison table ---\n",
    "nn_ls = compute_ls_returns(nn_test, pred_col='pred_nn')\n",
    "xgb_ls = compute_ls_returns(xgb_test, pred_col='pred_xgb')\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['NN Ensemble', 'XGBoost'],\n",
    "    'Mean IC': [nn_ic.mean(), xgb_ic.mean()],\n",
    "    'IC Std': [nn_ic.std(), xgb_ic.std()],\n",
    "    'IC IR': [nn_ic.mean()/nn_ic.std(), xgb_ic.mean()/xgb_ic.std()],\n",
    "    'IC > 0 (%)': [(nn_ic > 0).mean()*100, (xgb_ic > 0).mean()*100],\n",
    "    'LS Sharpe (ann)': [\n",
    "        nn_ls.mean()/nn_ls.std()*np.sqrt(12),\n",
    "        xgb_ls.mean()/xgb_ls.std()*np.sqrt(12)\n",
    "    ]\n",
    "}).set_index('Model')\n",
    "\n",
    "print('Model Comparison:')\n",
    "print(comparison.round(4).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Comparison plots ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Cumulative IC\n",
    "axes[0].plot(nn_ic.cumsum().values, label='NN Ensemble', linewidth=2)\n",
    "axes[0].plot(xgb_ic.cumsum().values, label='XGBoost', linewidth=2)\n",
    "axes[0].set_xlabel('Test Month')\n",
    "axes[0].set_ylabel('Cumulative IC')\n",
    "axes[0].set_title('Cumulative IC Comparison')\n",
    "axes[0].legend()\n",
    "\n",
    "# Cumulative long-short returns\n",
    "axes[1].plot((1 + nn_ls).cumprod().values, label='NN Ensemble', linewidth=2)\n",
    "axes[1].plot((1 + xgb_ls).cumprod().values, label='XGBoost', linewidth=2)\n",
    "axes[1].set_xlabel('Test Month')\n",
    "axes[1].set_ylabel('Cumulative Return')\n",
    "axes[1].set_title('Long-Short Cumulative Returns')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Ensemble (NN + XGBoost + LightGBM) (15 pts) --- SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LightGBM with same expanding-window splits ---\n",
    "lgb_test_dfs = []\n",
    "\n",
    "for fold_idx, (train_end, val_end, test_end) in enumerate(folds):\n",
    "    print(f'Fold {fold_idx+1}...', end=' ')\n",
    "\n",
    "    tr = df_raw[df_raw.date_idx < train_end]\n",
    "    va = df_raw[(df_raw.date_idx >= train_end) & (df_raw.date_idx < val_end)]\n",
    "    te = df_raw[(df_raw.date_idx >= val_end) & (df_raw.date_idx < test_end)].copy()\n",
    "\n",
    "    lgb_model = lgb.LGBMRegressor(\n",
    "        n_estimators=300, max_depth=4, learning_rate=0.05,\n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        verbosity=-1\n",
    "    )\n",
    "    lgb_model.fit(\n",
    "        tr[feature_cols], tr['ret_next'],\n",
    "        eval_set=[(va[feature_cols], va['ret_next'])],\n",
    "        callbacks=[lgb.early_stopping(20, verbose=False)]\n",
    "    )\n",
    "    te['pred_lgb'] = lgb_model.predict(te[feature_cols])\n",
    "    ic = compute_monthly_ic(te, pred_col='pred_lgb')\n",
    "    print(f'IC: {ic.mean():.4f}')\n",
    "    lgb_test_dfs.append(te)\n",
    "\n",
    "lgb_test = pd.concat(lgb_test_dfs)\n",
    "lgb_ic = compute_monthly_ic(lgb_test, pred_col='pred_lgb')\n",
    "print(f'\\nOverall LightGBM Test IC: {lgb_ic.mean():.4f} (IR: {lgb_ic.mean()/lgb_ic.std():.3f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build ensemble ---\n",
    "# Merge all predictions on the test set\n",
    "# Align by date_idx and stock_id\n",
    "\n",
    "ensemble_df = nn_test[['date_idx', 'stock_id', 'ret_next', 'pred_nn']].copy()\n",
    "ensemble_df = ensemble_df.merge(\n",
    "    xgb_test[['date_idx', 'stock_id', 'pred_xgb']],\n",
    "    on=['date_idx', 'stock_id']\n",
    ")\n",
    "ensemble_df = ensemble_df.merge(\n",
    "    lgb_test[['date_idx', 'stock_id', 'pred_lgb']],\n",
    "    on=['date_idx', 'stock_id']\n",
    ")\n",
    "\n",
    "# Rank-normalize within each month and average\n",
    "for col in ['pred_nn', 'pred_xgb', 'pred_lgb']:\n",
    "    ensemble_df[col + '_rank'] = ensemble_df.groupby('date_idx')[col].transform(\n",
    "        lambda x: rankdata(x) / len(x)\n",
    "    )\n",
    "\n",
    "ensemble_df['pred_ensemble'] = (\n",
    "    ensemble_df['pred_nn_rank']\n",
    "    + ensemble_df['pred_xgb_rank']\n",
    "    + ensemble_df['pred_lgb_rank']\n",
    ") / 3\n",
    "\n",
    "ens_ic = compute_monthly_ic(ensemble_df, pred_col='pred_ensemble')\n",
    "print(f'Ensemble Test IC: {ens_ic.mean():.4f} (IR: {ens_ic.mean()/ens_ic.std():.3f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final comparison table ---\n",
    "ens_ls = compute_ls_returns(ensemble_df, pred_col='pred_ensemble')\n",
    "lgb_ls = compute_ls_returns(lgb_test, pred_col='pred_lgb')\n",
    "\n",
    "final_comparison = pd.DataFrame({\n",
    "    'Model': ['NN Ensemble', 'XGBoost', 'LightGBM', 'Meta-Ensemble'],\n",
    "    'Mean IC': [nn_ic.mean(), xgb_ic.mean(), lgb_ic.mean(), ens_ic.mean()],\n",
    "    'IC Std': [nn_ic.std(), xgb_ic.std(), lgb_ic.std(), ens_ic.std()],\n",
    "    'IC IR': [\n",
    "        nn_ic.mean()/nn_ic.std(), xgb_ic.mean()/xgb_ic.std(),\n",
    "        lgb_ic.mean()/lgb_ic.std(), ens_ic.mean()/ens_ic.std()\n",
    "    ],\n",
    "    'IC > 0 (%)': [\n",
    "        (nn_ic > 0).mean()*100, (xgb_ic > 0).mean()*100,\n",
    "        (lgb_ic > 0).mean()*100, (ens_ic > 0).mean()*100\n",
    "    ],\n",
    "    'LS Sharpe (ann)': [\n",
    "        nn_ls.mean()/nn_ls.std()*np.sqrt(12),\n",
    "        xgb_ls.mean()/xgb_ls.std()*np.sqrt(12),\n",
    "        lgb_ls.mean()/lgb_ls.std()*np.sqrt(12),\n",
    "        ens_ls.mean()/ens_ls.std()*np.sqrt(12)\n",
    "    ]\n",
    "}).set_index('Model')\n",
    "\n",
    "print('Final Model Comparison:')\n",
    "print(final_comparison.round(4).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final comparison plots ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Cumulative IC\n",
    "for label, ic_s in [('NN', nn_ic), ('XGBoost', xgb_ic),\n",
    "                     ('LightGBM', lgb_ic), ('Ensemble', ens_ic)]:\n",
    "    axes[0].plot(ic_s.cumsum().values, label=label, linewidth=2)\n",
    "axes[0].set_xlabel('Test Month')\n",
    "axes[0].set_ylabel('Cumulative IC')\n",
    "axes[0].set_title('Cumulative IC --- All Models')\n",
    "axes[0].legend()\n",
    "\n",
    "# Cumulative long-short returns\n",
    "for label, ls_s in [('NN', nn_ls), ('XGBoost', xgb_ls),\n",
    "                     ('LightGBM', lgb_ls), ('Ensemble', ens_ls)]:\n",
    "    axes[1].plot((1 + ls_s).cumprod().values, label=label, linewidth=2)\n",
    "axes[1].set_xlabel('Test Month')\n",
    "axes[1].set_ylabel('Cumulative Return')\n",
    "axes[1].set_title('Long-Short Returns --- All Models')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Analysis --- Where NN Wins vs Trees (15 pts) --- SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Temporal analysis: rolling IC difference ---\n",
    "ic_diff = nn_ic.values - xgb_ic.values\n",
    "window = 6\n",
    "rolling_diff = pd.Series(ic_diff).rolling(window).mean()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.bar(range(len(ic_diff)), ic_diff, alpha=0.4, color='steelblue', label='Monthly IC diff')\n",
    "ax.plot(rolling_diff.values, color='red', linewidth=2, label=f'{window}-month rolling mean')\n",
    "ax.axhline(y=0, color='black', linewidth=0.5)\n",
    "ax.set_xlabel('Test Month')\n",
    "ax.set_ylabel('IC(NN) - IC(XGBoost)')\n",
    "ax.set_title('When Does NN Beat XGBoost? (positive = NN wins)')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'NN wins in {(ic_diff > 0).mean():.1%} of months')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cross-sectional analysis: IC by stock characteristic ---\n",
    "# Split by size (large vs small) and volatility (high vs low)\n",
    "\n",
    "# Get raw features for grouping\n",
    "raw_test = df_raw[df_raw.date_idx >= folds[0][2]].copy()\n",
    "raw_test['size_group'] = raw_test.groupby('date_idx')['size'].transform(\n",
    "    lambda x: pd.qcut(x, 2, labels=['Small', 'Large'])\n",
    ")\n",
    "raw_test['vol_group'] = raw_test.groupby('date_idx')['vol_20d'].transform(\n",
    "    lambda x: pd.qcut(x, 2, labels=['Low Vol', 'High Vol'])\n",
    ")\n",
    "\n",
    "# Merge with predictions\n",
    "merged = raw_test[['date_idx', 'stock_id', 'size_group', 'vol_group']].merge(\n",
    "    ensemble_df[['date_idx', 'stock_id', 'pred_nn', 'pred_xgb', 'ret_next']],\n",
    "    on=['date_idx', 'stock_id']\n",
    ")\n",
    "\n",
    "# IC by group\n",
    "results = []\n",
    "for group_col in ['size_group', 'vol_group']:\n",
    "    for group_val in merged[group_col].unique():\n",
    "        sub = merged[merged[group_col] == group_val]\n",
    "        nn_ic_g = compute_monthly_ic(sub, pred_col='pred_nn')\n",
    "        xgb_ic_g = compute_monthly_ic(sub, pred_col='pred_xgb')\n",
    "        results.append({\n",
    "            'Group': f'{group_col}: {group_val}',\n",
    "            'NN IC': nn_ic_g.mean(),\n",
    "            'XGB IC': xgb_ic_g.mean(),\n",
    "            'NN - XGB': nn_ic_g.mean() - xgb_ic_g.mean()\n",
    "        })\n",
    "\n",
    "group_table = pd.DataFrame(results).set_index('Group')\n",
    "print('IC by Stock Characteristic:')\n",
    "print(group_table.round(4).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "**1. By time period:**\n",
    "\n",
    "The rolling IC difference shows that the neural net and XGBoost take turns outperforming each other. Neither model consistently dominates. In periods of high cross-sectional dispersion, the NN may capture nonlinear interactions better, while in stable periods, XGBoost's piecewise-constant approximation is sufficient. This motivates the ensemble approach.\n",
    "\n",
    "**2. By stock characteristic:**\n",
    "\n",
    "The NN tends to perform relatively better among large-cap stocks where factor relationships are smoother and more stable. For small-cap and high-volatility stocks, tree models can sometimes perform better because the signal is more threshold-like (e.g., extreme momentum reversals). The ensemble benefits from combining both perspectives.\n",
    "\n",
    "**3. Feature importance:**\n",
    "\n",
    "XGBoost feature importance is straightforward to compute (gain-based). For the NN, we would need SHAP or permutation importance. The key difference is that the NN can model smooth interactions (e.g., the sin(mom_12m * size) term in our DGP), while trees approximate these with step functions. In practice, this means the NN better captures features that interact continuously rather than at discrete thresholds."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}