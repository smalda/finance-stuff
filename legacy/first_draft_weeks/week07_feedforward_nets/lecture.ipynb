{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7 --- Feedforward Neural Networks for Asset Pricing\n",
    "\n",
    "**Quantitative Finance ML Course**\n",
    "\n",
    "---\n",
    "\n",
    "## Roadmap\n",
    "\n",
    "1. PyTorch fundamentals for finance (tensors, Dataset/DataLoader)\n",
    "2. The Gu-Kelly-Xiu neural net architecture\n",
    "3. Training pitfalls in finance\n",
    "4. Financial loss functions\n",
    "5. Ensemble methods\n",
    "6. Working demo: feedforward net for cross-sectional return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'MPS available: {torch.backends.mps.is_available()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. PyTorch Fundamentals for Finance\n",
    "\n",
    "### Why PyTorch for Quant Finance?\n",
    "\n",
    "- **Flexibility**: custom loss functions (IC-based, asymmetric), custom architectures\n",
    "- **GPU/MPS acceleration**: train on Apple Silicon or NVIDIA GPUs\n",
    "- **Research-friendly**: most academic finance DL papers use PyTorch\n",
    "- **Ecosystem**: Lightning, TorchMetrics, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors on MPS\n",
    "\n",
    "Apple Silicon Macs have the MPS (Metal Performance Shaders) backend. It works like CUDA but for Apple GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device selection: prefer MPS > CUDA > CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Basic tensor operations\n",
    "x = torch.randn(1000, 50)  # 1000 stocks, 50 features\n",
    "x_device = x.to(device)\n",
    "print(f'Tensor shape: {x_device.shape}, device: {x_device.device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and DataLoader for Financial Panels\n",
    "\n",
    "Financial data is a **panel**: stocks x time x features. We need a custom `Dataset` that respects this structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossSectionalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for cross-sectional stock prediction.\n",
    "    Each sample is one (stock, date) observation.\n",
    "    \"\"\"\n",
    "    def __init__(self, features: np.ndarray, targets: np.ndarray, dates: np.ndarray):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: (N, K) array of K features for N observations\n",
    "            targets: (N,) array of forward returns\n",
    "            dates: (N,) array of date indices (for temporal splitting)\n",
    "        \"\"\"\n",
    "        self.X = torch.tensor(features, dtype=torch.float32)\n",
    "        self.y = torch.tensor(targets, dtype=torch.float32)\n",
    "        self.dates = dates\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "# Example: create a dummy panel\n",
    "n_stocks, n_months, n_features = 200, 120, 10\n",
    "N = n_stocks * n_months  # total observations\n",
    "\n",
    "features = np.random.randn(N, n_features)\n",
    "targets = np.random.randn(N) * 0.05  # monthly returns\n",
    "dates = np.repeat(np.arange(n_months), n_stocks)\n",
    "\n",
    "ds = CrossSectionalDataset(features, targets, dates)\n",
    "print(f'Dataset size: {len(ds)}')\n",
    "print(f'Sample: X shape={ds[0][0].shape}, y={ds[0][1].item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key point**: When creating the DataLoader, do NOT shuffle across time. We'll cover proper temporal splitting shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cross-sectional models, we can shuffle WITHIN each month\n",
    "# but not across months. A simple approach:\n",
    "# - split data by date into train/val/test\n",
    "# - shuffle within train, don't shuffle val/test\n",
    "\n",
    "train_mask = dates < 72   # first 72 months = train\n",
    "val_mask = (dates >= 72) & (dates < 96)  # next 24 months = val\n",
    "test_mask = dates >= 96   # last 24 months = test\n",
    "\n",
    "train_ds = CrossSectionalDataset(features[train_mask], targets[train_mask], dates[train_mask])\n",
    "val_ds = CrossSectionalDataset(features[val_mask], targets[val_mask], dates[val_mask])\n",
    "test_ds = CrossSectionalDataset(features[test_mask], targets[test_mask], dates[test_mask])\n",
    "\n",
    "# Shuffle train only\n",
    "train_loader = DataLoader(train_ds, batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=1024, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=1024, shuffle=False)\n",
    "\n",
    "print(f'Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. The Gu-Kelly-Xiu Neural Net\n",
    "\n",
    "**Reference**: Gu, Kelly, Xiu (2020) \"Empirical Asset Pricing via Machine Learning\", *Review of Financial Studies*.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "Their feedforward net (called NN3 in the paper) uses:\n",
    "- **3 hidden layers**: 32 -> 16 -> 8 neurons\n",
    "- **ReLU** activations\n",
    "- **Batch normalization** after each linear layer\n",
    "- **Dropout** for regularization\n",
    "- **Single output**: predicted return (or return rank)\n",
    "\n",
    "The architecture is deliberately **small** --- financial signal is weak, so large nets just memorize noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuKellyXiuNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Gu-Kelly-Xiu NN3 architecture for cross-sectional return prediction.\n",
    "    3 hidden layers: 32 -> 16 -> 8, with BN, ReLU, and dropout.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # Layer 1: input -> 32\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            # Layer 2: 32 -> 16\n",
    "            nn.Linear(32, 16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            # Layer 3: 16 -> 8\n",
    "            nn.Linear(16, 8),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            # Output: 8 -> 1\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "\n",
    "# Instantiate and inspect\n",
    "model = GuKellyXiuNet(input_dim=n_features)\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'\\nTrainable parameters: {n_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why This Architecture?\n",
    "\n",
    "| Design choice | Reason |\n",
    "|---|---|\n",
    "| Small layers (32-16-8) | Financial signals are weak; overfitting is the main risk |\n",
    "| Batch normalization | Stabilizes training with heterogeneous cross-sectional features |\n",
    "| Dropout (0.5) | Strong regularization needed for noisy financial data |\n",
    "| ReLU | Simple, works well, avoids vanishing gradients |\n",
    "| Single output | Predicting one number: next-period return |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Training Pitfalls in Finance\n",
    "\n",
    "### Pitfall 1: Shuffling Across Time\n",
    "\n",
    "**Wrong**: `shuffle=True` on the entire dataset (leaks future info into training).\n",
    "\n",
    "**Right**: Split by time first, then shuffle only within the training set.\n",
    "\n",
    "### Pitfall 2: No Early Stopping\n",
    "\n",
    "Financial data is so noisy that NNs overfit quickly. Always use early stopping on a temporal validation set.\n",
    "\n",
    "### Pitfall 3: Single Random Seed\n",
    "\n",
    "NN predictions are sensitive to initialization. Always train with multiple seeds and ensemble.\n",
    "\n",
    "### Pitfall 4: Normalizing Targets Wrong\n",
    "\n",
    "Cross-sectional normalization (rank or z-score within each month) is better than time-series normalization for returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Simple early stopping to prevent overfitting.\"\"\"\n",
    "    def __init__(self, patience=10, min_delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.best_state = None\n",
    "\n",
    "    def step(self, val_loss, model):\n",
    "        \"\"\"Returns True if training should stop.\"\"\"\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            return self.counter >= self.patience\n",
    "\n",
    "    def restore_best(self, model):\n",
    "        \"\"\"Restore the best model weights.\"\"\"\n",
    "        if self.best_state is not None:\n",
    "            model.load_state_dict(self.best_state)\n",
    "\n",
    "\n",
    "print('Early stopping class ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Train/Val/Test Split\n",
    "\n",
    "The standard approach:\n",
    "\n",
    "```\n",
    "Time ──────────────────────────────────────────────►\n",
    "│     TRAIN (60%)     │  VAL (20%)  │  TEST (20%)  │\n",
    "```\n",
    "\n",
    "For expanding-window CV (used in HW):\n",
    "\n",
    "```\n",
    "Fold 1: [=TRAIN=][VAL][TEST]\n",
    "Fold 2: [==TRAIN==][VAL][TEST]\n",
    "Fold 3: [===TRAIN===][VAL][TEST]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Financial Loss Functions\n",
    "\n",
    "### Standard MSE\n",
    "\n",
    "$$\\mathcal{L}_{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (r_i - \\hat{r}_i)^2$$\n",
    "\n",
    "Simple, but treats all stocks equally. A $1 stock and a $100B stock get the same weight.\n",
    "\n",
    "### Weighted MSE\n",
    "\n",
    "Weight by market cap or inverse volatility:\n",
    "\n",
    "$$\\mathcal{L}_{WMSE} = \\frac{1}{N} \\sum_{i=1}^{N} w_i (r_i - \\hat{r}_i)^2$$\n",
    "\n",
    "### IC-Based Loss\n",
    "\n",
    "Instead of minimizing prediction error, maximize the **rank correlation** (Information Coefficient) between predictions and actual returns:\n",
    "\n",
    "$$\\mathcal{L}_{IC} = -\\text{corr}(\\hat{r}, r)$$\n",
    "\n",
    "We care about ranking stocks correctly, not predicting exact returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_pred, y_true):\n",
    "    \"\"\"Standard MSE loss.\"\"\"\n",
    "    return ((y_pred - y_true) ** 2).mean()\n",
    "\n",
    "\n",
    "def weighted_mse_loss(y_pred, y_true, weights):\n",
    "    \"\"\"MSE weighted by e.g. market cap.\"\"\"\n",
    "    w = weights / weights.sum()\n",
    "    return (w * (y_pred - y_true) ** 2).sum()\n",
    "\n",
    "\n",
    "def ic_loss(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Negative Pearson correlation (IC) loss.\n",
    "    Minimizing this maximizes cross-sectional rank correlation.\n",
    "    \"\"\"\n",
    "    y_pred_dm = y_pred - y_pred.mean()\n",
    "    y_true_dm = y_true - y_true.mean()\n",
    "    corr = (y_pred_dm * y_true_dm).sum() / (\n",
    "        torch.sqrt((y_pred_dm ** 2).sum() * (y_true_dm ** 2).sum()) + 1e-8\n",
    "    )\n",
    "    return -corr  # negative because we minimize\n",
    "\n",
    "\n",
    "# Quick demo\n",
    "y_pred = torch.randn(200)\n",
    "y_true = y_pred * 0.3 + torch.randn(200) * 0.7  # weak signal\n",
    "\n",
    "print(f'MSE loss:  {mse_loss(y_pred, y_true):.4f}')\n",
    "print(f'IC loss:   {ic_loss(y_pred, y_true):.4f}')\n",
    "print(f'(IC = {-ic_loss(y_pred, y_true):.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Ensemble Methods\n",
    "\n",
    "Neural nets in finance benefit hugely from ensembling:\n",
    "\n",
    "1. **Seed ensembles**: Train the same architecture with different random seeds, average predictions\n",
    "2. **Architecture ensembles**: Combine different architectures (NN + XGBoost + LightGBM)\n",
    "3. **Temporal ensembles**: Average predictions from models trained on different expanding windows\n",
    "\n",
    "Gu-Kelly-Xiu use 10 random seed ensembles. This is standard practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(model_class, input_dim, train_loader, val_loader,\n",
    "                   n_seeds=5, n_epochs=100, lr=1e-3, device='cpu'):\n",
    "    \"\"\"\n",
    "    Train an ensemble of models with different random seeds.\n",
    "    Returns list of trained models.\n",
    "    \"\"\"\n",
    "    models = []\n",
    "\n",
    "    for seed in range(n_seeds):\n",
    "        torch.manual_seed(seed)\n",
    "        model = model_class(input_dim).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "        stopper = EarlyStopping(patience=10)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            # Train\n",
    "            model.train()\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                loss = mse_loss(model(X_batch), y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Validate\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in val_loader:\n",
    "                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                    val_losses.append(mse_loss(model(X_batch), y_batch).item())\n",
    "\n",
    "            if stopper.step(np.mean(val_losses), model):\n",
    "                break\n",
    "\n",
    "        stopper.restore_best(model)\n",
    "        model.to('cpu')\n",
    "        models.append(model)\n",
    "        print(f'  Seed {seed}: best val loss = {stopper.best_loss:.6f}, '\n",
    "              f'stopped at epoch {epoch+1}')\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "def predict_ensemble(models, X, device='cpu'):\n",
    "    \"\"\"Average predictions from an ensemble.\"\"\"\n",
    "    preds = []\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        with torch.no_grad():\n",
    "            preds.append(model(X_tensor).cpu().numpy())\n",
    "        model.to('cpu')\n",
    "    return np.mean(preds, axis=0)\n",
    "\n",
    "\n",
    "print('Ensemble functions ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Working Demo: Feedforward Net for Return Prediction\n",
    "\n",
    "Let's put it all together. We'll:\n",
    "1. Generate synthetic cross-sectional data (mimicking momentum, volatility, size features)\n",
    "2. Build the Gu-Kelly-Xiu net\n",
    "3. Train with proper temporal splitting and early stopping\n",
    "4. Evaluate with IC (Information Coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate synthetic cross-sectional data ---\n",
    "# Mimics the features from Weeks 4-5: momentum, volatility, etc.\n",
    "\n",
    "np.random.seed(42)\n",
    "n_stocks = 500\n",
    "n_months = 180  # 15 years\n",
    "\n",
    "records = []\n",
    "for t in range(n_months):\n",
    "    for i in range(n_stocks):\n",
    "        # Features\n",
    "        mom_1m = np.random.randn() * 0.08\n",
    "        mom_12m = np.random.randn() * 0.20\n",
    "        vol_20d = np.abs(np.random.randn()) * 0.02 + 0.01\n",
    "        size = np.random.randn() * 2 + 15  # log market cap\n",
    "        bm = np.random.randn() * 0.5  # book-to-market\n",
    "        turnover = np.abs(np.random.randn()) * 0.01\n",
    "        rev_1m = np.random.randn() * 0.05  # short-term reversal\n",
    "\n",
    "        # Target: next-month return with weak factor structure\n",
    "        ret_next = (\n",
    "            -0.002 * mom_1m       # reversal\n",
    "            + 0.003 * mom_12m     # momentum\n",
    "            - 0.005 * vol_20d     # low-vol premium\n",
    "            + 0.001 * bm          # value\n",
    "            + 0.002 * np.sin(mom_12m * size)  # nonlinearity!\n",
    "            + np.random.randn() * 0.08  # noise\n",
    "        )\n",
    "\n",
    "        records.append({\n",
    "            'date_idx': t, 'stock_id': i,\n",
    "            'mom_1m': mom_1m, 'mom_12m': mom_12m, 'vol_20d': vol_20d,\n",
    "            'size': size, 'bm': bm, 'turnover': turnover, 'rev_1m': rev_1m,\n",
    "            'ret_next': ret_next\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "feature_cols = ['mom_1m', 'mom_12m', 'vol_20d', 'size', 'bm', 'turnover', 'rev_1m']\n",
    "\n",
    "print(f'Panel shape: {df.shape}')\n",
    "print(f'Date range: 0 to {df.date_idx.max()}')\n",
    "print(f'Features: {feature_cols}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cross-sectional normalization ---\n",
    "# Rank-transform features within each month (standard in finance ML)\n",
    "\n",
    "for col in feature_cols:\n",
    "    df[col] = df.groupby('date_idx')[col].transform(\n",
    "        lambda x: (x.rank() - 1) / (len(x) - 1) - 0.5  # map to [-0.5, 0.5]\n",
    "    )\n",
    "\n",
    "print('After cross-sectional rank normalization:')\n",
    "df[feature_cols].describe().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Temporal split ---\n",
    "train_end = 108   # 9 years\n",
    "val_end = 144     # 3 years val\n",
    "# test: last 3 years\n",
    "\n",
    "train_df = df[df.date_idx < train_end]\n",
    "val_df = df[(df.date_idx >= train_end) & (df.date_idx < val_end)]\n",
    "test_df = df[df.date_idx >= val_end]\n",
    "\n",
    "X_train = train_df[feature_cols].values\n",
    "y_train = train_df['ret_next'].values\n",
    "X_val = val_df[feature_cols].values\n",
    "y_val = val_df['ret_next'].values\n",
    "X_test = test_df[feature_cols].values\n",
    "y_test = test_df['ret_next'].values\n",
    "\n",
    "train_ds = CrossSectionalDataset(X_train, y_train, train_df['date_idx'].values)\n",
    "val_ds = CrossSectionalDataset(X_val, y_val, val_df['date_idx'].values)\n",
    "test_ds = CrossSectionalDataset(X_test, y_test, test_df['date_idx'].values)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=2048, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=4096, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=4096, shuffle=False)\n",
    "\n",
    "print(f'Train: {len(train_ds):,}, Val: {len(val_ds):,}, Test: {len(test_ds):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train the model ---\n",
    "print('Training NN ensemble (5 seeds)...')\n",
    "use_device = 'cpu'  # change to 'mps' or 'cuda' if available\n",
    "\n",
    "models = train_ensemble(\n",
    "    GuKellyXiuNet,\n",
    "    input_dim=len(feature_cols),\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    n_seeds=5,\n",
    "    n_epochs=100,\n",
    "    lr=1e-3,\n",
    "    device=use_device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluate with IC ---\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def compute_monthly_ic(df, pred_col='pred', ret_col='ret_next'):\n",
    "    \"\"\"Compute monthly rank IC (Spearman) and return a Series.\"\"\"\n",
    "    ic_series = df.groupby('date_idx').apply(\n",
    "        lambda g: spearmanr(g[pred_col], g[ret_col])[0]\n",
    "    )\n",
    "    return ic_series\n",
    "\n",
    "\n",
    "# Predict on test set\n",
    "test_df = test_df.copy()\n",
    "test_df['pred'] = predict_ensemble(models, X_test)\n",
    "\n",
    "# Compute monthly IC\n",
    "ic_series = compute_monthly_ic(test_df)\n",
    "\n",
    "print(f'Test IC:')\n",
    "print(f'  Mean IC:   {ic_series.mean():.4f}')\n",
    "print(f'  Std IC:    {ic_series.std():.4f}')\n",
    "print(f'  IR (IC/std): {ic_series.mean()/ic_series.std():.4f}')\n",
    "print(f'  IC > 0:    {(ic_series > 0).mean():.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot IC over time ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Monthly IC\n",
    "axes[0].bar(ic_series.index, ic_series.values, color='steelblue', alpha=0.7)\n",
    "axes[0].axhline(y=ic_series.mean(), color='red', linestyle='--',\n",
    "                label=f'Mean IC = {ic_series.mean():.4f}')\n",
    "axes[0].axhline(y=0, color='black', linewidth=0.5)\n",
    "axes[0].set_xlabel('Month')\n",
    "axes[0].set_ylabel('Rank IC')\n",
    "axes[0].set_title('Monthly Information Coefficient (Test Set)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Cumulative IC\n",
    "axes[1].plot(ic_series.index, ic_series.cumsum().values, color='steelblue', linewidth=2)\n",
    "axes[1].set_xlabel('Month')\n",
    "axes[1].set_ylabel('Cumulative IC')\n",
    "axes[1].set_title('Cumulative IC (Test Set)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Long-short portfolio returns ---\n",
    "# Each month: go long top quintile, short bottom quintile\n",
    "\n",
    "def compute_ls_returns(df, pred_col='pred', ret_col='ret_next', n_quantiles=5):\n",
    "    \"\"\"Compute long-short (top minus bottom quintile) returns.\"\"\"\n",
    "    def _ls(g):\n",
    "        g = g.copy()\n",
    "        g['q'] = pd.qcut(g[pred_col], n_quantiles, labels=False, duplicates='drop')\n",
    "        long_ret = g[g['q'] == n_quantiles - 1][ret_col].mean()\n",
    "        short_ret = g[g['q'] == 0][ret_col].mean()\n",
    "        return long_ret - short_ret\n",
    "\n",
    "    return df.groupby('date_idx').apply(_ls)\n",
    "\n",
    "\n",
    "ls_returns = compute_ls_returns(test_df)\n",
    "\n",
    "print(f'Long-Short Portfolio (Test):')\n",
    "print(f'  Mean monthly return: {ls_returns.mean():.4f} ({ls_returns.mean()*12:.2%} annualized)')\n",
    "print(f'  Sharpe (annualized):  {ls_returns.mean()/ls_returns.std()*np.sqrt(12):.2f}')\n",
    "\n",
    "# Plot cumulative returns\n",
    "cum_ret = (1 + ls_returns).cumprod()\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(cum_ret.index, cum_ret.values, linewidth=2, color='steelblue')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Cumulative Return')\n",
    "plt.title('Long-Short Portfolio: Cumulative Returns (Test Set)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Architecture**: The Gu-Kelly-Xiu net is deliberately small (32-16-8). Financial signals are weak.\n",
    "2. **Temporal splitting**: Never shuffle across time. Train/val/test must be ordered chronologically.\n",
    "3. **Early stopping**: Essential. Financial NNs overfit in just a few epochs.\n",
    "4. **Loss function**: IC-based loss aligns with what we actually care about (ranking stocks).\n",
    "5. **Ensembles**: Always average over random seeds. Cheap and effective.\n",
    "6. **Cross-sectional normalization**: Rank-transform features within each month.\n",
    "\n",
    "### Next Week\n",
    "\n",
    "Week 8: LSTM/GRU for volatility forecasting --- when **sequence** matters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}