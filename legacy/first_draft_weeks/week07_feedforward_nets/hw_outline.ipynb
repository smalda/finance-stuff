{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7 Homework --- Deep Cross-Sectional Model\n",
    "\n",
    "**Quantitative Finance ML Course**\n",
    "\n",
    "**Total: 100 points**\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this homework, you will:\n",
    "1. Implement the Gu-Kelly-Xiu feedforward architecture in PyTorch\n",
    "2. Train with expanding-window cross-validation\n",
    "3. Compare against your best tree-based model from Week 5\n",
    "4. Build a model ensemble (NN + XGBoost + LightGBM)\n",
    "5. Analyze where neural nets win vs trees\n",
    "\n",
    "### Grading\n",
    "\n",
    "| Part | Points | Topic |\n",
    "|------|--------|-------|\n",
    "| 1 | 20 | Gu-Kelly-Xiu architecture |\n",
    "| 2 | 25 | Expanding-window CV with MPS |\n",
    "| 3 | 25 | Compare against Week 5 best model |\n",
    "| 4 | 15 | Ensemble (NN + XGBoost + LightGBM) |\n",
    "| 5 | 15 | Analysis: where NN wins vs trees |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.stats import spearmanr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "\n",
    "# Device selection\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data generation (same synthetic panel as lecture/seminar) ---\n",
    "\n",
    "np.random.seed(42)\n",
    "n_stocks = 500\n",
    "n_months = 240  # 20 years for expanding window\n",
    "\n",
    "records = []\n",
    "for t in range(n_months):\n",
    "    for i in range(n_stocks):\n",
    "        mom_1m = np.random.randn() * 0.08\n",
    "        mom_12m = np.random.randn() * 0.20\n",
    "        vol_20d = np.abs(np.random.randn()) * 0.02 + 0.01\n",
    "        size = np.random.randn() * 2 + 15\n",
    "        bm = np.random.randn() * 0.5\n",
    "        turnover = np.abs(np.random.randn()) * 0.01\n",
    "        rev_1m = np.random.randn() * 0.05\n",
    "\n",
    "        ret_next = (\n",
    "            -0.002 * mom_1m + 0.003 * mom_12m - 0.005 * vol_20d\n",
    "            + 0.001 * bm + 0.002 * np.sin(mom_12m * size)\n",
    "            + np.random.randn() * 0.08\n",
    "        )\n",
    "\n",
    "        records.append({\n",
    "            'date_idx': t, 'stock_id': i,\n",
    "            'mom_1m': mom_1m, 'mom_12m': mom_12m, 'vol_20d': vol_20d,\n",
    "            'size': size, 'bm': bm, 'turnover': turnover, 'rev_1m': rev_1m,\n",
    "            'ret_next': ret_next\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "feature_cols = ['mom_1m', 'mom_12m', 'vol_20d', 'size', 'bm', 'turnover', 'rev_1m']\n",
    "\n",
    "# Cross-sectional rank normalization\n",
    "for col in feature_cols:\n",
    "    df[col] = df.groupby('date_idx')[col].transform(\n",
    "        lambda x: (x.rank() - 1) / (len(x) - 1) - 0.5\n",
    "    )\n",
    "\n",
    "print(f'Panel: {df.shape[0]:,} obs, {n_months} months, {n_stocks} stocks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Gu-Kelly-Xiu Architecture (20 pts)\n",
    "\n",
    "Implement the NN3 architecture from the paper:\n",
    "- 3 hidden layers: 32 -> 16 -> 8\n",
    "- BatchNorm + ReLU + Dropout after each hidden layer\n",
    "- Single linear output\n",
    "\n",
    "Also implement:\n",
    "- `CrossSectionalDataset` class\n",
    "- `EarlyStopping` class\n",
    "\n",
    "**Grading**: Architecture correct (10), Dataset/EarlyStopping (5), forward pass works (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossSectionalDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def step(self, val_loss, model):\n",
    "        # TODO: return True if should stop\n",
    "        pass\n",
    "\n",
    "    def restore_best(self, model):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "\n",
    "class GuKellyXiuNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_sizes=(32, 16, 8), dropout=0.5):\n",
    "        super().__init__()\n",
    "        # TODO: Build 3-layer network with BN, ReLU, Dropout\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "\n",
    "# Verify\n",
    "model = GuKellyXiuNet(input_dim=len(feature_cols))\n",
    "print(model)\n",
    "x_test = torch.randn(32, len(feature_cols))\n",
    "print(f'Output shape: {model(x_test).shape}')  # should be (32,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Expanding-Window CV with MPS (25 pts)\n",
    "\n",
    "Implement expanding-window cross-validation:\n",
    "\n",
    "```\n",
    "Fold 1: Train [0, 120)   Val [120, 144)  Test [144, 168)\n",
    "Fold 2: Train [0, 144)   Val [144, 168)  Test [168, 192)\n",
    "Fold 3: Train [0, 168)   Val [168, 192)  Test [192, 216)\n",
    "Fold 4: Train [0, 192)   Val [192, 216)  Test [216, 240)\n",
    "```\n",
    "\n",
    "For each fold:\n",
    "1. Train an ensemble of 3 models (different seeds) with early stopping\n",
    "2. Predict on the test period\n",
    "3. Compute monthly IC\n",
    "\n",
    "Use MPS (or CUDA) for training if available.\n",
    "\n",
    "**Grading**: Correct splitting (10), training loop with early stopping (10), MPS usage (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expanding_window_folds(n_months, train_start=0, initial_train=120,\n",
    "                            val_size=24, test_size=24):\n",
    "    \"\"\"\n",
    "    TODO: Generate expanding-window fold definitions.\n",
    "    Returns list of (train_end, val_end, test_end) tuples.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# folds = expanding_window_folds(n_months)\n",
    "# for i, (tr, va, te) in enumerate(folds):\n",
    "#     print(f'Fold {i+1}: Train [0, {tr})  Val [{tr}, {va})  Test [{va}, {te})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_model(model, train_loader, val_loader, n_epochs=100,\n",
    "                        lr=1e-3, device='cpu'):\n",
    "    \"\"\"\n",
    "    TODO: Train a single model with early stopping.\n",
    "    Return the trained model (with best weights restored).\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def train_fold(df, feature_cols, train_end, val_end, test_end,\n",
    "               n_seeds=3, device='cpu'):\n",
    "    \"\"\"\n",
    "    TODO: Train an ensemble for one fold.\n",
    "    1. Split data into train/val/test by date_idx\n",
    "    2. Create DataLoaders\n",
    "    3. Train n_seeds models\n",
    "    4. Predict on test, compute IC\n",
    "    Return: test_df with predictions, IC series\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run expanding-window CV\n",
    "# Collect all test predictions and IC values\n",
    "\n",
    "# all_test_preds = []\n",
    "# all_ic = []\n",
    "# for fold_idx, (tr, va, te) in enumerate(folds):\n",
    "#     print(f'\\nFold {fold_idx+1}...')\n",
    "#     test_df, ic = train_fold(df, feature_cols, tr, va, te, device=str(device))\n",
    "#     all_test_preds.append(test_df)\n",
    "#     all_ic.append(ic)\n",
    "\n",
    "print('Expanding-window CV: implement above')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Compare Against Week 5 Best Model (25 pts)\n",
    "\n",
    "Train an XGBoost model using the same expanding-window splits and compare:\n",
    "\n",
    "1. Mean IC\n",
    "2. IC information ratio (IC / std(IC))\n",
    "3. Long-short portfolio Sharpe ratio\n",
    "4. Cumulative return plot\n",
    "\n",
    "**Grading**: XGBoost implementation (10), fair comparison (10), plots and analysis (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import and train XGBoost with same expanding-window splits\n",
    "# from xgboost import XGBRegressor\n",
    "\n",
    "# Use the same folds as Part 2\n",
    "# For each fold:\n",
    "#   1. Train XGBoost on train set\n",
    "#   2. Predict on test set\n",
    "#   3. Compute IC\n",
    "\n",
    "print('XGBoost comparison: implement above')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create comparison table\n",
    "# Columns: Model, Mean IC, IC Std, IC IR, Sharpe, Hit Rate (IC > 0)\n",
    "# Rows: NN Ensemble, XGBoost\n",
    "\n",
    "print('Comparison table: implement above')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot cumulative long-short returns for both models\n",
    "# Side by side: cumulative returns, monthly IC\n",
    "\n",
    "print('Comparison plots: implement above')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Ensemble (NN + XGBoost + LightGBM) (15 pts)\n",
    "\n",
    "Build a meta-ensemble that combines predictions from:\n",
    "1. Neural net ensemble (from Part 2)\n",
    "2. XGBoost (from Part 3)\n",
    "3. LightGBM (new)\n",
    "\n",
    "Combination method: simple average of rank-normalized predictions.\n",
    "\n",
    "**Grading**: LightGBM implementation (5), ensemble logic (5), improvement over individual models (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train LightGBM with same expanding-window splits\n",
    "# import lightgbm as lgb\n",
    "\n",
    "print('LightGBM: implement above')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Combine predictions\n",
    "# 1. Rank-normalize each model's predictions within each month\n",
    "# 2. Average the ranks\n",
    "# 3. Compute IC for the ensemble\n",
    "\n",
    "# def ensemble_predictions(nn_preds, xgb_preds, lgb_preds):\n",
    "#     \"\"\"Average rank-normalized predictions.\"\"\"\n",
    "#     from scipy.stats import rankdata\n",
    "#     ranks = [rankdata(p) for p in [nn_preds, xgb_preds, lgb_preds]]\n",
    "#     return np.mean(ranks, axis=0)\n",
    "\n",
    "print('Ensemble: implement above')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Final comparison table with all 4 models\n",
    "# NN, XGBoost, LightGBM, Ensemble\n",
    "\n",
    "print('Final comparison: implement above')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Analysis --- Where NN Wins vs Trees (15 pts)\n",
    "\n",
    "Analyze when and why the neural net outperforms or underperforms tree-based models.\n",
    "\n",
    "**Required analysis** (write 2-3 paragraphs for each):\n",
    "\n",
    "1. **By time period**: Are there periods where NN beats trees? Do these correspond to market regimes?\n",
    "2. **By stock characteristic**: Does NN do better for large-cap vs small-cap? High-vol vs low-vol?\n",
    "3. **Feature importance**: Use SHAP or permutation importance to understand what NN and XGBoost focus on.\n",
    "\n",
    "**Grading**: Temporal analysis (5), cross-sectional analysis (5), depth of insight (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Temporal analysis\n",
    "# Plot rolling IC difference (NN IC minus XGBoost IC) over time\n",
    "# Identify periods where NN dominates vs underperforms\n",
    "\n",
    "print('Temporal analysis: implement above')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Cross-sectional analysis\n",
    "# Split stocks into groups by size, vol, etc.\n",
    "# Compare IC within each group for NN vs XGBoost\n",
    "\n",
    "print('Cross-sectional analysis: implement above')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Analysis\n",
    "\n",
    "*Write your analysis here (2-3 paragraphs per question):*\n",
    "\n",
    "**1. By time period:**\n",
    "\n",
    "...\n",
    "\n",
    "**2. By stock characteristic:**\n",
    "\n",
    "...\n",
    "\n",
    "**3. Feature importance:**\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Submission Checklist\n",
    "\n",
    "- [ ] Part 1: `GuKellyXiuNet` class works, forward pass produces correct shape\n",
    "- [ ] Part 2: Expanding-window CV produces IC for each fold\n",
    "- [ ] Part 3: Fair comparison with XGBoost, comparison table and plots\n",
    "- [ ] Part 4: Ensemble predictions and IC\n",
    "- [ ] Part 5: Written analysis (at least 2 paragraphs per question)\n",
    "- [ ] All cells run without errors\n",
    "- [ ] Notebook is clean and well-organized"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}