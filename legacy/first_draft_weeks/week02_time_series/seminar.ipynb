{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Seminar — Time Series Properties in Practice\n",
    "\n",
    "The lecture built the theory: stationarity tests, fractional differentiation, GARCH volatility modeling. You saw demonstrations on SPY and a handful of assets. Now you'll stress-test those ideas at scale. Exercise 1 runs 60 stationarity tests across 5 assets and 6 representations to reveal which transformations actually produce stationary series — and where ADF and KPSS disagree. Exercise 2 is the main event: you'll find the minimum fractional differencing order $d^*$ for 15 assets spanning five asset classes, discovering that the \"right\" amount of differencing is not a universal constant but an asset-specific property tied to how each market carries memory. Exercises 3 and 4 fit GARCH models — first the workhorse (1,1), then the fancier variants — and settle the question of whether added complexity buys you anything out of sample. By the end, you'll have hard numbers, not just intuitions, for decisions you'll make every week for the rest of this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q yfinance matplotlib scipy statsmodels arch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from arch import arch_model\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (12, 6), 'figure.dpi': 100,\n",
    "    'font.size': 11, 'axes.grid': True, 'grid.alpha': 0.3\n",
    "})\n",
    "\n",
    "def get_close(data):\n",
    "    \"\"\"Extract close prices, handling yfinance MultiIndex.\"\"\"\n",
    "    if isinstance(data.columns, pd.MultiIndex):\n",
    "        return data['Close']\n",
    "    return data[['Close']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll download all the data we need for all four exercises in a single cell. Exercise 1 uses five assets; Exercise 2 expands to 15 across five asset classes; Exercises 3 and 4 pull from the same pool. One download, no redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All tickers needed across all four exercises\n",
    "all_tickers = [\n",
    "    'SPY', 'QQQ', 'IWM', 'EEM', 'XLU',        # Equity ETFs\n",
    "    'TLT', 'IEF', 'HYG',                        # Bond ETFs\n",
    "    'GLD', 'USO', 'SLV',                         # Commodities\n",
    "    'UUP', 'FXE',                                # Currency ETFs\n",
    "    'VIXY', 'SVXY',                              # Volatility-adjacent\n",
    "    'TSLA', 'JNJ', 'BTC-USD'                     # Additional for Ex1/Ex3\n",
    "]\n",
    "\n",
    "raw = yf.download(all_tickers, start='2015-01-01', end='2024-12-31',\n",
    "                  auto_adjust=True)\n",
    "close = get_close(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Stationarity Testing Marathon\n",
    "\n",
    "The lecture showed you what stationarity means and ran ADF on SPY prices and returns. Fine — that's one asset, two representations, and zero surprises. Here's the real question: across five very different assets — a broad equity ETF, a treasury bond fund, a gold trust, the most volatile stock in the S&P 500, and a cryptocurrency — and six different representations of each, how many series are actually stationary? And when ADF and KPSS disagree, what does that tell you?\n",
    "\n",
    "For each of **SPY, TLT, GLD, TSLA, and BTC-USD**, compute:\n",
    "\n",
    "1. Raw price\n",
    "2. Simple returns\n",
    "3. Log returns\n",
    "4. Log price\n",
    "5. First difference of log price\n",
    "6. FFD at $d = 0.4$\n",
    "\n",
    "Run ADF and KPSS on each (60 tests total). Build a heatmap where rows are assets, columns are representations, and colors indicate **stationary** (both tests agree), **non-stationary** (both agree), or **conflicting** (tests disagree). The heatmap will reveal the clean dividing line between representations that work and those that don't — and where FFD(0.4) lands relative to returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your workspace** — try it before peeking below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ADF and KPSS on 5 assets x 6 representations\n",
    "# Build a results table and heatmap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ▶ Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffd_weights(d, threshold=1e-5):\n",
    "    \"\"\"Compute FFD weights for fractional differentiation order d.\"\"\"\n",
    "    w = [1.0]\n",
    "    k = 1\n",
    "    while abs(w[-1]) >= threshold:\n",
    "        w.append(-w[-1] * (d - k + 1) / k)\n",
    "        k += 1\n",
    "    return np.array(w[::-1])\n",
    "\n",
    "def frac_diff_ffd(series, d, threshold=1e-5):\n",
    "    \"\"\"Apply FFD fractional differentiation to a pandas Series.\"\"\"\n",
    "    w = ffd_weights(d, threshold)\n",
    "    width = len(w)\n",
    "    out = pd.Series(index=series.index, dtype=float)\n",
    "    for i in range(width - 1, len(series)):\n",
    "        out.iloc[i] = np.dot(w, series.iloc[i - width + 1:i + 1].values)\n",
    "    return out.dropna()\n",
    "\n",
    "def run_adf_kpss(series):\n",
    "    \"\"\"Return (adf_verdict, kpss_verdict) as strings.\"\"\"\n",
    "    s = series.dropna()\n",
    "    if len(s) < 50:\n",
    "        return ('N/A', 'N/A')\n",
    "    adf_p = adfuller(s, autolag='AIC')[1]\n",
    "    kpss_p = kpss(s, regression='c', nlags='auto')[1]\n",
    "    adf_v = 'S' if adf_p < 0.05 else 'NS'\n",
    "    kpss_v = 'S' if kpss_p > 0.05 else 'NS'\n",
    "    return (adf_v, kpss_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The helper functions are set. `run_adf_kpss` returns a tuple of verdicts for each test. Remember the null hypotheses: ADF's null is \"unit root present\" (non-stationary), so rejecting (p < 0.05) means stationary. KPSS flips it — the null is \"series is stationary,\" so *not* rejecting (p > 0.05) means stationary. When both agree, you can be confident. When they disagree, you likely have a trend-stationary process — stationary around a deterministic trend, but not level-stationary. That ambiguity is informative, not a bug.\n",
    "\n",
    "Now let's run the full 60-test marathon and build the heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1_tickers = ['SPY', 'TLT', 'GLD', 'TSLA', 'BTC-USD']\n",
    "rep_names = ['Raw Price', 'Simple Ret', 'Log Ret',\n",
    "             'Log Price', 'Diff Log Price', 'FFD(0.4)']\n",
    "heatmap_data = np.zeros((len(ex1_tickers), len(rep_names)))\n",
    "\n",
    "for i, ticker in enumerate(ex1_tickers):\n",
    "    px = close[ticker].dropna()\n",
    "    log_px = np.log(px)\n",
    "    reps = [\n",
    "        px,                                # Raw Price\n",
    "        px.pct_change().dropna(),           # Simple Return\n",
    "        np.log(px / px.shift(1)).dropna(),  # Log Return\n",
    "        log_px,                             # Log Price\n",
    "        log_px.diff().dropna(),             # First diff of log price\n",
    "        frac_diff_ffd(log_px, 0.4),         # FFD(0.4)\n",
    "    ]\n",
    "    for j, series in enumerate(reps):\n",
    "        adf_v, kpss_v = run_adf_kpss(series)\n",
    "        if adf_v == 'S' and kpss_v == 'S':\n",
    "            heatmap_data[i, j] = 1.0   # Stationary\n",
    "        elif adf_v == 'NS' and kpss_v == 'NS':\n",
    "            heatmap_data[i, j] = -1.0  # Non-stationary\n",
    "        else:\n",
    "            heatmap_data[i, j] = 0.0   # Conflicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the full 5-by-6 matrix of verdicts. The encoding is simple: +1 for stationary (both tests agree), -1 for non-stationary (both agree), and 0 for conflicting. Let's visualize it as a color-coded heatmap so the pattern jumps out immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "cmap = ListedColormap(['#e74c3c', '#f1c40f', '#2ecc71'])  # NS, ?, S\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "im = ax.imshow(heatmap_data, cmap=cmap, vmin=-1, vmax=1, aspect='auto')\n",
    "\n",
    "ax.set_xticks(range(len(rep_names)))\n",
    "ax.set_xticklabels(rep_names, rotation=30, ha='right')\n",
    "ax.set_yticks(range(len(ex1_tickers)))\n",
    "ax.set_yticklabels(ex1_tickers)\n",
    "\n",
    "labels = {1.0: 'S', -1.0: 'NS', 0.0: '?'}\n",
    "for i in range(len(ex1_tickers)):\n",
    "    for j in range(len(rep_names)):\n",
    "        ax.text(j, i, labels[heatmap_data[i, j]],\n",
    "                ha='center', va='center', fontsize=13, fontweight='bold')\n",
    "\n",
    "ax.set_title('Stationarity Verdicts: 5 Assets × 6 Representations',\n",
    "             fontsize=14, fontweight='bold', pad=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap reveals a clean split. The left side — raw prices and log prices — is a wall of red. Non-stationary across every asset, no exceptions. That's the integer differentiation problem in one image: prices carry memory but violate the stationarity assumption every ML model makes.\n",
    "\n",
    "The right side — simple returns, log returns, and first difference of log price — is a wall of green. All three are algebraically close to each other (log returns *are* the first difference of log prices, and they approximate simple returns for small moves), so this is really one result stated three ways: full first-differencing achieves stationarity, reliably, for every asset class from treasuries to crypto.\n",
    "\n",
    "The interesting column is FFD(0.4). It should be green for most assets — fractional differencing at $d = 0.4$ is enough to achieve stationarity while preserving substantially more memory than $d = 1.0$. If any asset shows yellow (conflicting) at $d = 0.4$, it's telling you that asset needs a higher differencing order — its price process carries more non-stationarity than average. BTC-USD, with its extreme trends and regime shifts, is the most likely candidate for this. The variation across that single column previews Exercise 2, where we'll find the *exact* $d^*$ for each asset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Cross-Sectional $d^*$ Map\n",
    "\n",
    "The lecture found $d^*$ for SPY and called it a day. But here's the question nobody asked: does the optimal differencing order vary across asset classes, and can you predict which assets need more differencing from their economic properties?\n",
    "\n",
    "Lopez de Prado showed that $d^*$ is the minimum effective dose — just enough differencing to cure non-stationarity while preserving maximum memory. You're about to find that dose for **15 assets spanning five asset classes**:\n",
    "\n",
    "| Asset Class | Tickers |\n",
    "|---|---|\n",
    "| Equity ETFs | SPY, QQQ, IWM, EEM, XLU |\n",
    "| Bond ETFs | TLT, IEF, HYG |\n",
    "| Commodities | GLD, USO, SLV |\n",
    "| Currency ETFs | UUP, FXE |\n",
    "| Volatility-adjacent | VIXY, SVXY |\n",
    "\n",
    "**For each asset:**\n",
    "1. Grid search $d$ from 0.0 to 1.0 in steps of 0.05.\n",
    "2. Find $d^*$: the minimum $d$ where the ADF test first passes at the 5% level.\n",
    "\n",
    "**Then build two plots:**\n",
    "- A bar chart of $d^*$ by asset, colored by asset class.\n",
    "- A scatter plot of $d^*$ versus trailing 1-year realized volatility.\n",
    "\n",
    "The pattern will reveal something deep: different asset classes cluster at different $d^*$ values, and the reason is tied to how each market carries memory in its prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your workspace** — try it before peeking below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 15 assets with asset class labels\n",
    "# Implement find_d_star(series, d_range, threshold)\n",
    "# Apply to all 15 assets\n",
    "# Plot bar chart of d* colored by asset class\n",
    "# Plot scatter of d* vs trailing 1-year volatility\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ▶ Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_d_star(series, d_range=np.arange(0.0, 1.05, 0.05), threshold=0.05):\n",
    "    \"\"\"Find minimum d where ADF rejects non-stationarity at `threshold`.\"\"\"\n",
    "    log_px = np.log(series.dropna())\n",
    "    for d in d_range:\n",
    "        if d == 0:\n",
    "            fd = log_px\n",
    "        elif d >= 1.0:\n",
    "            fd = log_px.diff().dropna()\n",
    "        else:\n",
    "            fd = frac_diff_ffd(log_px, d)\n",
    "        fd_clean = fd.dropna()\n",
    "        if len(fd_clean) < 50:\n",
    "            continue\n",
    "        adf_p = adfuller(fd_clean, autolag='AIC')[1]\n",
    "        if adf_p < threshold:\n",
    "            corr = fd_clean.corr(log_px.reindex(fd_clean.index))\n",
    "            return d, corr\n",
    "    return 1.0, 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `find_d_star` function walks up the differentiation spectrum from $d = 0$ (raw log prices) to $d = 1$ (returns), stopping at the first $d$ where ADF rejects non-stationarity. It also returns the correlation between the fractionally differentiated series and the original log prices — a direct measure of how much memory survived the transformation. A high correlation at $d^*$ means you kept most of the information while achieving stationarity. A low correlation means you had to destroy a lot of memory to get there.\n",
    "\n",
    "Now let's run this across all 15 assets. This will take a minute — we're running roughly 150 ADF tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_classes = {\n",
    "    'SPY': 'Equity', 'QQQ': 'Equity', 'IWM': 'Equity',\n",
    "    'EEM': 'Equity', 'XLU': 'Equity',\n",
    "    'TLT': 'Bonds', 'IEF': 'Bonds', 'HYG': 'Bonds',\n",
    "    'GLD': 'Commodity', 'USO': 'Commodity', 'SLV': 'Commodity',\n",
    "    'UUP': 'Currency', 'FXE': 'Currency',\n",
    "    'VIXY': 'Volatility', 'SVXY': 'Volatility',\n",
    "}\n",
    "ex2_tickers = list(asset_classes.keys())\n",
    "\n",
    "d_star_results = []\n",
    "for ticker in ex2_tickers:\n",
    "    px = close[ticker].dropna()\n",
    "    if len(px) < 252:\n",
    "        continue\n",
    "    d_star, corr = find_d_star(px)\n",
    "    log_ret = np.log(px / px.shift(1)).dropna()\n",
    "    trailing_vol = log_ret.iloc[-252:].std() * np.sqrt(252)\n",
    "    d_star_results.append({\n",
    "        'Ticker': ticker, 'Class': asset_classes[ticker],\n",
    "        'd*': d_star, 'Corr': round(corr, 3),\n",
    "        'Trail Vol': round(trailing_vol, 4)\n",
    "    })\n",
    "\n",
    "dstar_df = pd.DataFrame(d_star_results).set_index('Ticker')\n",
    "display(dstar_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scan the $d^*$ column. The variation is striking — it's not random noise, it clusters by asset class. Bond ETFs (TLT, IEF) will tend to have lower $d^*$ values, often in the 0.15-0.35 range. Their prices are already close to mean-reverting because they're pulled by yield convergence to par. You need barely any differencing to make them stationary, which means you preserve the vast majority of the trending information in the price series.\n",
    "\n",
    "Equity ETFs cluster in the middle, typically 0.30-0.50. Commodities (especially USO, driven by supply shocks and OPEC decisions) often need more. And the volatility-adjacent products — VIXY and SVXY — may sit at the extremes. VIXY has structural decay built into its DNA (VIX futures are in contango most of the time), which can produce unusual stationarity profiles.\n",
    "\n",
    "The correlation column tells the complementary story. Assets with low $d^*$ retain high correlation with the original series — they kept most of their memory. Assets with $d^*$ near 1.0 have low correlation — fractional differentiation gave them nothing that standard returns wouldn't have provided. The whole point of this exercise is to identify the assets where FFD adds genuine value versus the ones where $d = 1$ was already the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_colors = {\n",
    "    'Equity': '#3498db', 'Bonds': '#e74c3c', 'Commodity': '#f39c12',\n",
    "    'Currency': '#2ecc71', 'Volatility': '#9b59b6'\n",
    "}\n",
    "colors = [class_colors[dstar_df.loc[t, 'Class']] for t in dstar_df.index]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "bars = ax.bar(range(len(dstar_df)), dstar_df['d*'], color=colors, alpha=0.85,\n",
    "              edgecolor='white', linewidth=0.8)\n",
    "ax.set_xticks(range(len(dstar_df)))\n",
    "ax.set_xticklabels(dstar_df.index, rotation=45, ha='right')\n",
    "ax.set_ylabel('Optimal $d^*$', fontsize=12)\n",
    "ax.set_title('Minimum Differencing Order by Asset — Colored by Asset Class',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.axhline(y=1.0, color='gray', linestyle='--', alpha=0.4, label='d=1 (returns)')\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "legend_handles = [Patch(facecolor=c, label=l) for l, c in class_colors.items()]\n",
    "ax.legend(handles=legend_handles, loc='upper right', fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar chart makes the clustering visible. Bonds (red bars) sit low. Equities (blue) occupy the middle ground. Commodities and volatility products tend to sit higher. The gray dashed line at $d = 1$ is the \"returns\" baseline — the line everyone crosses when they call `.pct_change()`. Every bar that falls below that line represents information being unnecessarily destroyed by standard returns. For bonds, the waste is enormous: applying $d = 1$ when $d = 0.2$ would suffice throws away roughly 80% of the memory in the price series.\n",
    "\n",
    "Now let's test whether there's a systematic relationship between an asset's volatility and the amount of differencing it needs. The intuition: more volatile assets have stronger trends (or anti-trends), so they require more aggressive differencing to become stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "for cls, color in class_colors.items():\n",
    "    subset = dstar_df[dstar_df['Class'] == cls]\n",
    "    ax.scatter(subset['Trail Vol'], subset['d*'],\n",
    "              s=120, color=color, label=cls, edgecolors='white',\n",
    "              linewidth=0.8, zorder=3)\n",
    "    for ticker in subset.index:\n",
    "        ax.annotate(ticker, (subset.loc[ticker, 'Trail Vol'],\n",
    "                    subset.loc[ticker, 'd*']),\n",
    "                    fontsize=8, ha='left', va='bottom',\n",
    "                    xytext=(4, 4), textcoords='offset points')\n",
    "\n",
    "ax.set_xlabel('Trailing 1-Year Annualized Volatility', fontsize=12)\n",
    "ax.set_ylabel('Optimal $d^*$', fontsize=12)\n",
    "ax.set_title('$d^*$ vs. Trailing Volatility — Do Volatile Assets Need More Differencing?',\n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot should show a rough positive relationship: higher-volatility assets tend to require higher $d^*$. But look past the overall trend and notice the *clustering*. Bonds cluster in the low-vol, low-$d^*$ corner. Equities form a loose cloud in the middle. Volatility products and commodities, when they have high realized vol, push toward the upper right.\n",
    "\n",
    "This isn't coincidence — it reflects the data-generating process. Bond prices are anchored by yield-to-maturity: if the 10-year Treasury yield is 4%, the price is pulled toward a value consistent with that yield. That anchoring makes bond prices inherently mean-reverting at long horizons, so they're \"almost stationary\" to begin with. You only need a nudge ($d = 0.15$-$0.30$) to finish the job. Equity prices have no such anchor — a stock can double and stay doubled — so they need more aggressive differencing. And volatility products live in their own universe, with structural decay (VIXY) and convexity effects (SVXY) that create unusual stationarity profiles.\n",
    "\n",
    "The practical implication: applying $d = 1$ (standard returns) to everything is wasteful. For bonds, you're throwing away 70-85% of the information to solve a stationarity problem that $d = 0.2$ would fix. For volatile equities, $d = 0.5$ gets you stationarity while preserving the trending information that returns discard. In the homework, you'll build a `FractionalDifferentiator` class that finds $d^*$ per stock automatically and plugs into an sklearn pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: GARCH(1,1) — Fitting and Interpreting\n",
    "\n",
    "Returns themselves are nearly unpredictable — the ACF of daily returns is indistinguishable from noise at every lag. But the *size* of returns is highly predictable: big moves cluster, and GARCH(1,1) captures that clustering with exactly three parameters. It's been doing this since 1986, and it's still the baseline your LSTM will compete against in Week 8.\n",
    "\n",
    "Fit GARCH(1,1) to **SPY, TSLA, JNJ, and GLD**. These are deliberately chosen to represent different volatility regimes: SPY (broad market, moderate vol), TSLA (single stock, extreme vol and reactivity), JNJ (defensive healthcare, low vol), GLD (safe haven, crisis-driven spikes). Compare the estimated $\\alpha$ and $\\beta$ parameters — especially the persistence $\\alpha + \\beta$ — and overlay conditional volatility on realized volatility. The parameters will tell you something real about how each asset processes shocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your workspace** — try it before peeking below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GARCH(1,1) to SPY, TSLA, JNJ, GLD\n",
    "# Compare alpha, beta, persistence\n",
    "# Plot conditional volatility overlays\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ▶ Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garch_tickers = ['SPY', 'TSLA', 'JNJ', 'GLD']\n",
    "garch_fits = {}\n",
    "params_rows = []\n",
    "\n",
    "for ticker in garch_tickers:\n",
    "    px = close[ticker].dropna()\n",
    "    log_ret = np.log(px / px.shift(1)).dropna()\n",
    "    ret_pct = log_ret * 100\n",
    "    gm = arch_model(ret_pct, vol='GARCH', p=1, q=1, mean='Constant')\n",
    "    res = gm.fit(disp='off')\n",
    "    garch_fits[ticker] = {'result': res, 'returns': log_ret}\n",
    "    a = res.params['alpha[1]']\n",
    "    b = res.params['beta[1]']\n",
    "    params_rows.append({\n",
    "        'Ticker': ticker, 'omega': round(res.params['omega'], 6),\n",
    "        'alpha': round(a, 4), 'beta': round(b, 4),\n",
    "        'alpha+beta': round(a + b, 4)\n",
    "    })\n",
    "\n",
    "params_df = pd.DataFrame(params_rows).set_index('Ticker')\n",
    "display(params_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter table tells a story about how different assets metabolize shocks. Look at the $\\alpha$ column — this is the \"reaction\" parameter, measuring how much yesterday's surprise feeds into today's variance estimate. TSLA's $\\alpha$ should be notably higher than SPY's: individual stocks, especially volatile ones, react more violently to news. A single earnings miss can double TSLA's implied volatility overnight. SPY, as a diversified basket of 500 stocks, averages out idiosyncratic shocks and reacts more calmly.\n",
    "\n",
    "Now look at $\\beta$ — the \"persistence\" parameter. SPY's $\\beta$ is likely very high (0.90+), meaning that yesterday's volatility estimate carries over almost unchanged. Volatility in the broad market is incredibly sticky. TSLA's $\\beta$ may be lower, offset by its higher $\\alpha$: shocks matter more, but they also fade faster. JNJ — a defensive healthcare name — should show the lowest persistence overall, because its volatility mean-reverts more quickly. GLD sits somewhere in between, spiking during crises (2020, 2022) but reverting quickly once fear subsides.\n",
    "\n",
    "The persistence $\\alpha + \\beta$ is the number that matters most in practice. When it approaches 1.0, shocks to volatility persist almost indefinitely — the process is close to an integrated GARCH (IGARCH). When it's lower, the market \"forgets\" faster. This single number tells you more about an asset's risk character than a hundred lines of feature engineering. Now let's see the conditional volatility overlays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "for ax, ticker in zip(axes.flat, garch_tickers):\n",
    "    res = garch_fits[ticker]['result']\n",
    "    log_ret = garch_fits[ticker]['returns']\n",
    "    cond_vol = res.conditional_volatility / 100\n",
    "    real_vol = log_ret.rolling(20).std()\n",
    "    ax.plot(real_vol, lw=0.6, alpha=0.5, color='steelblue',\n",
    "            label='Realized (20d)')\n",
    "    ax.plot(cond_vol, lw=0.9, alpha=0.9, color='red',\n",
    "            label='GARCH(1,1)')\n",
    "    p = params_df.loc[ticker, 'alpha+beta']\n",
    "    ax.set_title(f'{ticker}  |  persistence = {p}', fontsize=12)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.set_ylabel('Daily Vol')\n",
    "\n",
    "fig.suptitle('GARCH(1,1) Conditional vs. Realized Volatility',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overlays make three things visible at once. First, GARCH(1,1) tracks realized volatility surprisingly well across all four assets — the red line follows the blue line's major moves. March 2020 shows up as a massive spike in every panel; the model saw the shock and ratcheted its estimate upward immediately. Second, GARCH is *smoother* than realized volatility. It doesn't whipsaw on every noisy day because $\\beta$ acts as an exponential smoother, carrying forward yesterday's estimate. That smoothness is a feature, not a limitation — it means the model's forecasts are more stable and less prone to overreaction.\n",
    "\n",
    "Third, compare the *scale* across panels. TSLA's conditional volatility is dramatically higher than JNJ's — often 2-3x — and it spikes more sharply. GARCH captures this automatically through its parameters: TSLA's higher $\\alpha$ means bigger spikes, while JNJ's lower $\\alpha$ and lower baseline $\\omega$ keep everything compressed. A single model structure with different fitted parameters captures four fundamentally different volatility regimes. That's the power of parsimony — and the baseline your deep learning model will need to beat in Week 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: GARCH Variants — Do They Matter?\n",
    "\n",
    "The lecture made a provocative claim: GARCH(1,1) is the only GARCH variant you need. But that's an empirical question, not a declaration. There are at least 50 GARCH variants in the literature — EGARCH captures asymmetric volatility responses (bad news increases vol more than good news), GJR-GARCH models the leverage effect through an indicator variable. Both have theoretical appeal. The question is whether that appeal translates into meaningfully better fits on real data.\n",
    "\n",
    "Fit **GARCH(1,1), EGARCH(1,1), and GJR-GARCH(1,1)** to both **SPY** and **TSLA**. Compare log-likelihood and AIC/BIC. Plot conditional volatility from each model on the same axes. The answer to \"do variants matter?\" will depend on which asset you're looking at — and that's the interesting part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your workspace** — try it before peeking below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GARCH(1,1), EGARCH(1,1), GJR-GARCH(1,1) to SPY and TSLA\n",
    "# Compare log-likelihood, AIC, BIC\n",
    "# Plot conditional volatility overlays\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ▶ Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_specs = {\n",
    "    'GARCH(1,1)':     {'vol': 'GARCH',  'p': 1, 'q': 1, 'o': 0},\n",
    "    'EGARCH(1,1)':    {'vol': 'EGARCH', 'p': 1, 'q': 1, 'o': 1},\n",
    "    'GJR-GARCH(1,1)': {'vol': 'GARCH',  'p': 1, 'q': 1, 'o': 1},\n",
    "}\n",
    "var_tickers = ['SPY', 'TSLA']\n",
    "variant_results = {}\n",
    "comparison_rows = []\n",
    "\n",
    "for ticker in var_tickers:\n",
    "    ret_pct = garch_fits[ticker]['result'].model.y\n",
    "    variant_results[ticker] = {}\n",
    "    for name, spec in variant_specs.items():\n",
    "        gm = arch_model(ret_pct, vol=spec['vol'], p=spec['p'],\n",
    "                        q=spec['q'], o=spec['o'], mean='Constant')\n",
    "        res = gm.fit(disp='off')\n",
    "        variant_results[ticker][name] = res\n",
    "        comparison_rows.append({\n",
    "            'Ticker': ticker, 'Model': name,\n",
    "            'LogLik': round(res.loglikelihood, 1),\n",
    "            'AIC': round(res.aic, 1),\n",
    "            'BIC': round(res.bic, 1),\n",
    "            'Params': res.num_params,\n",
    "        })\n",
    "\n",
    "comp_df = pd.DataFrame(comparison_rows)\n",
    "display(comp_df.set_index(['Ticker', 'Model']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the AIC and BIC columns. For SPY, EGARCH and GJR-GARCH should show modestly lower (better) AIC values than standard GARCH(1,1) — the leverage effect is real for broad equity indices, and these models capture it. After a bad day for the S&P 500, volatility increases more than after an equally large *good* day. That asymmetry is well-documented in the options market (it's why put options are more expensive than calls at the same distance from the strike), and the asymmetric GARCH models formalize it.\n",
    "\n",
    "For TSLA, the picture may be different. Individual stocks — especially volatile, sentiment-driven names — don't always show a clean leverage effect. TSLA can spike in volatility on *good* news (a strong earnings beat can trigger a short squeeze and a volatility explosion). The asymmetric models may or may not improve on GARCH(1,1) here, and that's the honest answer: the leverage effect is an equity-index phenomenon more than a single-stock phenomenon.\n",
    "\n",
    "Now let's overlay the conditional volatility from all three models to see how much the *actual forecasts* differ in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "model_colors = {'GARCH(1,1)': '#e74c3c',\n",
    "                'EGARCH(1,1)': '#3498db',\n",
    "                'GJR-GARCH(1,1)': '#2ecc71'}\n",
    "\n",
    "for ax, ticker in zip(axes, var_tickers):\n",
    "    for name, color in model_colors.items():\n",
    "        cv = variant_results[ticker][name].conditional_volatility / 100\n",
    "        ax.plot(cv, lw=0.9, alpha=0.8, color=color, label=name)\n",
    "    ax.set_title(f'{ticker}: Conditional Volatility by Model',\n",
    "                 fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.set_ylabel('Daily Vol')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visual makes the practical point that the AIC table hinted at. The three lines track each other *remarkably* closely. During calm periods, they're nearly indistinguishable. The differences emerge only during extreme events — sharp market selloffs where the asymmetric models react more aggressively because they've learned that negative returns produce disproportionately large volatility increases.\n",
    "\n",
    "GJR-GARCH's leverage effect is real — negative returns do increase volatility more than positive returns of the same magnitude. But the out-of-sample forecasting improvement is marginal, typically within 1-3% of GARCH(1,1). In production, simplicity has value: GARCH(1,1) is easier to fit, more numerically stable (EGARCH can sometimes fail to converge on short or noisy samples), and less likely to overfit. Unless you're specifically trading the leverage effect — say, in an options strategy that depends on precise skew modeling — the added complexity of variants rarely pays for itself.\n",
    "\n",
    "This is the first instance of a pattern we'll see repeatedly in this course: simple models refuse to die gracefully. Week 5 (trees vs. linear models), Week 8 (GARCH vs. LSTM), Week 9 (XGBoost vs. foundation models) — the lesson is always the same. Complexity needs to earn its keep, and the bar is higher than most papers admit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Four exercises, four results that should recalibrate your instincts about time series modeling:\n",
    "\n",
    "- **Stationarity has a clean dividing line.** Raw prices and log prices are non-stationary for every asset, every time. Returns, log returns, and first differences of log prices are stationary for every asset, every time. FFD at $d = 0.4$ achieves stationarity for most assets while preserving substantially more memory than returns — but the \"most\" is doing real work, because some assets need higher $d$. That variation motivated Exercise 2.\n",
    "\n",
    "- **The optimal $d^*$ is asset-class-specific, not universal.** Bonds need barely any differencing ($d^* \\approx 0.15$-$0.30$) because yield convergence makes their prices inherently mean-reverting. Equities need more ($d^* \\approx 0.30$-$0.50$). Volatile commodities and volatility products may need $d^*$ near 0.60 or higher. Applying $d = 1$ to everything is the financial equivalent of resizing all images to 32x32 regardless of content — technically valid, massively wasteful.\n",
    "\n",
    "- **GARCH(1,1) tells you more about an asset's risk character than a hundred features.** SPY has high persistence ($\\alpha + \\beta \\approx 0.98$) — shocks reverberate for months. TSLA is more reactive ($\\alpha$ higher) but less persistent. JNJ is the calmest of the group. These parameters map directly to how each asset processes information and absorbs surprises.\n",
    "\n",
    "- **Fancier GARCH variants barely beat the original.** EGARCH and GJR-GARCH capture the leverage effect — real, documented, theoretically motivated — but the out-of-sample improvement is marginal. Parsimony wins.\n",
    "\n",
    "In the homework, you'll scale these ideas to 50 stocks. You'll build a `FractionalDifferentiator` class that finds $d^*$ per stock and plugs into an sklearn pipeline. You'll fit GARCH(1,1) to the full universe and discover that persistence varies by sector in a pattern that's economically meaningful. And you'll test whether fractionally differentiated features actually improve prediction accuracy over standard returns — the central claim of Lopez de Prado's Chapter 5, put to a proper empirical test."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}