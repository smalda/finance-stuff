{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9 Seminar -- Foundation Models for Financial Time Series\n",
    "\n",
    "**Duration:** 90 minutes\n",
    "\n",
    "| Exercise | Topic | Time |\n",
    "|----------|-------|------|\n",
    "| 1 | Chronos zero-shot on SPY | 25 min |\n",
    "| 2 | Fine-tune a small model on financial data | 25 min |\n",
    "| 3 | Hybrid: FM embeddings + XGBoost | 20 min |\n",
    "| 4 | Discussion: Will FMs replace trees? | 20 min |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "\n",
    "print('Imports ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared data loading\n",
    "try:\n",
    "    import yfinance as yf\n",
    "    spy = yf.download('SPY', start='2020-01-01', end='2024-12-31', progress=False)\n",
    "    spy_close = spy['Close'].dropna()\n",
    "    spy_returns = spy_close.pct_change().dropna()\n",
    "    print(f'Loaded {len(spy_close)} days of SPY data.')\n",
    "except Exception as e:\n",
    "    print(f'yfinance unavailable ({e}). Generating synthetic data.')\n",
    "    np.random.seed(42)\n",
    "    dates = pd.bdate_range('2020-01-01', '2024-12-31')\n",
    "    returns = np.random.randn(len(dates)) * 0.012 + 0.0003\n",
    "    spy_close = pd.Series(np.exp(np.cumsum(returns)) * 320, index=dates, name='Close')\n",
    "    spy_returns = spy_close.pct_change().dropna()\n",
    "    print(f'Generated {len(spy_close)} synthetic trading days.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 1: Chronos Zero-Shot on SPY (25 min)\n",
    "\n",
    "**Goal:** Install Chronos, run zero-shot forecasting on SPY, and evaluate the results.\n",
    "\n",
    "**Setup:**\n",
    "```bash\n",
    "pip install chronos-forecasting torch\n",
    "```\n",
    "\n",
    "### Tasks\n",
    "1. Load the last 252 trading days as context\n",
    "2. Forecast the next 21 trading days (one month)\n",
    "3. Compare against a naive \"last value\" baseline\n",
    "4. Compute RMSE and directional accuracy\n",
    "5. Plot the forecast with confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1-2: Run Chronos zero-shot\n",
    "import torch\n",
    "\n",
    "context_length = 252  # 1 year\n",
    "forecast_horizon = 21  # 1 month\n",
    "\n",
    "# Split: use last forecast_horizon days as test\n",
    "context_data = spy_close.iloc[-(context_length + forecast_horizon):-forecast_horizon]\n",
    "test_data = spy_close.iloc[-forecast_horizon:]\n",
    "\n",
    "print(f'Context: {context_data.index[0].date()} to {context_data.index[-1].date()} ({len(context_data)} days)')\n",
    "print(f'Test:    {test_data.index[0].date()} to {test_data.index[-1].date()} ({len(test_data)} days)')\n",
    "\n",
    "try:\n",
    "    from chronos import ChronosPipeline\n",
    "\n",
    "    pipeline = ChronosPipeline.from_pretrained(\n",
    "        'amazon/chronos-t5-tiny',\n",
    "        device_map='cpu',\n",
    "        torch_dtype=torch.float32,\n",
    "    )\n",
    "\n",
    "    context_tensor = torch.tensor(context_data.values, dtype=torch.float32).unsqueeze(0)\n",
    "    samples = pipeline.predict(context_tensor, prediction_length=forecast_horizon, num_samples=50)\n",
    "\n",
    "    # Extract median, 10th, 90th percentiles\n",
    "    chronos_median = samples.median(dim=1).values.squeeze().numpy()\n",
    "    chronos_p10 = samples.quantile(0.1, dim=1).squeeze().numpy()\n",
    "    chronos_p90 = samples.quantile(0.9, dim=1).squeeze().numpy()\n",
    "    chronos_available = True\n",
    "    print('Chronos forecast ready.')\n",
    "\n",
    "except ImportError:\n",
    "    print('Chronos not installed. Using simulated forecast for demonstration.')\n",
    "    np.random.seed(77)\n",
    "    last_val = context_data.values[-1]\n",
    "    drift = np.linspace(0, 3, forecast_horizon)\n",
    "    noise = np.cumsum(np.random.randn(forecast_horizon) * 0.7)\n",
    "    chronos_median = last_val + drift + noise\n",
    "    chronos_p10 = chronos_median - 5\n",
    "    chronos_p90 = chronos_median + 5\n",
    "    chronos_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Naive baseline (last value repeated)\n",
    "naive_forecast = np.full(forecast_horizon, context_data.values[-1])\n",
    "\n",
    "# Task 4: Evaluate\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "actual = test_data.values\n",
    "\n",
    "rmse_chronos = np.sqrt(mean_squared_error(actual, chronos_median))\n",
    "rmse_naive = np.sqrt(mean_squared_error(actual, naive_forecast))\n",
    "\n",
    "dir_actual = np.sign(np.diff(actual))\n",
    "dir_chronos = np.sign(np.diff(chronos_median))\n",
    "dir_naive = np.sign(np.diff(naive_forecast))  # always 0 for naive\n",
    "\n",
    "dir_acc_chronos = np.mean(dir_actual == dir_chronos)\n",
    "dir_acc_naive = 0.0  # naive predicts no change\n",
    "\n",
    "print(f'{\"Metric\":<20} {\"Chronos\":>10} {\"Naive\":>10}')\n",
    "print('-' * 42)\n",
    "print(f'{\"RMSE\":<20} {rmse_chronos:>10.2f} {rmse_naive:>10.2f}')\n",
    "print(f'{\"Dir. Accuracy\":<20} {dir_acc_chronos:>10.1%} {dir_acc_naive:>10.1%}')\n",
    "print()\n",
    "if rmse_chronos > rmse_naive:\n",
    "    print('Chronos has HIGHER RMSE than naive -- typical for zero-shot on finance.')\n",
    "else:\n",
    "    print('Chronos beats naive on RMSE -- unusual, worth investigating further.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Plot with confidence intervals\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "# Context tail\n",
    "tail_len = 60\n",
    "ax.plot(context_data.index[-tail_len:], context_data.values[-tail_len:],\n",
    "        color='black', linewidth=1.5, label='Context (historical)')\n",
    "\n",
    "# Actual test\n",
    "ax.plot(test_data.index, actual, color='black', linewidth=1.5, linestyle='-',\n",
    "        marker='o', markersize=3, label='Actual')\n",
    "\n",
    "# Chronos forecast\n",
    "ax.plot(test_data.index, chronos_median, color='indianred', linestyle='--',\n",
    "        marker='s', markersize=3, label='Chronos (median)')\n",
    "ax.fill_between(test_data.index, chronos_p10, chronos_p90, alpha=0.2,\n",
    "                color='indianred', label='Chronos (80% CI)')\n",
    "\n",
    "# Naive\n",
    "ax.axhline(y=naive_forecast[0], color='steelblue', linestyle=':', label='Naive (last value)')\n",
    "\n",
    "ax.axvline(x=test_data.index[0], color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_title('Exercise 1: Chronos Zero-Shot Forecast on SPY', fontweight='bold')\n",
    "ax.set_ylabel('Price')\n",
    "ax.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 -- Discussion Points\n",
    "\n",
    "- Does Chronos capture the trend? The volatility?\n",
    "- How wide are the confidence intervals relative to actual moves?\n",
    "- Would this be tradeable? What Sharpe could you achieve?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Fine-Tuning a Small Model on Financial Data (25 min)\n",
    "\n",
    "**Goal:** Fine-tune the Chronos-tiny model on financial return data and see if it improves.\n",
    "\n",
    "> **Note:** Full fine-tuning of TimesFM (200M params) requires a GPU. Here we work with Chronos-tiny (8M) which fits on a CPU/M4 MacBook. If compute is limited, we walk through the setup and discuss results.\n",
    "\n",
    "### Approach\n",
    "1. Prepare multiple stock return series as training data\n",
    "2. Fine-tune Chronos-tiny on these series for a few epochs\n",
    "3. Evaluate fine-tuned model vs. zero-shot on held-out data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare financial training data: multiple tickers\n",
    "tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META', 'NVDA', 'JPM', 'BAC', 'XOM', 'JNJ']\n",
    "\n",
    "try:\n",
    "    import yfinance as yf\n",
    "    price_data = yf.download(tickers, start='2018-01-01', end='2024-06-30', progress=False)['Close']\n",
    "    price_data = price_data.dropna()\n",
    "    print(f'Downloaded data for {len(tickers)} tickers, {len(price_data)} days each.')\n",
    "except Exception:\n",
    "    np.random.seed(42)\n",
    "    dates = pd.bdate_range('2018-01-01', '2024-06-30')\n",
    "    price_data = pd.DataFrame(\n",
    "        {t: np.exp(np.cumsum(np.random.randn(len(dates)) * 0.015 + 0.0003)) * (50 + i * 20)\n",
    "         for i, t in enumerate(tickers)},\n",
    "        index=dates\n",
    "    )\n",
    "    print(f'Generated synthetic data for {len(tickers)} tickers.')\n",
    "\n",
    "print(f'Shape: {price_data.shape}')\n",
    "price_data.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning setup for Chronos\n",
    "# Note: This requires the chronos-forecasting package with training support.\n",
    "# The training API uses GluonTS data format.\n",
    "\n",
    "print('=== Chronos Fine-Tuning Setup ===')\n",
    "print()\n",
    "print('Step 1: Prepare data in GluonTS format')\n",
    "print('Step 2: Configure training (learning rate, epochs, batch size)')\n",
    "print('Step 3: Run fine-tuning loop')\n",
    "print('Step 4: Evaluate on held-out test set')\n",
    "print()\n",
    "\n",
    "# Prepare data as list of series (GluonTS-compatible format)\n",
    "train_series = []\n",
    "test_series = []\n",
    "test_horizon = 21\n",
    "\n",
    "for ticker in tickers:\n",
    "    series = price_data[ticker].values\n",
    "    train_series.append(series[:-test_horizon])\n",
    "    test_series.append(series[-test_horizon:])\n",
    "\n",
    "print(f'Prepared {len(train_series)} training series.')\n",
    "print(f'Each training series: ~{len(train_series[0])} time steps.')\n",
    "print(f'Test horizon: {test_horizon} steps.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning demonstration\n",
    "# In practice, you would run:\n",
    "#\n",
    "#   from chronos.training import train\n",
    "#   train(\n",
    "#       model_id='amazon/chronos-t5-tiny',\n",
    "#       training_data=train_dataset,\n",
    "#       output_dir='./chronos-finetuned',\n",
    "#       learning_rate=1e-4,\n",
    "#       num_epochs=5,\n",
    "#       per_device_train_batch_size=8,\n",
    "#   )\n",
    "#\n",
    "# For this seminar, we simulate the fine-tuning improvement.\n",
    "\n",
    "print('Fine-tuning code (for reference):')\n",
    "print()\n",
    "print('from chronos.training import train')\n",
    "print()\n",
    "print('train(')\n",
    "print('    model_id=\"amazon/chronos-t5-tiny\",')\n",
    "print('    training_data=train_dataset,')\n",
    "print('    output_dir=\"./chronos-finetuned\",')\n",
    "print('    learning_rate=1e-4,')\n",
    "print('    num_epochs=5,')\n",
    "print('    per_device_train_batch_size=8,')\n",
    "print(')')\n",
    "print()\n",
    "print('Typical fine-tuning time on M4 MacBook: ~15-30 min for tiny model.')\n",
    "print('Expected improvement: 10-30% RMSE reduction vs. zero-shot.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated comparison: zero-shot vs fine-tuned vs ARIMA\n",
    "np.random.seed(42)\n",
    "\n",
    "results = []\n",
    "for i, ticker in enumerate(tickers):\n",
    "    actual = test_series[i]\n",
    "    # Simulated RMSE values (based on typical literature results)\n",
    "    base_rmse = np.std(np.diff(actual)) * np.sqrt(test_horizon) * 0.5\n",
    "    results.append({\n",
    "        'Ticker': ticker,\n",
    "        'ARIMA RMSE': base_rmse * (0.9 + np.random.rand() * 0.3),\n",
    "        'Chronos ZS RMSE': base_rmse * (1.0 + np.random.rand() * 0.4),\n",
    "        'Chronos FT RMSE': base_rmse * (0.85 + np.random.rand() * 0.25),\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results).set_index('Ticker')\n",
    "print('Simulated RMSE Comparison (illustrative):')\n",
    "print(results_df.round(2).to_string())\n",
    "print()\n",
    "print('Average improvement from fine-tuning: '\n",
    "      f'{(1 - results_df[\"Chronos FT RMSE\"].mean() / results_df[\"Chronos ZS RMSE\"].mean()):.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "x = np.arange(len(tickers))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(x - width, results_df['ARIMA RMSE'], width, label='ARIMA', color='steelblue')\n",
    "ax.bar(x, results_df['Chronos ZS RMSE'], width, label='Chronos (zero-shot)', color='indianred')\n",
    "ax.bar(x + width, results_df['Chronos FT RMSE'], width, label='Chronos (fine-tuned)', color='green')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(tickers)\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_title('Exercise 2: Zero-Shot vs. Fine-Tuned Chronos (Simulated)', fontweight='bold')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 -- Key Takeaways\n",
    "\n",
    "- Fine-tuning helps, but does not close the gap to finance-native models\n",
    "- The tokenizer is still calibrated for generic distributions\n",
    "- With limited financial data, overfitting is a real risk\n",
    "- Consider: is the improvement worth the compute cost vs. just using ARIMA?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Hybrid Approach -- FM Embeddings + XGBoost (20 min)\n",
    "\n",
    "**Goal:** Extract embeddings from a foundation model and use them as features for XGBoost.\n",
    "\n",
    "**Key insight:** Even if the FM is bad at *forecasting* financial data, its internal representations might capture useful temporal patterns that XGBoost can exploit.\n",
    "\n",
    "### Pipeline\n",
    "```\n",
    "Price window --> FM encoder --> embedding vector (256-dim)\n",
    "                                      |\n",
    "Price window --> hand-crafted features (momentum, vol, etc.)\n",
    "                                      |\n",
    "                                      v\n",
    "                              Concatenate --> XGBoost --> return prediction\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create hand-crafted features\n",
    "\n",
    "def make_features(prices, lookback=20):\n",
    "    \"\"\"Create standard alpha features from price series.\"\"\"\n",
    "    df = pd.DataFrame({'close': prices})\n",
    "    df['ret_1d'] = df['close'].pct_change(1)\n",
    "    df['ret_5d'] = df['close'].pct_change(5)\n",
    "    df['ret_20d'] = df['close'].pct_change(20)\n",
    "    df['vol_20d'] = df['ret_1d'].rolling(20).std()\n",
    "    df['vol_60d'] = df['ret_1d'].rolling(60).std()\n",
    "    df['mom_12_1'] = df['close'].pct_change(252) - df['close'].pct_change(21)\n",
    "    df['sma_ratio'] = df['close'] / df['close'].rolling(50).mean()\n",
    "    df['high_20d'] = df['close'].rolling(20).max() / df['close'] - 1\n",
    "    df['low_20d'] = df['close'] / df['close'].rolling(20).min() - 1\n",
    "    return df.dropna()\n",
    "\n",
    "features_df = make_features(spy_close)\n",
    "print(f'Created {features_df.shape[1] - 1} hand-crafted features.')\n",
    "print(f'Features: {[c for c in features_df.columns if c != \"close\"]}')\n",
    "features_df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Generate FM embeddings\n",
    "# In practice: pass rolling windows through the FM encoder and extract hidden states.\n",
    "# Here we simulate this with a learned representation.\n",
    "\n",
    "def extract_fm_embeddings(prices, window=60, embedding_dim=32):\n",
    "    \"\"\"\n",
    "    Simulate FM embedding extraction.\n",
    "    \n",
    "    In practice, you would:\n",
    "      1. Take a rolling window of prices\n",
    "      2. Pass through the FM encoder (e.g., Chronos encoder)\n",
    "      3. Extract the last hidden state or pooled representation\n",
    "    \n",
    "    Real code (with Chronos):\n",
    "        model = ChronosModel.from_pretrained('amazon/chronos-t5-tiny')\n",
    "        for i in range(window, len(prices)):\n",
    "            context = prices[i-window:i]\n",
    "            with torch.no_grad():\n",
    "                hidden = model.encoder(tokenize(context))\n",
    "                embedding = hidden.last_hidden_state.mean(dim=1)  # pool\n",
    "            embeddings.append(embedding)\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    n = len(prices) - window\n",
    "    \n",
    "    # Simulate: embeddings that capture some temporal structure\n",
    "    returns = np.diff(np.log(prices))\n",
    "    embeddings = np.zeros((n, embedding_dim))\n",
    "    \n",
    "    for i in range(n):\n",
    "        window_returns = returns[i:i + window]\n",
    "        # First few dims: statistical summaries (what an FM might learn)\n",
    "        embeddings[i, 0] = np.mean(window_returns)\n",
    "        embeddings[i, 1] = np.std(window_returns)\n",
    "        embeddings[i, 2] = np.mean(window_returns[-5:])\n",
    "        embeddings[i, 3] = np.mean(window_returns[-20:])\n",
    "        # Remaining dims: nonlinear features (what deep learning captures)\n",
    "        for j in range(4, embedding_dim):\n",
    "            embeddings[i, j] = np.tanh(\n",
    "                np.dot(window_returns[::window // (j + 1)][:5],\n",
    "                       np.random.randn(5) * 0.1)\n",
    "            )\n",
    "    \n",
    "    idx = prices.index[window:] if hasattr(prices, 'index') else range(window, len(prices))\n",
    "    cols = [f'emb_{i}' for i in range(embedding_dim)]\n",
    "    return pd.DataFrame(embeddings, index=idx, columns=cols)\n",
    "\n",
    "emb_df = extract_fm_embeddings(spy_close, window=60, embedding_dim=32)\n",
    "print(f'Generated embeddings: {emb_df.shape}')\n",
    "emb_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Combine features and train XGBoost\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Align features and embeddings by date\n",
    "feature_cols = [c for c in features_df.columns if c != 'close']\n",
    "combined = features_df[feature_cols].join(emb_df, how='inner')\n",
    "\n",
    "# Target: next-day return\n",
    "combined['target'] = spy_close.pct_change().shift(-1)\n",
    "combined = combined.dropna()\n",
    "\n",
    "# Train/test split (time-based)\n",
    "split_idx = int(len(combined) * 0.8)\n",
    "X_train = combined.iloc[:split_idx].drop(columns='target')\n",
    "y_train = combined.iloc[:split_idx]['target']\n",
    "X_test = combined.iloc[split_idx:].drop(columns='target')\n",
    "y_test = combined.iloc[split_idx:]['target']\n",
    "\n",
    "print(f'Train: {len(X_train)} samples, Test: {len(X_test)} samples')\n",
    "print(f'Total features: {X_train.shape[1]} ({len(feature_cols)} hand-crafted + {emb_df.shape[1]} embeddings)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train three models: hand-crafted only, embeddings only, combined\n",
    "\n",
    "xgb_params = dict(\n",
    "    n_estimators=200, max_depth=4, learning_rate=0.05,\n",
    "    subsample=0.8, colsample_bytree=0.8, random_state=42\n",
    ")\n",
    "\n",
    "emb_cols = [c for c in emb_df.columns]\n",
    "\n",
    "models = {\n",
    "    'Hand-crafted only': (feature_cols, XGBRegressor(**xgb_params)),\n",
    "    'FM embeddings only': (emb_cols, XGBRegressor(**xgb_params)),\n",
    "    'Combined (hybrid)': (feature_cols + emb_cols, XGBRegressor(**xgb_params)),\n",
    "}\n",
    "\n",
    "results = []\n",
    "predictions = {}\n",
    "\n",
    "for name, (cols, model) in models.items():\n",
    "    usable_cols = [c for c in cols if c in X_train.columns]\n",
    "    model.fit(X_train[usable_cols], y_train)\n",
    "    y_pred = model.predict(X_test[usable_cols])\n",
    "    predictions[name] = y_pred\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    ic = np.corrcoef(y_test, y_pred)[0, 1]\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    dir_acc = np.mean(np.sign(y_test) == np.sign(y_pred))\n",
    "    \n",
    "    results.append({'Model': name, 'RMSE': rmse, 'IC': ic, 'R2': r2, 'Dir Acc': dir_acc})\n",
    "    print(f'{name}: IC={ic:.4f}, R2={r2:.4f}, Dir Acc={dir_acc:.1%}')\n",
    "\n",
    "results_df = pd.DataFrame(results).set_index('Model')\n",
    "print()\n",
    "print(results_df.round(4).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# IC comparison\n",
    "colors = ['steelblue', 'indianred', 'green']\n",
    "axes[0].bar(results_df.index, results_df['IC'], color=colors, edgecolor='white')\n",
    "axes[0].set_ylabel('Information Coefficient')\n",
    "axes[0].set_title('IC Comparison')\n",
    "axes[0].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# Feature importance for hybrid model\n",
    "hybrid_model = models['Combined (hybrid)'][1]\n",
    "all_cols = feature_cols + emb_cols\n",
    "usable = [c for c in all_cols if c in X_train.columns]\n",
    "importances = hybrid_model.feature_importances_\n",
    "\n",
    "# Aggregate: hand-crafted vs. embedding importance\n",
    "hc_imp = sum(importances[i] for i, c in enumerate(usable) if c in feature_cols)\n",
    "emb_imp = sum(importances[i] for i, c in enumerate(usable) if c in emb_cols)\n",
    "\n",
    "axes[1].pie([hc_imp, emb_imp], labels=['Hand-crafted', 'FM Embeddings'],\n",
    "            colors=['steelblue', 'indianred'], autopct='%1.0f%%', startangle=90)\n",
    "axes[1].set_title('Feature Importance Split (Hybrid Model)')\n",
    "\n",
    "plt.suptitle('Exercise 3: Hybrid FM Embeddings + XGBoost', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 -- Key Takeaways\n",
    "\n",
    "- FM embeddings add incremental value on top of hand-crafted features\n",
    "- The improvement is typically modest (a few basis points of IC)\n",
    "- The hybrid approach is more robust than using FMs alone for forecasting\n",
    "- XGBoost effectively selects which embedding dimensions are useful\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Discussion -- Will FMs Replace Trees for Quant Finance? (20 min)\n",
    "\n",
    "### Discussion Questions\n",
    "\n",
    "1. **Data efficiency:** Trees work well with 1,000-5,000 samples. FMs need millions for pre-training. Which is more practical for a single stock?\n",
    "\n",
    "2. **Interpretability:** Regulators require model explainability (SR 11-7, EU AI Act). Can you explain what a 256-dim embedding captures? How does this compare to SHAP values on a tree?\n",
    "\n",
    "3. **Signal decay:** Alpha signals in finance decay quickly (months, not years). If you fine-tune an FM, how fast does it go stale?\n",
    "\n",
    "4. **Competitive dynamics:** If everyone uses the same open-source FM (Chronos, Kronos), does the alpha disappear? Is the data moat more important than the model?\n",
    "\n",
    "5. **The hybrid question:** If the best approach is FM embeddings + XGBoost, does that mean trees are still doing the heavy lifting?\n",
    "\n",
    "### Positions to consider\n",
    "\n",
    "**Pro-FM camp:**\n",
    "- FMs capture complex nonlinear patterns that hand-crafted features miss\n",
    "- Transfer learning from vast data compensates for small financial datasets\n",
    "- Kronos shows that finance-native FMs can dramatically outperform trees\n",
    "\n",
    "**Pro-tree camp:**\n",
    "- Trees + good features have been the workhorse of quant for a decade\n",
    "- Explainability is non-negotiable for institutional investors\n",
    "- The marginal improvement from FMs does not justify the complexity\n",
    "- Data (alternative data, faster data) matters more than model architecture\n",
    "\n",
    "**Pragmatist camp:**\n",
    "- Use FMs as feature extractors, trees as decision-makers\n",
    "- The pipeline matters more than any single component\n",
    "- Invest in data infrastructure, not model architecture"
   ]
  }
 ]
}