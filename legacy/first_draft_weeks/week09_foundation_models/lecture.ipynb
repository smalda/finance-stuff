{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9 -- Foundation Models for Financial Time Series\n",
    "\n",
    "**Key question:** Can we take a model pre-trained on millions of generic time series and use it to forecast stock prices?\n",
    "\n",
    "**Spoiler:** It is not that simple. This lecture covers the three-layer reality of foundation models (FMs) in finance.\n",
    "\n",
    "---\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. The hype vs. reality of foundation models for finance\n",
    "2. Generic TSFMs: Chronos (Amazon), TimesFM (Google)\n",
    "3. Why generic TSFMs underperform in finance\n",
    "4. Finance-native FMs: Kronos, FinCast\n",
    "5. The hybrid approach: FM embeddings + XGBoost\n",
    "6. Bank AI investment context\n",
    "7. Demo: Chronos zero-shot on SPY vs. ARIMA\n",
    "8. Key papers and references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "\n",
    "print('Imports ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. The Hype vs. Reality\n",
    "\n",
    "**The promise:** Train a single model on millions of time series, then zero-shot forecast *anything* -- energy, weather, retail, finance.\n",
    "\n",
    "**The reality in finance:**\n",
    "\n",
    "| Layer | Approach | Finance Performance |\n",
    "|-------|----------|--------------------|\n",
    "| Layer 1 | Generic TSFMs (Chronos, TimesFM) zero-shot | Poor -- often worse than ARIMA |\n",
    "| Layer 2 | Finance-native FMs (Kronos, FinCast) | Strong -- trained on financial data |\n",
    "| Layer 3 | Hybrid: FM embeddings --> XGBoost | Potentially best of both worlds |\n",
    "\n",
    "**Why?** Financial time series have fundamentally different statistical properties than the data these models were trained on:\n",
    "- Near-zero autocorrelation in returns\n",
    "- Heavy tails / non-Gaussian distributions\n",
    "- Regime changes\n",
    "- Signal-to-noise ratio is extremely low\n",
    "- Non-stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Generic TSFMs: Chronos and TimesFM\n",
    "\n",
    "### Chronos (Amazon, 2024)\n",
    "\n",
    "- **Paper:** Ansari et al., \"Chronos: Learning the Language of Time Series\" (2024)\n",
    "- **Idea:** Tokenize time series values (quantile binning), then train a T5-style language model on them\n",
    "- **Architecture:** Encoder-decoder transformer (T5)\n",
    "- **Training data:** 27 publicly available datasets (energy, traffic, weather, economics) + synthetic data via Gaussian processes\n",
    "- **Sizes:** 8M, 46M, 200M, 710M parameters\n",
    "- **Key innovation:** Treats forecasting as a language modeling problem -- each real-valued time step becomes a token\n",
    "\n",
    "```\n",
    "Raw values: [100.5, 101.2, 99.8, 102.1, ...]\n",
    "     --> Scaling (mean/std)\n",
    "     --> Quantile binning into 4096 bins\n",
    "     --> Token IDs: [2048, 2103, 1995, 2150, ...]\n",
    "     --> Feed into T5 transformer\n",
    "     --> Output: probability distribution over next tokens\n",
    "     --> Decode back to real values\n",
    "```\n",
    "\n",
    "### TimesFM (Google, 2024)\n",
    "\n",
    "- **Paper:** Das et al., \"A decoder-only foundation model for time-series forecasting\" (2024)\n",
    "- **Idea:** Decoder-only transformer (like GPT) for time series\n",
    "- **Architecture:** Patched decoder-only transformer with input/output projection layers\n",
    "- **Training data:** ~100B time points from Google Trends, Wiki pageviews, synthetic data\n",
    "- **Size:** 200M parameters\n",
    "- **Key innovation:** Patch-based input (groups of consecutive time steps), handles variable context/horizon lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: How Chronos tokenization works\n",
    "\n",
    "np.random.seed(42)\n",
    "raw_values = np.cumsum(np.random.randn(100) * 0.02) + 100  # simulated price\n",
    "\n",
    "# Step 1: Normalize\n",
    "mean_val, std_val = raw_values.mean(), raw_values.std()\n",
    "normalized = (raw_values - mean_val) / std_val\n",
    "\n",
    "# Step 2: Quantile bin (simplified -- 16 bins for illustration)\n",
    "n_bins = 16\n",
    "bin_edges = np.linspace(normalized.min() - 0.1, normalized.max() + 0.1, n_bins + 1)\n",
    "token_ids = np.digitize(normalized, bin_edges) - 1\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(raw_values, color='steelblue')\n",
    "axes[0].set_title('Step 1: Raw Time Series')\n",
    "axes[0].set_ylabel('Price')\n",
    "\n",
    "axes[1].plot(normalized, color='darkorange')\n",
    "axes[1].set_title('Step 2: Normalized')\n",
    "axes[1].set_ylabel('z-score')\n",
    "\n",
    "axes[2].step(range(len(token_ids)), token_ids, color='green', where='mid')\n",
    "axes[2].set_title(f'Step 3: Tokenized ({n_bins} bins)')\n",
    "axes[2].set_ylabel('Token ID')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('Time step')\n",
    "\n",
    "plt.suptitle('Chronos Tokenization Pipeline (Simplified)', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Why Generic TSFMs Underperform in Finance\n",
    "\n",
    "Generic TSFMs were trained on datasets where:\n",
    "- **Trends are persistent:** electricity demand follows daily/seasonal cycles\n",
    "- **Signal-to-noise is high:** temperature tomorrow is predictable from temperature today\n",
    "- **Distributions are well-behaved:** approximately Gaussian\n",
    "\n",
    "Financial returns are the *opposite*:\n",
    "\n",
    "| Property | Generic Time Series | Financial Returns |\n",
    "|----------|-------------------|-----------------|\n",
    "| Autocorrelation | High (seasonal patterns) | Near zero |\n",
    "| Distribution | Near-Gaussian | Heavy tails (kurtosis ~10+) |\n",
    "| Stationarity | Often stationary after differencing | Regime changes, structural breaks |\n",
    "| Predictability | High (R-squared ~0.9+) | Very low (R-squared ~0.01 is good) |\n",
    "| Noise level | Low to moderate | Extremely high |\n",
    "\n",
    "**The tokenization problem:** Chronos uses fixed quantile bins calibrated on its training distribution. Financial return distributions have much fatter tails, so the tokenizer either clips extreme values or wastes resolution on the center of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrate the distributional mismatch\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generic time series: near-Gaussian noise around a trend\n",
    "generic_residuals = np.random.randn(5000)\n",
    "\n",
    "# Financial returns: heavy-tailed (Student's t with df=3)\n",
    "financial_returns = np.random.standard_t(df=3, size=5000) * 0.01\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].hist(generic_residuals, bins=80, density=True, alpha=0.7, color='steelblue', edgecolor='white')\n",
    "axes[0].set_title('Generic Time Series Residuals\\n(Gaussian, kurtosis ~ 3)')\n",
    "axes[0].set_xlim(-6, 6)\n",
    "from scipy import stats\n",
    "axes[0].annotate(f'Kurtosis: {stats.kurtosis(generic_residuals, fisher=False):.1f}', xy=(0.7, 0.9),\n",
    "                 xycoords='axes fraction', fontsize=11)\n",
    "\n",
    "axes[1].hist(financial_returns, bins=80, density=True, alpha=0.7, color='indianred', edgecolor='white')\n",
    "axes[1].set_title('Financial Returns\\n(Heavy-tailed, kurtosis >> 3)')\n",
    "axes[1].set_xlim(-0.06, 0.06)\n",
    "axes[1].annotate(f'Kurtosis: {stats.kurtosis(financial_returns, fisher=False):.1f}', xy=(0.7, 0.9),\n",
    "                 xycoords='axes fraction', fontsize=11)\n",
    "\n",
    "plt.suptitle('The Distributional Mismatch Problem', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Finance-Native Foundation Models\n",
    "\n",
    "### Kronos (AAAI 2026)\n",
    "\n",
    "- **Paper:** Li et al., \"Kronos: A Foundation Model for Stock Market Prediction\" (AAAI 2026)\n",
    "- **Key innovation -- K-line tokenization:** Instead of tokenizing raw prices, Kronos tokenizes *candlestick (K-line) patterns* -- (open, high, low, close, volume) tuples\n",
    "- This preserves the intra-period price dynamics that matter for trading\n",
    "- **Model sizes:** 4.1M, 21M, 102M parameters (runnable on M4 MacBook)\n",
    "- **License:** MIT -- fully open source\n",
    "- **Results:** 93% improvement in RankIC over generic TSFMs, competitive with task-specific models\n",
    "\n",
    "```\n",
    "Traditional tokenization:  Close price --> single token\n",
    "K-line tokenization:       (O, H, L, C, V) --> structured token\n",
    "                           Preserves price dynamics within each bar\n",
    "```\n",
    "\n",
    "### FinCast (CIKM 2025)\n",
    "\n",
    "- **Paper:** \"FinCast: A Large-scale Financial Forecasting Foundation Model\" (CIKM 2025)\n",
    "- **Scale:** 1B parameters, trained on diverse financial data\n",
    "- **Approach:** Multi-task pre-training on returns, volatility, and cross-sectional rankings\n",
    "- **Key insight:** Financial forecasting requires understanding *relative* movements (cross-sectional), not just individual time series\n",
    "\n",
    "### Why finance-native FMs work better\n",
    "\n",
    "1. **Domain-specific tokenization:** K-line tokens capture OHLCV structure\n",
    "2. **Financial training data:** Pre-trained on actual market data\n",
    "3. **Appropriate loss functions:** RankIC, portfolio-aware metrics\n",
    "4. **Cross-sectional awareness:** Understanding relative stock movements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize K-line tokenization vs. traditional tokenization\n",
    "\n",
    "np.random.seed(123)\n",
    "n_bars = 20\n",
    "\n",
    "# Generate synthetic OHLCV data\n",
    "closes = np.cumsum(np.random.randn(n_bars) * 0.5) + 100\n",
    "opens = closes + np.random.randn(n_bars) * 0.3\n",
    "highs = np.maximum(opens, closes) + np.abs(np.random.randn(n_bars) * 0.2)\n",
    "lows = np.minimum(opens, closes) - np.abs(np.random.randn(n_bars) * 0.2)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Traditional: just close prices\n",
    "axes[0].plot(closes, 'o-', color='steelblue', markersize=5)\n",
    "axes[0].set_title('Traditional Tokenization\\nOnly close prices --> one token per step')\n",
    "axes[0].set_ylabel('Price')\n",
    "axes[0].set_xlabel('Time step')\n",
    "\n",
    "# K-line: candlestick chart\n",
    "for i in range(n_bars):\n",
    "    color = 'green' if closes[i] >= opens[i] else 'red'\n",
    "    axes[1].plot([i, i], [lows[i], highs[i]], color='black', linewidth=0.8)\n",
    "    axes[1].plot([i, i], [opens[i], closes[i]], color=color, linewidth=4, solid_capstyle='butt')\n",
    "\n",
    "axes[1].set_title('K-line Tokenization (Kronos)\\n(O, H, L, C, V) --> structured token per step')\n",
    "axes[1].set_ylabel('Price')\n",
    "axes[1].set_xlabel('Time step')\n",
    "\n",
    "plt.suptitle('Why K-line Tokenization Preserves More Information', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. The Hybrid Approach: FM Embeddings + XGBoost\n",
    "\n",
    "**The idea:** Use a foundation model as a *feature extractor*, not as a direct forecaster.\n",
    "\n",
    "```\n",
    "Raw price data --> Foundation Model --> Internal embeddings (e.g., 256-dim vector)\n",
    "                                               |\n",
    "                                               v\n",
    "                                    Concatenate with hand-crafted features\n",
    "                                               |\n",
    "                                               v\n",
    "                                          XGBoost / LightGBM\n",
    "                                               |\n",
    "                                               v\n",
    "                                        Return prediction\n",
    "```\n",
    "\n",
    "**Why this can work better than either approach alone:**\n",
    "\n",
    "1. **FM embeddings** capture complex temporal patterns the model learned from pre-training\n",
    "2. **XGBoost** handles the tabular-data aspects that trees excel at (feature interactions, non-linearities)\n",
    "3. **Hand-crafted features** encode domain knowledge (momentum, volatility, etc.)\n",
    "4. **Regularization** is easier to control with XGBoost than with fine-tuning a large FM\n",
    "\n",
    "**Practical implementation:**\n",
    "- Run a forward pass through the FM (e.g., Chronos encoder)\n",
    "- Extract the last hidden state or pool across time steps\n",
    "- Concatenate with your standard alpha features\n",
    "- Train XGBoost on the combined feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schematic of the three approaches\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Simulated performance comparison\n",
    "approaches = ['Generic TSFM\\n(Chronos zero-shot)', 'Finance-Native FM\\n(Kronos)', 'Hybrid\\n(FM emb + XGBoost)']\n",
    "rank_ic = [0.02, 0.065, 0.072]\n",
    "colors = ['#e74c3c', '#27ae60', '#2980b9']\n",
    "\n",
    "axes[0].bar(approaches, rank_ic, color=colors, edgecolor='white', width=0.6)\n",
    "axes[0].set_ylabel('Rank IC')\n",
    "axes[0].set_title('Forecasting Quality (Rank IC)')\n",
    "axes[0].axhline(y=0.03, color='gray', linestyle='--', label='ARIMA baseline')\n",
    "axes[0].legend()\n",
    "\n",
    "r_squared = [0.001, 0.008, 0.010]\n",
    "axes[1].bar(approaches, r_squared, color=colors, edgecolor='white', width=0.6)\n",
    "axes[1].set_ylabel('OOS R-squared')\n",
    "axes[1].set_title('Out-of-Sample R-squared')\n",
    "\n",
    "sharpe = [0.3, 1.1, 1.3]\n",
    "axes[2].bar(approaches, sharpe, color=colors, edgecolor='white', width=0.6)\n",
    "axes[2].set_ylabel('Sharpe Ratio')\n",
    "axes[2].set_title('Portfolio Sharpe (long-short top/bottom quintile)')\n",
    "\n",
    "plt.suptitle('The Three-Layer Reality: Stylized Performance Comparison', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Note: These are stylized/illustrative numbers based on reported results in the literature.')\n",
    "print('Actual performance varies significantly by universe, frequency, and implementation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Bank AI Investment Context\n",
    "\n",
    "Banks have invested **$35B+** in AI, but the allocation tells the story:\n",
    "\n",
    "| Application | % of AI spend | Maturity |\n",
    "|------------|--------------|----------|\n",
    "| Fraud detection | ~25% | Production |\n",
    "| Risk management / compliance | ~25% | Production |\n",
    "| Customer service / chatbots | ~20% | Production |\n",
    "| Operations / document processing | ~20% | Production |\n",
    "| **Alpha / signal generation** | **~10%** | **Experimental** |\n",
    "\n",
    "**~90% of bank AI spending is operational, not alpha-generating.**\n",
    "\n",
    "Why?\n",
    "- Operational AI has clear ROI (reduce fraud losses, cut headcount)\n",
    "- Alpha generation is zero-sum: if everyone has the same FM, the alpha disappears\n",
    "- Regulatory constraints: models must be explainable (FMs are black boxes)\n",
    "- Data moats matter more than model architecture in finance\n",
    "\n",
    "**Implication for FMs:** Foundation models are most likely to succeed in finance as *components* of a pipeline (feature extractors, embedding generators) rather than as end-to-end predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Demo: Chronos Zero-Shot on SPY vs. ARIMA\n",
    "\n",
    "We will now run a concrete comparison:\n",
    "1. Download SPY daily close prices\n",
    "2. Use Chronos (tiny, 8M params) for zero-shot forecasting\n",
    "3. Compare against a simple ARIMA baseline\n",
    "4. Evaluate on RMSE and directional accuracy\n",
    "\n",
    "**Installation:**\n",
    "```bash\n",
    "pip install chronos-forecasting yfinance statsmodels\n",
    "```\n",
    "\n",
    "> **Note:** If Chronos is not installed, we provide a fallback using cached predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download SPY data\n",
    "try:\n",
    "    import yfinance as yf\n",
    "    spy = yf.download('SPY', start='2023-01-01', end='2024-12-31', progress=False)\n",
    "    spy_close = spy['Close'].dropna()\n",
    "    print(f'Downloaded {len(spy_close)} days of SPY data.')\n",
    "except Exception as e:\n",
    "    print(f'yfinance not available ({e}), using synthetic data.')\n",
    "    np.random.seed(42)\n",
    "    dates = pd.bdate_range('2023-01-01', '2024-12-31')\n",
    "    returns = np.random.randn(len(dates)) * 0.01 + 0.0003\n",
    "    spy_close = pd.Series(np.exp(np.cumsum(returns)) * 380, index=dates, name='Close')\n",
    "    print(f'Generated {len(spy_close)} days of synthetic SPY data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train / test\n",
    "train_size = int(len(spy_close) * 0.8)\n",
    "train = spy_close.iloc[:train_size]\n",
    "test = spy_close.iloc[train_size:]\n",
    "forecast_horizon = len(test)\n",
    "\n",
    "print(f'Train: {len(train)} days ({train.index[0].date()} to {train.index[-1].date()})')\n",
    "print(f'Test:  {len(test)} days ({test.index[0].date()} to {test.index[-1].date()})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA baseline\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Fit ARIMA(1,1,1) on training data\n",
    "arima_model = ARIMA(train.values, order=(1, 1, 1))\n",
    "arima_fit = arima_model.fit()\n",
    "\n",
    "# Forecast\n",
    "arima_forecast = arima_fit.forecast(steps=forecast_horizon)\n",
    "arima_forecast = pd.Series(arima_forecast, index=test.index)\n",
    "\n",
    "print('ARIMA(1,1,1) fitted and forecast generated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chronos zero-shot forecast\n",
    "import torch\n",
    "\n",
    "chronos_available = False\n",
    "try:\n",
    "    from chronos import ChronosPipeline\n",
    "\n",
    "    pipeline = ChronosPipeline.from_pretrained(\n",
    "        'amazon/chronos-t5-tiny',  # 8M params -- fast\n",
    "        device_map='cpu',\n",
    "        torch_dtype=torch.float32,\n",
    "    )\n",
    "\n",
    "    context = torch.tensor(train.values, dtype=torch.float32).unsqueeze(0)\n",
    "    chronos_pred = pipeline.predict(\n",
    "        context,\n",
    "        prediction_length=forecast_horizon,\n",
    "        num_samples=20,  # probabilistic forecast\n",
    "    )\n",
    "    # Take median of samples\n",
    "    chronos_forecast = chronos_pred.median(dim=1).values.squeeze().numpy()\n",
    "    chronos_forecast = pd.Series(chronos_forecast, index=test.index)\n",
    "    chronos_available = True\n",
    "    print('Chronos forecast generated successfully.')\n",
    "\n",
    "except ImportError:\n",
    "    print('Chronos not installed. Using simulated Chronos predictions.')\n",
    "    print('Install with: pip install chronos-forecasting')\n",
    "    print()\n",
    "    # Simulated Chronos: slightly worse than ARIMA (typical zero-shot behavior)\n",
    "    np.random.seed(99)\n",
    "    last_price = train.values[-1]\n",
    "    chronos_noise = np.cumsum(np.random.randn(forecast_horizon) * 0.8)\n",
    "    chronos_forecast = pd.Series(\n",
    "        last_price + np.linspace(0, 5, forecast_horizon) + chronos_noise,\n",
    "        index=test.index\n",
    "    )\n",
    "    print('Simulated Chronos forecast generated (to illustrate typical behavior).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and compare\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def evaluate_forecast(actual, predicted, name):\n",
    "    rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    # Directional accuracy\n",
    "    actual_dir = np.sign(np.diff(actual.values))\n",
    "    pred_dir = np.sign(np.diff(predicted.values))\n",
    "    dir_acc = np.mean(actual_dir == pred_dir)\n",
    "    return {'Model': name, 'RMSE': rmse, 'MAE': mae, 'Dir. Accuracy': dir_acc}\n",
    "\n",
    "results = [\n",
    "    evaluate_forecast(test, arima_forecast, 'ARIMA(1,1,1)'),\n",
    "    evaluate_forecast(test, chronos_forecast, 'Chronos (zero-shot)'),\n",
    "]\n",
    "\n",
    "results_df = pd.DataFrame(results).set_index('Model')\n",
    "print(results_df.round(4).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Full forecast\n",
    "axes[0].plot(test.index, test.values, label='Actual SPY', color='black', linewidth=1.5)\n",
    "axes[0].plot(test.index, arima_forecast.values, label='ARIMA', color='steelblue', linestyle='--')\n",
    "axes[0].plot(test.index, chronos_forecast.values, label='Chronos (zero-shot)', color='indianred', linestyle='--')\n",
    "axes[0].set_title('SPY Forecast Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].set_ylabel('Price')\n",
    "\n",
    "# Zoomed first 30 days\n",
    "n_zoom = min(30, len(test))\n",
    "axes[1].plot(test.index[:n_zoom], test.values[:n_zoom], label='Actual', color='black', linewidth=1.5)\n",
    "axes[1].plot(test.index[:n_zoom], arima_forecast.values[:n_zoom], label='ARIMA', color='steelblue',\n",
    "             linestyle='--', marker='o', markersize=3)\n",
    "axes[1].plot(test.index[:n_zoom], chronos_forecast.values[:n_zoom], label='Chronos', color='indianred',\n",
    "             linestyle='--', marker='s', markersize=3)\n",
    "axes[1].set_title(f'First {n_zoom} Days (Zoomed)')\n",
    "axes[1].legend()\n",
    "axes[1].set_ylabel('Price')\n",
    "\n",
    "plt.suptitle('Chronos Zero-Shot vs. ARIMA on SPY', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why does Chronos underperform on SPY?\n",
    "\n",
    "1. **Training distribution mismatch:** Chronos was trained on energy, traffic, weather -- not financial data\n",
    "2. **Tokenization resolution:** 4096 bins calibrated on generic data waste precision on the narrow return distribution\n",
    "3. **No cross-sectional information:** Chronos sees only one time series at a time\n",
    "4. **Long-horizon drift:** Zero-shot forecasts tend to revert to the training distribution's mean behavior\n",
    "5. **No market microstructure:** The model has no concept of trading days, earnings, or regime changes\n",
    "\n",
    "**This is not a failure of foundation models -- it is a failure of applying *generic* foundation models to a *specialized* domain without adaptation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Key Papers and References\n",
    "\n",
    "### Generic Time Series Foundation Models\n",
    "- Ansari et al., \"Chronos: Learning the Language of Time Series\" (Amazon, 2024). [arXiv:2403.07815](https://arxiv.org/abs/2403.07815)\n",
    "- Das et al., \"A decoder-only foundation model for time-series forecasting\" (Google, 2024). [arXiv:2310.10688](https://arxiv.org/abs/2310.10688)\n",
    "\n",
    "### Finance-Native Foundation Models\n",
    "- Li et al., \"Kronos: A Foundation Model for Stock Market Prediction\" (AAAI 2026). K-line tokenization, 93% RankIC improvement, MIT-licensed, 4.1M--102M params.\n",
    "- \"FinCast: A Large-scale Financial Forecasting Foundation Model\" (CIKM 2025). 1B params, multi-task pre-training.\n",
    "\n",
    "### Surveys and Context\n",
    "- Liang et al., \"Foundation Models for Time Series Analysis: A Tutorial and Survey\" (KDD 2024)\n",
    "- Bank AI spending analysis: Evident AI Index (2024), McKinsey Global AI Survey (2024)\n",
    "\n",
    "### Practical Notes\n",
    "- **Kronos** is MIT-licensed and fits on an M4 MacBook (4.1M smallest, 102M largest)\n",
    "- **Chronos** is available via `pip install chronos-forecasting` with models on HuggingFace\n",
    "- **TimesFM** is available via `pip install timesfm` from Google\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "1. Generic TSFMs (Chronos, TimesFM) are impressive engineering but perform poorly on financial data zero-shot\n",
    "2. Finance-native FMs (Kronos, FinCast) incorporate domain-specific tokenization and training data -- much better results\n",
    "3. The hybrid approach (FM embeddings fed into XGBoost) may be the most practical path forward\n",
    "4. Most bank AI investment is operational, not alpha-generating -- FMs for alpha are still experimental\n",
    "5. The key is not whether FMs \"work\" but *how you use them* in the pipeline"
   ]
  }
 ]
}