{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 — Homework Solution: Cross-Sectional Alpha Model v2 (Trees)\n",
    "\n",
    "**Course:** ML for Quantitative Finance  \n",
    "**Status:** SOLUTION — do not distribute to students before deadline\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy import stats\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import shap\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Extended Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same 200 tickers as Week 4 (abbreviated here for readability)\n",
    "TICKERS = [\n",
    "    'AAPL', 'MSFT', 'AMZN', 'GOOGL', 'META', 'NVDA', 'JPM', 'JNJ', 'V', 'PG',\n",
    "    'UNH', 'HD', 'MA', 'DIS', 'BAC', 'XOM', 'CSCO', 'PFE', 'COST', 'ABT',\n",
    "    'PEP', 'AVGO', 'CRM', 'NKE', 'CVX', 'WMT', 'MRK', 'LLY', 'ABBV', 'INTC',\n",
    "    'T', 'VZ', 'QCOM', 'TXN', 'PM', 'UNP', 'NEE', 'LOW', 'BMY', 'AMGN',\n",
    "    'MDT', 'HON', 'SBUX', 'GS', 'MS', 'BLK', 'GILD', 'MMC', 'ADP', 'AMT',\n",
    "    'CME', 'CI', 'LRCX', 'MO', 'MDLZ', 'SO', 'DUK', 'CL', 'ZTS', 'BDX',\n",
    "    'REGN', 'ITW', 'APD', 'SHW', 'FISV', 'NOC', 'ICE', 'CSX', 'WM', 'FDX',\n",
    "    'EMR', 'PNC', 'USB', 'NSC', 'CCI', 'D', 'GM', 'F', 'TGT', 'AEP',\n",
    "]\n",
    "\n",
    "cache_path = Path('w5_data_cache.pkl')\n",
    "if cache_path.exists():\n",
    "    raw = pd.read_pickle(cache_path)\n",
    "else:\n",
    "    raw = yf.download(TICKERS, start='2010-01-01', end='2024-12-31', progress=True)\n",
    "    raw.to_pickle(cache_path)\n",
    "\n",
    "prices = raw['Close'].ffill()\n",
    "volume = raw['Volume'].ffill()\n",
    "returns_daily = prices.pct_change()\n",
    "monthly_prices = prices.resample('M').last()\n",
    "monthly_returns = monthly_prices.pct_change()\n",
    "\n",
    "# Drop tickers with too much missing\n",
    "good = prices.isnull().mean() < 0.2\n",
    "prices = prices.loc[:, good]\n",
    "volume = volume.loc[:, prices.columns]\n",
    "\n",
    "print(f\"Universe: {prices.shape[1]} stocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature computation (same as Week 4)\n",
    "features = {}\n",
    "features['mom_1m'] = monthly_prices.pct_change(1)\n",
    "features['mom_3m'] = monthly_prices.pct_change(3)\n",
    "features['mom_6m'] = monthly_prices.pct_change(6)\n",
    "features['mom_12m_skip1'] = monthly_prices.pct_change(12).shift(1)\n",
    "features['reversal_1m'] = -monthly_prices.pct_change(1)\n",
    "features['vol_20d'] = returns_daily.rolling(20).std().resample('M').last()\n",
    "features['vol_60d'] = returns_daily.rolling(60).std().resample('M').last()\n",
    "features['vol_ratio'] = (volume.rolling(5).mean() / volume.rolling(60).mean()).resample('M').last()\n",
    "features['ma_50_ratio'] = (prices / prices.rolling(50).mean()).resample('M').last()\n",
    "features['ma_200_ratio'] = (prices / prices.rolling(200).mean()).resample('M').last()\n",
    "\n",
    "target = monthly_returns.shift(-1)\n",
    "\n",
    "# Build panel\n",
    "months = sorted(set.intersection(*[set(f.index) for f in features.values()]))\n",
    "months = [m for m in months if pd.Timestamp('2012-01-01') <= m <= pd.Timestamp('2024-06-30')]\n",
    "\n",
    "X_all, y_all, dates_all = [], [], []\n",
    "for month in months:\n",
    "    X_cs = pd.DataFrame({name: feat.loc[month] for name, feat in features.items() if month in feat.index})\n",
    "    y_cs = target.loc[month] if month in target.index else pd.Series(dtype=float)\n",
    "    valid = X_cs.dropna().index.intersection(y_cs.dropna().index)\n",
    "    if len(valid) > 5:\n",
    "        X_all.append(X_cs.loc[valid])\n",
    "        y_all.append(y_cs.loc[valid])\n",
    "        dates_all.extend([month] * len(valid))\n",
    "\n",
    "X_panel = pd.concat(X_all)\n",
    "y_panel = pd.concat(y_all)\n",
    "dates_panel = np.array(dates_all)\n",
    "\n",
    "print(f\"Panel: {len(X_panel)} obs, {X_panel.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Optuna Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_objective(model_type='xgb'):\n",
    "    def objective(trial):\n",
    "        if model_type == 'xgb':\n",
    "            params = {\n",
    "                'max_depth': trial.suggest_int('max_depth', 2, 6),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 0.9),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.9),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10, log=True),\n",
    "            }\n",
    "            model_cls = lambda: xgb.XGBRegressor(**params, verbosity=0)\n",
    "        else:\n",
    "            params = {\n",
    "                'max_depth': trial.suggest_int('max_depth', 2, 6),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 0.9),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.9),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10, log=True),\n",
    "            }\n",
    "            model_cls = lambda: lgb.LGBMRegressor(**params, verbosity=-1)\n",
    "\n",
    "        splits = [(pd.Timestamp('2016-01-31'), pd.Timestamp('2018-01-31')),\n",
    "                  (pd.Timestamp('2018-01-31'), pd.Timestamp('2020-01-31')),\n",
    "                  (pd.Timestamp('2020-01-31'), pd.Timestamp('2022-01-31'))]\n",
    "\n",
    "        ic_scores = []\n",
    "        for split, test_end in splits:\n",
    "            tr_mask = dates_panel < split\n",
    "            te_mask = (dates_panel >= split) & (dates_panel < test_end)\n",
    "            if te_mask.sum() < 10:\n",
    "                continue\n",
    "            model = model_cls()\n",
    "            model.fit(X_panel.values[tr_mask], y_panel.values[tr_mask])\n",
    "            pred = model.predict(X_panel.values[te_mask])\n",
    "            test_dates = dates_panel[te_mask]\n",
    "            for m in np.unique(test_dates):\n",
    "                m_mask = test_dates == m\n",
    "                if m_mask.sum() > 5:\n",
    "                    ic = stats.spearmanr(pred[m_mask], y_panel.values[te_mask][m_mask])[0]\n",
    "                    ic_scores.append(ic)\n",
    "        return np.mean(ic_scores) if ic_scores else 0\n",
    "    return objective\n",
    "\n",
    "# Tune XGBoost\n",
    "study_xgb = optuna.create_study(direction='maximize')\n",
    "study_xgb.optimize(make_objective('xgb'), n_trials=30)\n",
    "print(f\"XGBoost best IC: {study_xgb.best_value:.4f}\")\n",
    "\n",
    "# Tune LightGBM\n",
    "study_lgb = optuna.create_study(direction='maximize')\n",
    "study_lgb.optimize(make_objective('lgb'), n_trials=30)\n",
    "print(f\"LightGBM best IC: {study_lgb.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Full Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all models with expanding window\n",
    "pred_start = pd.Timestamp('2018-01-31')\n",
    "pred_months = [m for m in months if m >= pred_start]\n",
    "\n",
    "all_models = {\n",
    "    'OLS': lambda: LinearRegression(),\n",
    "    'Ridge': lambda: Ridge(alpha=1.0),\n",
    "    'RF': lambda: RandomForestRegressor(n_estimators=100, max_depth=5, n_jobs=-1),\n",
    "    'XGBoost': lambda: xgb.XGBRegressor(**study_xgb.best_params, verbosity=0),\n",
    "    'LightGBM': lambda: lgb.LGBMRegressor(**study_lgb.best_params, verbosity=-1),\n",
    "}\n",
    "\n",
    "results = {name: [] for name in all_models}\n",
    "predictions_store = {name: {} for name in all_models}\n",
    "\n",
    "for month in pred_months:\n",
    "    train_mask = dates_panel < month\n",
    "    test_mask = dates_panel == month\n",
    "    if test_mask.sum() < 5 or train_mask.sum() < 100:\n",
    "        continue\n",
    "\n",
    "    X_tr, y_tr = X_panel.values[train_mask], y_panel.values[train_mask]\n",
    "    X_te, y_te = X_panel.values[test_mask], y_panel.values[test_mask]\n",
    "\n",
    "    for name, model_fn in all_models.items():\n",
    "        model = model_fn()\n",
    "        model.fit(X_tr, y_tr)\n",
    "        pred = model.predict(X_te)\n",
    "        ic = stats.spearmanr(pred, y_te)[0]\n",
    "        results[name].append({'month': month, 'IC': ic})\n",
    "        predictions_store[name][month] = pred\n",
    "\n",
    "# Comparison table\n",
    "comparison = []\n",
    "for name in all_models:\n",
    "    ic_vals = [x['IC'] for x in results[name]]\n",
    "    comparison.append({\n",
    "        'Model': name,\n",
    "        'Avg IC': f\"{np.mean(ic_vals):.4f}\",\n",
    "        'IC t-stat': f\"{np.mean(ic_vals)/np.std(ic_vals)*np.sqrt(len(ic_vals)):.2f}\",\n",
    "        'IC>0 %': f\"{np.mean([x>0 for x in ic_vals]):.0%}\",\n",
    "    })\n",
    "\n",
    "pd.DataFrame(comparison).set_index('Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final XGBoost model for SHAP\n",
    "xgb_final = xgb.XGBRegressor(**study_xgb.best_params, verbosity=0)\n",
    "xgb_final.fit(X_panel.values, y_panel.values)\n",
    "\n",
    "explainer = shap.TreeExplainer(xgb_final)\n",
    "# Use a subsample for speed\n",
    "sample_idx = np.random.choice(len(X_panel), min(1000, len(X_panel)), replace=False)\n",
    "shap_values = explainer.shap_values(X_panel.values[sample_idx])\n",
    "\n",
    "# Summary plot\n",
    "shap.summary_plot(shap_values, X_panel.iloc[sample_idx], show=False)\n",
    "plt.title('SHAP Feature Importance (XGBoost)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance bar chart\n",
    "mean_shap = np.abs(shap_values).mean(axis=0)\n",
    "feat_importance = pd.Series(mean_shap, index=X_panel.columns).sort_values(ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "feat_importance.plot(kind='barh', ax=ax, color='steelblue')\n",
    "ax.set_title('Mean |SHAP| by Feature')\n",
    "ax.set_xlabel('Mean |SHAP value|')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Model Combination (Stretch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average predictions from Ridge, XGBoost, LightGBM\n",
    "combo_ics = []\n",
    "combo_models = ['Ridge', 'XGBoost', 'LightGBM']\n",
    "\n",
    "for month in pred_months:\n",
    "    preds = []\n",
    "    for name in combo_models:\n",
    "        if month in predictions_store[name]:\n",
    "            preds.append(predictions_store[name][month])\n",
    "    if len(preds) != len(combo_models):\n",
    "        continue\n",
    "\n",
    "    avg_pred = np.mean(preds, axis=0)\n",
    "    test_mask = dates_panel == month\n",
    "    y_te = y_panel.values[test_mask]\n",
    "    if len(avg_pred) == len(y_te) and len(y_te) > 5:\n",
    "        ic = stats.spearmanr(avg_pred, y_te)[0]\n",
    "        combo_ics.append(ic)\n",
    "\n",
    "print(f\"Ensemble (Ridge + XGB + LGBM average):\")\n",
    "print(f\"  Avg IC: {np.mean(combo_ics):.4f}\")\n",
    "print(f\"  IC t-stat: {np.mean(combo_ics)/np.std(combo_ics)*np.sqrt(len(combo_ics)):.2f}\")\n",
    "print(f\"\\nConclusion: Ensembles often beat individual models because they average out\")\n",
    "print(f\"model-specific errors while preserving shared signal.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}