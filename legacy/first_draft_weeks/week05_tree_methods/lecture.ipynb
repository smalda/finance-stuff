{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 — Tree-Based Methods & Gradient Boosting\n",
    "\n",
    "**Course:** ML for Quantitative Finance  \n",
    "**Type:** Lecture (90 min)\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "Gradient boosted trees (XGBoost, LightGBM) are the **single most used ML model class**  \n",
    "in production quant finance. They dominate Kaggle finance competitions.  \n",
    "If you learn only one ML method for finance, learn this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from scipy import stats\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Trees → Random Forests → Gradient Boosting\n",
    "\n",
    "**Decision Tree:** Recursive binary splits. Greedy, interpretable, high variance.\n",
    "\n",
    "**Random Forest:** Bag many trees (bootstrap + feature subsampling). Reduces variance.\n",
    "\n",
    "**Gradient Boosting:** Build trees sequentially, each correcting the previous one's errors.\n",
    "\n",
    "### Why Trees Dominate Tabular Financial Data\n",
    "- Handle missing values natively (no imputation needed)\n",
    "- Capture nonlinear interactions (momentum × volatility)\n",
    "- No feature scaling required\n",
    "- Built-in feature importance\n",
    "- Regularization through depth limits, subsampling\n",
    "- Fast to train and predict\n",
    "\n",
    "### XGBoost vs. LightGBM vs. CatBoost\n",
    "\n",
    "| | XGBoost | LightGBM | CatBoost |\n",
    "|---|---------|----------|----------|\n",
    "| Tree growth | Level-wise | Leaf-wise (faster) | Symmetric |\n",
    "| Speed | Fast | Fastest | Moderate |\n",
    "| Categorical | Needs encoding | Native support | Best native support |\n",
    "| Missing values | Native | Native | Native |\n",
    "| When to use | Default choice | Large datasets | Heavy categorical data |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse feature pipeline from Week 4 (simplified)\n",
    "tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'JPM', 'JNJ', 'XOM', 'PG', 'V', 'UNH',\n",
    "           'HD', 'MA', 'PFE', 'COST', 'NKE', 'BAC', 'NVDA', 'META', 'LLY', 'ABBV']\n",
    "\n",
    "data = yf.download(tickers, start='2010-01-01', end='2024-12-31', progress=False)\n",
    "prices = data['Close'].ffill().dropna()\n",
    "volume = data['Volume'].ffill().dropna()\n",
    "returns_daily = prices.pct_change()\n",
    "\n",
    "# Monthly\n",
    "monthly_prices = prices.resample('M').last()\n",
    "monthly_returns = monthly_prices.pct_change()\n",
    "\n",
    "# Features\n",
    "features = {}\n",
    "features['mom_1m'] = monthly_prices.pct_change(1)\n",
    "features['mom_3m'] = monthly_prices.pct_change(3)\n",
    "features['mom_12m_skip1'] = monthly_prices.pct_change(12).shift(1)\n",
    "features['vol_20d'] = returns_daily.rolling(20).std().resample('M').last()\n",
    "features['vol_60d'] = returns_daily.rolling(60).std().resample('M').last()\n",
    "features['vol_ratio'] = (volume.rolling(5).mean() / volume.rolling(60).mean()).resample('M').last()\n",
    "features['ma_ratio'] = (prices / prices.rolling(50).mean()).resample('M').last()\n",
    "\n",
    "target = monthly_returns.shift(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. XGBoost for Cross-Sectional Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build panel data\n",
    "months = sorted(set.intersection(*[set(f.index) for f in features.values()]))\n",
    "months = [m for m in months if m >= pd.Timestamp('2012-01-01') and m <= pd.Timestamp('2024-06-30')]\n",
    "\n",
    "X_all, y_all, dates_all = [], [], []\n",
    "for month in months:\n",
    "    X_cs = pd.DataFrame({name: feat.loc[month] for name, feat in features.items()})\n",
    "    y_cs = target.loc[month] if month in target.index else pd.Series(dtype=float)\n",
    "    valid = X_cs.dropna().index.intersection(y_cs.dropna().index)\n",
    "    if len(valid) > 5:\n",
    "        X_all.append(X_cs.loc[valid])\n",
    "        y_all.append(y_cs.loc[valid])\n",
    "        dates_all.extend([month] * len(valid))\n",
    "\n",
    "X_panel = pd.concat(X_all)\n",
    "y_panel = pd.concat(y_all)\n",
    "dates_panel = np.array(dates_all)\n",
    "\n",
    "print(f\"Panel: {len(X_panel)} observations, {X_panel.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost with expanding window\n",
    "pred_start = pd.Timestamp('2018-01-31')\n",
    "ics = {'XGBoost': [], 'LightGBM': [], 'RandomForest': []}\n",
    "\n",
    "for month in months:\n",
    "    if month < pred_start:\n",
    "        continue\n",
    "\n",
    "    train_mask = dates_panel < month\n",
    "    test_mask = dates_panel == month\n",
    "\n",
    "    if test_mask.sum() < 5 or train_mask.sum() < 100:\n",
    "        continue\n",
    "\n",
    "    X_tr, y_tr = X_panel[train_mask].values, y_panel[train_mask].values\n",
    "    X_te, y_te = X_panel[test_mask].values, y_panel[test_mask].values\n",
    "\n",
    "    # XGBoost\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        n_estimators=100, max_depth=3, learning_rate=0.1,\n",
    "        subsample=0.8, colsample_bytree=0.8, verbosity=0\n",
    "    )\n",
    "    xgb_model.fit(X_tr, y_tr)\n",
    "    pred_xgb = xgb_model.predict(X_te)\n",
    "    ics['XGBoost'].append({'month': month, 'IC': stats.spearmanr(pred_xgb, y_te)[0]})\n",
    "\n",
    "    # LightGBM\n",
    "    lgb_model = lgb.LGBMRegressor(\n",
    "        n_estimators=100, max_depth=3, learning_rate=0.1,\n",
    "        subsample=0.8, colsample_bytree=0.8, verbosity=-1\n",
    "    )\n",
    "    lgb_model.fit(X_tr, y_tr)\n",
    "    pred_lgb = lgb_model.predict(X_te)\n",
    "    ics['LightGBM'].append({'month': month, 'IC': stats.spearmanr(pred_lgb, y_te)[0]})\n",
    "\n",
    "    # Random Forest\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, max_depth=5, n_jobs=-1)\n",
    "    rf_model.fit(X_tr, y_tr)\n",
    "    pred_rf = rf_model.predict(X_te)\n",
    "    ics['RandomForest'].append({'month': month, 'IC': stats.spearmanr(pred_rf, y_te)[0]})\n",
    "\n",
    "for name in ics:\n",
    "    ic_vals = [x['IC'] for x in ics[name]]\n",
    "    print(f\"{name}: avg IC = {np.mean(ic_vals):.4f}, t-stat = {np.mean(ic_vals)/np.std(ic_vals)*np.sqrt(len(ic_vals)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Importance: SHAP\n",
    "\n",
    "**Gain-based importance** (built into XGBoost) is biased toward high-cardinality features.  \n",
    "**SHAP values** (SHapley Additive exPlanations) give consistent, unbiased feature importance.\n",
    "\n",
    "SHAP decomposes each prediction: $f(x) = \\phi_0 + \\sum_j \\phi_j$  \n",
    "Where $\\phi_j$ is feature $j$'s contribution to this specific prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Fit final XGBoost model on all data\n",
    "xgb_final = xgb.XGBRegressor(\n",
    "    n_estimators=100, max_depth=3, learning_rate=0.1,\n",
    "    subsample=0.8, colsample_bytree=0.8, verbosity=0\n",
    ")\n",
    "xgb_final.fit(X_panel.values, y_panel.values)\n",
    "\n",
    "# SHAP values\n",
    "explainer = shap.TreeExplainer(xgb_final)\n",
    "shap_values = explainer.shap_values(X_panel.values[-500:])  # last 500 observations\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Bar plot\n",
    "shap.summary_plot(shap_values, X_panel.iloc[-500:], plot_type='bar',\n",
    "                  show=False, max_display=10)\n",
    "plt.title('SHAP Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning with Optuna\n",
    "\n",
    "**Critical:** Use time-series-aware CV — never shuffle across time.\n",
    "\n",
    "Key hyperparameters:\n",
    "- `max_depth`: 3-6 for financial data (deeper = more overfitting)\n",
    "- `learning_rate`: 0.01-0.3 (lower = more trees needed)\n",
    "- `n_estimators`: 50-500 (use early stopping)\n",
    "- `subsample`: 0.6-0.9 (row sampling)\n",
    "- `colsample_bytree`: 0.6-0.9 (column sampling)\n",
    "\n",
    "**Aggressive regularization is needed** — financial data is extremely noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 6),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 0.9),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.9),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10, log=True),\n",
    "    }\n",
    "\n",
    "    # Time-series CV: 3 expanding splits\n",
    "    split_points = [pd.Timestamp('2016-01-31'), pd.Timestamp('2018-01-31'), pd.Timestamp('2020-01-31')]\n",
    "    test_end_points = [pd.Timestamp('2018-01-31'), pd.Timestamp('2020-01-31'), pd.Timestamp('2022-01-31')]\n",
    "\n",
    "    ic_scores = []\n",
    "    for split, test_end in zip(split_points, test_end_points):\n",
    "        tr_mask = dates_panel < split\n",
    "        te_mask = (dates_panel >= split) & (dates_panel < test_end)\n",
    "        if te_mask.sum() < 10:\n",
    "            continue\n",
    "        model = xgb.XGBRegressor(**params, verbosity=0)\n",
    "        model.fit(X_panel.values[tr_mask], y_panel.values[tr_mask])\n",
    "        pred = model.predict(X_panel.values[te_mask])\n",
    "\n",
    "        # Average IC across months in test period\n",
    "        test_dates = dates_panel[te_mask]\n",
    "        for m in np.unique(test_dates):\n",
    "            m_mask = test_dates == m\n",
    "            if m_mask.sum() > 5:\n",
    "                ic = stats.spearmanr(pred[m_mask], y_panel.values[te_mask][m_mask])[0]\n",
    "                ic_scores.append(ic)\n",
    "\n",
    "    return np.mean(ic_scores) if ic_scores else 0\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)  # use 50+ in practice\n",
    "\n",
    "print(f\"\\nBest IC: {study.best_value:.4f}\")\n",
    "print(f\"Best params: {study.best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **XGBoost/LightGBM are the production workhorses** of quant finance.\n",
    "2. **Trees capture interactions** (momentum × volatility) that linear models miss.\n",
    "3. **SHAP > gain-based importance** for understanding what the model learned.\n",
    "4. **Aggressive regularization** (low depth, high subsample) prevents overfitting.\n",
    "5. **Never use shuffled k-fold** with financial data — always temporal CV.\n",
    "6. **Optuna with time-series CV** is the standard hyperparameter tuning approach.\n",
    "\n",
    "**Next week:** Financial ML methodology — triple-barrier labeling, meta-labeling, purged k-fold CV."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}