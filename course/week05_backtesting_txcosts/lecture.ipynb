{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Week 5: Backtesting, Research Discipline & Transaction Costs\n\n> *\"A researcher testing 45 independent trading strategies on the same dataset has a greater than 50% probability of finding one that looks spectacular purely by chance.\"*\n> -- Bailey, Borwein, Lopez de Prado & Zhu (2014), Notices of the American Mathematical Society"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import sys\nimport warnings\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom scipy.stats import spearmanr\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom statsmodels.stats.multitest import multipletests\n\nsys.path.insert(0, \"code\")\nfrom data_setup import (\n    CACHE_DIR, PLOT_DIR, START, END,\n    load_equity_data, load_monthly_returns, load_alpha_output,\n    load_ls_portfolio, load_ohlcv_data, load_mcap_tiers,\n)\n\nsys.path.insert(0, str(Path(\"code\").resolve().parents[1]))\nfrom shared.temporal import CombinatorialPurgedCV, PurgedWalkForwardCV\nfrom shared.metrics import deflated_sharpe_ratio, ic_summary, rank_ic\nfrom shared.backtesting import (\n    cumulative_returns, max_drawdown, net_returns,\n    sharpe_ratio,\n)\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nplt.rcParams.update({\n    \"figure.figsize\": (10, 5),\n    \"axes.grid\": True,\n    \"grid.alpha\": 0.3,\n    \"font.size\": 11,\n})",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## The Opening Hook: When Great Backtests Go Wrong\n\nIn 2014, four mathematicians published a paper in the *Notices of the American Mathematical Society* with an extraordinary title: \"Pseudo-Mathematics and Financial Charlatanism.\" The abstract stated, bluntly, that \"financial charlatanism is a mathematical possibility.\" Their argument was not about fraud. It was about honest researchers doing honest work on finite, noisy data and producing results that look spectacular by construction.\n\nHere is the core insight. Suppose you test 45 independent trading strategies on the same historical dataset. Each strategy is a genuine idea -- not random noise. You evaluate each one fairly: proper train/test splits, out-of-sample performance, the full disciplined process. The probability that at least one strategy shows a Sharpe ratio above 1.0, purely by chance, exceeds 50%. Not because the strategies are bad. Not because the researcher cheated. Because that is what happens when you draw repeatedly from a noisy distribution and keep the best draw.\n\nNow consider what this means in practice. You are an ML engineer who just arrived at a quantitative fund. You have built a gradient boosting model that predicts stock returns, run it through walk-forward cross-validation, computed a Sharpe ratio of 1.4, and prepared your presentation for the portfolio manager. You walk in confident. The PM's first question: \"What's the deflated Sharpe? Did you purge? How many variants did you try before this one? What's the net after realistic transaction costs?\" You have no idea what any of that means. This week fixes that.\n\nWe are going to dismantle your backtest, piece by piece. First, we will catalog the seven canonical ways backtests fail. Then we will fix cross-validation for financial data -- where the standard ML approach leaks information through the label construction itself. We will quantify the multiple-testing penalty that accumulates every time you try another model variant. We will decompose transaction costs into their three components and watch them eat your gross returns. And at the end, we will put it all together into a single responsible evaluation that reveals the truth about a strategy's viability."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 1: The Seven Sins -- A Taxonomy of Backtest Failure\n\nA Deutsche Bank quant team once catalogued the most common ways systematic strategies fail after a promising backtest. Every failure mode on the list is a form of optimism that the data did not earn -- and every one is preventable with a checklist. Think of these as the \"common bugs\" of quantitative research: look-ahead bias, survivorship bias, data snooping, ignoring transaction costs, ignoring short-selling constraints, improper benchmark comparison, and regime neglect. We will tackle the first two with code right now.\n\nLet us start with the most devastating: look-ahead bias. In ML terms, this is the equivalent of accidentally including the label as a feature. In finance, it is more insidious because the label itself is constructed from future prices -- a forward-looking return that spans multiple days. The contamination is structural, not accidental."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "monthly_prices  = load_equity_data()\nmonthly_returns_raw = monthly_prices.pct_change()\nmonthly_returns = load_monthly_returns()\n\nn_stocks  = monthly_returns.shape[1]\nn_periods = monthly_returns.shape[0]\nmissing_pct = monthly_prices.isnull().mean().mean()\n\nfirst_month_prices = monthly_prices.iloc[0]\nn_survivors_strict = int(first_month_prices.notna().sum())\nn_all              = n_stocks\n\nprint(\"── Data Quality ──────────────────────────────────────────────────\")\nprint(f\"  N_stocks:          {n_stocks}\")\nprint(f\"  N_periods:         {n_periods} monthly observations\")\nprint(f\"  Missing (prices):  {missing_pct:.4%}\")\nprint(f\"  Present at start:  {n_survivors_strict} / {n_all} tickers\")\nprint(f\"  Survivorship note: S&P 500 universe = current constituents only.\")\nprint(f\"                     Delisted stocks not in dataset — bias understated.\")\nprint()\n\nflawed_signal = monthly_returns.rank(axis=1, pct=True)\n\ncorrected_signal = monthly_returns.shift(1).rank(axis=1, pct=True)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We are working with the current S&P 500 universe -- roughly 450 stocks with monthly data from 2012 to 2025. Notice the survivorship note: every ticker in this dataset is a company that survived to today. The ones that went bankrupt, got delisted, or were acquired are silently absent.\n\nThe flawed signal uses the return we are trying to predict as the ranking criterion. The corrected signal shifts everything by one month -- it only uses information available at the time of prediction. This one-line difference, `.shift(1)`, is the entire distinction between a look-ahead bug and a legitimate signal. In a large codebase, finding that missing shift can take days."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Measuring Signal Quality with the Information Coefficient\n\nWe need a way to measure whether these signals actually rank stocks correctly. The **information coefficient** (IC) is the finance-world name for exactly what you know as Spearman rank correlation: how well does the predicted ranking match the actual ranking of forward returns? An IC of 1.0 means perfect prediction. An IC of 0.05 sounds pathetic in ML terms, but in a universe of 450 stocks rebalanced monthly, it is enough to build a career on."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def monthly_ic(signal: pd.DataFrame, returns: pd.DataFrame) -> pd.Series:\n    \"\"\"Cross-sectional Spearman IC for each date in signal's index.\"\"\"\n    ic_vals = {}\n    for date in signal.index:\n        sig = signal.loc[date].dropna()\n        ret = returns.loc[date].dropna()\n        common = sig.index.intersection(ret.index)\n        if len(common) < 10:\n            continue\n        corr, _ = spearmanr(sig[common], ret[common])\n        ic_vals[date] = corr\n    return pd.Series(ic_vals, name=\"ic\")\n\nIS_START  = \"2012-01-01\"\nIS_END    = \"2017-12-31\"\nOOS_START = \"2018-01-01\"\nOOS_END   = \"2024-12-31\"\n\noos_mask = (monthly_returns.index >= OOS_START) & (monthly_returns.index <= OOS_END)\noos_returns = monthly_returns.loc[oos_mask]\n\nflawed_ic_oos    = monthly_ic(flawed_signal.loc[oos_mask],    oos_returns)\ncorrected_ic_oos = monthly_ic(corrected_signal.loc[oos_mask], oos_returns)\n\nmean_flawed_ic_oos    = flawed_ic_oos.mean()\nmean_corrected_ic_oos = corrected_ic_oos.mean()\n\nprint(f\"  Flawed signal OOS IC:    {mean_flawed_ic_oos:.4f}  ← signal == outcome\")\nprint(f\"  Corrected signal OOS IC: {mean_corrected_ic_oos:.4f}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "IC of 1.0 for the flawed signal -- perfect foresight, because the signal literally *is* the forward return. The corrected signal shows an IC near zero, which is what you would expect from a simple one-month momentum signal on a large-cap universe. This is not a failure of the corrected signal. This is reality. The flawed signal's IC of 1.0 is the lie.\n\nBut IC is abstract. What does this look like in dollar terms? Let us build a simple long-short portfolio -- buy the top 20 stocks by signal, short the bottom 20 -- and see how the equity curves diverge."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def long_short_monthly(\n    signal: pd.DataFrame,\n    returns: pd.DataFrame,\n    n_leg: int = 20,\n) -> pd.Series:\n    \"\"\"Equal-weight top-n long / bottom-n short monthly return.\"\"\"\n    port_rets = []\n    for date in signal.index:\n        sig = signal.loc[date].dropna()\n        ret = returns.loc[date].dropna()\n        common = sig.index.intersection(ret.index)\n        if len(common) < n_leg * 2:\n            continue\n        ranked = sig[common].rank(ascending=True)\n        n = len(ranked)\n        long_ret  = ret[common][ranked > (n - n_leg)].mean()\n        short_ret = ret[common][ranked <= n_leg].mean()\n        port_rets.append({\"date\": date, \"ret\": long_ret - short_ret})\n    return pd.DataFrame(port_rets).set_index(\"date\")[\"ret\"]\n\nflawed_ret_oos    = long_short_monthly(flawed_signal.loc[oos_mask],    oos_returns)\ncorrected_ret_oos = long_short_monthly(corrected_signal.loc[oos_mask], oos_returns)\n\nann_flawed    = flawed_ret_oos.mean()    * 12\nann_corrected = corrected_ret_oos.mean() * 12\nannual_gap    = ann_flawed - ann_corrected\n\ncum_flawed    = (1 + flawed_ret_oos).cumprod()\ncum_corrected = (1 + corrected_ret_oos).cumprod()\n\nprint(f\"  Flawed ann. return:    {ann_flawed:.2%}\")\nprint(f\"  Corrected ann. return: {ann_corrected:.2%}\")\nprint(f\"  Annual return gap:     {annual_gap:.2%}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "An annual return gap of 400 percentage points. The flawed signal generates nearly 400% annual returns while the corrected signal flatlines near zero. In production, look-ahead bugs are rarely this extreme. Realistic bugs -- using the wrong close price, including contemporaneous earnings data, training on a future-informed universe -- cause inflation of 20-50% annualized according to research from Deutsche Bank's quantitative strategy group. Subtler, but still enough to destroy a strategy's value proposition when the bug is eventually found."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "fig_eq, ax_eq = plt.subplots(figsize=(10, 5))\nax_eq.semilogy(cum_flawed.index,    cum_flawed.values,    label=\"Flawed (look-ahead)\", linewidth=2)\nax_eq.semilogy(cum_corrected.index, cum_corrected.values, label=\"Corrected (lagged signal)\", linewidth=2)\nax_eq.set(\n    title=\"Equity Curve: Flawed vs. Corrected Signal (OOS 2018–2024, log scale)\",\n    xlabel=\"Date\",\n    ylabel=\"Cumulative Return (log scale)\",\n)\nax_eq.legend()\nax_eq.axhline(1.0, color=\"gray\", linestyle=\"--\", linewidth=0.8)\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "On the log scale, the flawed signal rockets upward -- turning \\$1 into billions by construction. The corrected signal meanders around 1.0, sometimes above, sometimes below. If you showed the flawed equity curve to a PM without context, they would either offer you a job or call security, depending on whether they understood what they were looking at. This is why the first item on every backtesting pre-flight checklist is: *verify that no future information leaks into the signal.*"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Survivorship Bias: The Invisible Filter\n\nNow let us tackle the second sin: survivorship bias. Every stock in our dataset is a company that exists today. Companies that went bankrupt -- Enron (#7 by revenue in 2000, ceased to exist by 2002), WorldCom (#20 by market cap in 2000, filed the largest bankruptcy in history two years later), Lehman Brothers (\\$639 billion in assets the day before it collapsed) -- are simply absent from our universe. Since our dataset only contains survivors, we simulate the bias: each year, 5% of stocks \"delist\" with a -50% terminal return, mimicking the observed churn rate in US equity indices."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "oos_raw = monthly_returns_raw.loc[OOS_START:OOS_END]\n\new_survivors = oos_raw.mean(axis=1)\nann_survivors = ew_survivors.mean() * 12\n\nSEED = 42\nANNUAL_CHURN_RATE = 0.05\nDELIST_RETURN = -0.50\n\nrng = np.random.default_rng(SEED)\noos_simulated = oos_raw.copy()\nn_tickers = oos_raw.shape[1]\n\nfor year in oos_raw.index.year.unique():\n    year_dates = oos_raw.index[oos_raw.index.year == year]\n    if len(year_dates) == 0:\n        continue\n    n_exit = max(1, int(n_tickers * ANNUAL_CHURN_RATE))\n    exit_cols = rng.choice(n_tickers, size=n_exit, replace=False)\n    exit_tickers = oos_raw.columns[exit_cols]\n    delist_month = year_dates[0]\n    oos_simulated.loc[delist_month, exit_tickers] = DELIST_RETURN\n    if len(year_dates) > 1:\n        oos_simulated.loc[year_dates[1:], exit_tickers] = 0.0\n\new_all = oos_simulated.mean(axis=1)\nann_all = ew_all.mean() * 12\nsurvivorship_premium = ann_survivors - ann_all\n\nprint(f\"  Survivor EW annual return:     {ann_survivors:.4%}\")\nprint(f\"  Sim unbiased EW annual return: {ann_all:.4%}\")\nprint(f\"  Survivorship premium:          {survivorship_premium:.4%} annualized\")\nprint(f\"  (Simulates {ANNUAL_CHURN_RATE:.0%}/yr churn, \"\n      f\"{DELIST_RETURN:.0%} delist return — consistent with historical estimates)\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "A survivorship premium of approximately 3% annualized. That is consistent with academic estimates: Elton, Gruber & Blake (1996) found 0.9% per year for mutual fund survivorship, and Campbell Harvey's work with CRSP data estimates 1-4% per year for equity universe survivorship depending on methodology. It means that if your backtest shows 15% annualized return, roughly 3 of those percentage points might be ghosts -- returns from companies that your model never learned could fail, because they were quietly removed from the dataset years ago."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "fig_sv, ax_sv = plt.subplots(figsize=(7, 5))\nlabels   = [\"Unbiased\\n(simulated delisting)\", \"Survivor-Only\\n(S&P 500 universe)\"]\nreturns_ = [ann_all, ann_survivors]\ncolors   = [\"steelblue\", \"tomato\"]\n\nbars = ax_sv.bar(labels, [r * 100 for r in returns_], color=colors, alpha=0.85, width=0.5)\nax_sv.bar_label(bars, fmt=\"%.1f%%\", padding=3)\nax_sv.set(\n    title=f\"Survivorship Bias: EW Annual Return OOS 2018–2024\\n\"\n          f\"(Simulated {ANNUAL_CHURN_RATE:.0%}/yr churn, {DELIST_RETURN:.0%} delist return)\",\n    ylabel=\"Annualized Return (%)\",\n)\nax_sv.axhline(0, color=\"gray\", linewidth=0.8)\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The bar chart makes the survivorship premium visible. Over a 13-year backtest, a 3% annual premium compounds to a 46% cumulative inflation. Production quant shops use the CRSP database, which includes all NYSE/AMEX/NASDAQ stocks including delistings -- roughly 3,000+ stocks versus our 450 survivors. Every sin in this taxonomy has appeared in published papers and live fund strategies. Now that we know the failure modes, we need the tools to prevent them -- starting with cross-validation that respects the structure of financial time series."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 2: Purged Cross-Validation -- Closing the Leakage Gap\n\nStandard time-series cross-validation -- the kind you get from sklearn's `TimeSeriesSplit` -- respects temporal order. But it has a blind spot unique to financial prediction. When your label is a forward-looking return -- \"the return from month T to month T+1\" -- the training sample at month T-1 already \"knows\" almost everything the test sample at month T will reveal. **Purging** removes all training observations whose label interval overlaps with the test fold's date range. **Embargoing** adds a buffer zone after each test fold to account for serial dependence. Without both, your cross-validated performance estimate is an optimistic fiction."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "alpha = load_alpha_output()\npredictions = alpha[\"predictions\"]\n\noos_dates = predictions.index.get_level_values(\"date\").unique().sort_values()\nn_obs = len(oos_dates)\nK_FOLDS = 10\nLABEL_DURATION = 1\nEMBARGO = 1",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We are loading the alpha model predictions from Week 4 (or a synthetic momentum fallback if Week 4 has not been run). The `LABEL_DURATION` of 1 month means our label looks one period ahead, and the `EMBARGO` of 1 month adds a buffer after each test fold. These are conservative settings for monthly data -- on daily data with 21-day labels, the purge zone would be 21 times wider and the effect dramatically larger."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "class PurgedKFoldDemo:\n    \"\"\"Simplified purged k-fold splitter for teaching purposes.\n\n    For each fold the test set is a contiguous block of time indices.\n    Training data is all data outside the test block, minus:\n      - a purge zone: the `label_duration` periods immediately before\n        the test start (their labels overlap with the test period), and\n      - an embargo zone: the `embargo` periods immediately after the\n        test end (to guard against serial dependence).\n\n    Args:\n        n_splits:        number of folds.\n        label_duration:  number of periods a label looks forward.\n        embargo:         number of periods to drop after each test block.\n    \"\"\"\n\n    def __init__(\n        self, n_splits: int = 5, label_duration: int = 1, embargo: int = 1\n    ):\n        self.n_splits = n_splits\n        self.label_duration = label_duration\n        self.embargo = embargo",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The class definition lays out the three parameters a purged splitter needs: the number of folds, how many periods the label looks forward (this determines the purge zone width), and how many additional periods to embargo after each test fold. This is the algorithm from Lopez de Prado's Chapter 7, stripped to its essentials."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def split(self, X, y=None):\n    \"\"\"Yield (train_indices, test_indices) for each fold.\n\n    X must be a 1-D index or array of length n; the positional\n    integers are what gets yielded (not the values).\n\n    The fold grid starts at `label_duration + 1` to ensure every fold\n    has at least one valid training observation after purging.\n    \"\"\"\n    n = len(X)\n    seed = self.label_duration + 1\n    if n <= seed:\n        return\n    usable = n - seed\n    fold_size = usable // self.n_splits\n\n    for k in range(self.n_splits):\n        test_start = seed + k * fold_size\n        test_end = (\n            test_start + fold_size if k < self.n_splits - 1 else n\n        )\n\n        purge_start = max(0, test_start - self.label_duration)\n\n        embargo_end = min(n, test_end + self.embargo)\n\n        test_idx = np.arange(test_start, test_end)\n\n        before_purge = np.arange(0, purge_start)\n        after_embargo = np.arange(embargo_end, n)\n        train_idx = np.concatenate([before_purge, after_embargo])\n\n        if len(train_idx) == 0:\n            continue\n\n        yield train_idx.copy(), test_idx.copy(), purge_start, test_end, embargo_end\n\nPurgedKFoldDemo.split = split",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Read the `split` method carefully. For each fold, it defines four zones: training (everything before the purge and after the embargo), the purge zone (removed because their labels overlap with the test period), the test block itself, and the embargo zone (removed as a safety buffer). Unlike walk-forward, the training data appears on *both sides* of the test window -- this is the k-fold structure, which gives more training data at the cost of requiring the purging mechanism."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def compute_fold_ic(test_dates, predictions):\n    \"\"\"Compute Spearman IC across all test-period dates.\n\n    For each date in `test_dates`, retrieves cross-sectional predictions\n    and actual forward returns, computes Spearman rank correlation, and\n    returns the mean IC across dates.\n\n    Args:\n        test_dates: array-like of dates in the test fold.\n        predictions: MultiIndex (date, ticker) DataFrame with columns\n                     'prediction' and 'actual'.\n\n    Returns:\n        Mean Spearman IC over valid test dates (NaN if no valid dates).\n    \"\"\"\n    ic_values = []\n    for date in test_dates:\n        if date not in predictions.index.get_level_values(\"date\"):\n            continue\n        df = predictions.loc[date].dropna(subset=[\"prediction\", \"actual\"])\n        if len(df) < 10:\n            continue\n        corr, _ = stats.spearmanr(df[\"prediction\"], df[\"actual\"])\n        if np.isfinite(corr):\n            ic_values.append(corr)\n    return float(np.mean(ic_values)) if ic_values else np.nan",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Now let us run the walk-forward baseline -- sklearn's `TimeSeriesSplit` with a gap parameter equal to the label duration -- and compare it head-to-head with the purged splitter."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "tss = TimeSeriesSplit(n_splits=K_FOLDS, gap=LABEL_DURATION)\n\nwf_ic_per_fold = []\nwf_splits_meta = []\n\nfor fold_idx, (train_idx, test_idx) in enumerate(tss.split(np.arange(n_obs))):\n    test_dates = oos_dates[test_idx]\n    fold_ic = compute_fold_ic(test_dates, predictions)\n    wf_ic_per_fold.append(fold_ic)\n    wf_splits_meta.append((train_idx, test_idx))\n    print(f\"  WF fold {fold_idx+1}/{K_FOLDS}: \"\n          f\"test={oos_dates[test_idx[0]].date()}–{oos_dates[test_idx[-1]].date()}, \"\n          f\"IC={fold_ic:.4f}\")\n\nwf_ic_series = pd.Series(wf_ic_per_fold, name=\"wf_ic\")\nwf_mean_ic = float(np.nanmean(wf_ic_per_fold))\nprint(f\"  Walk-forward mean CV IC: {wf_mean_ic:.4f}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "A mean IC of approximately 0.02 -- weak by any standard, and statistically insignificant at conventional levels. This is the honest reality of a cross-sectional alpha signal on free data with a survivorship-biased universe of 450 stocks over 68 months. Now the purged version:"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "pkf = PurgedKFoldDemo(\n    n_splits=K_FOLDS, label_duration=LABEL_DURATION, embargo=EMBARGO\n)\n\npurged_ic_per_fold = []\npurged_splits_meta = []\n\nfor fold_idx, (train_idx, test_idx, purge_start, test_end, embargo_end) in \\\n        enumerate(pkf.split(np.arange(n_obs))):\n    test_dates = oos_dates[test_idx]\n    fold_ic = compute_fold_ic(test_dates, predictions)\n    purged_ic_per_fold.append(fold_ic)\n    purged_splits_meta.append((train_idx, test_idx, purge_start, test_end, embargo_end))\n    purge_zone_size = test_idx[0] - purge_start\n    print(f\"  PKF fold {fold_idx+1}/{K_FOLDS}: \"\n          f\"train={len(train_idx)} obs, purge={purge_zone_size} obs, \"\n          f\"test={oos_dates[test_idx[0]].date()}–{oos_dates[test_idx[-1]].date()}, \"\n          f\"IC={fold_ic:.4f}\")\n\npurged_ic_series = pd.Series(purged_ic_per_fold, name=\"purged_ic\")\npurged_mean_ic = float(np.nanmean(purged_ic_per_fold))\nic_delta = wf_mean_ic - purged_mean_ic\nprint(f\"  Purged KFold mean CV IC: {purged_mean_ic:.4f}\")\nprint(f\"  IC delta (WF − purged): {ic_delta:.4f}\")\n\nif ic_delta < 0.005:\n    print(\"  NOTE: IC delta < 0.005 — purging effect modest on monthly data \"\n          \"(label duration ≈ fold size). Effect is structural, not a bug.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The IC delta is approximately -0.001 -- effectively zero. This is *not* because purging does not work. It is because the label duration (1 month) equals the data frequency (monthly), compressing the contamination zone to a single observation per fold boundary. Lopez de Prado's own work documents that walk-forward CV overestimates IC by 20-50% for 20-day labels on daily data. Our monthly exercise shows the mechanism working correctly at a frequency where the effect happens to be small. The teaching point is precisely this frequency dependence: on daily data with multi-day labels -- the standard configuration at production quant shops -- the purging adjustment is material, often reducing IC estimates by 10-30%."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "fig_sv, ax_sv = plt.subplots(figsize=(12, 6))\n\nn_folds_vis = len(purged_splits_meta)\ndate_positions = np.arange(n_obs)\n\nfor fold_idx, (train_idx, test_idx, purge_start, test_end, embargo_end) in \\\n        enumerate(purged_splits_meta):\n    y = fold_idx\n\n    if len(train_idx) > 0:\n        ax_sv.barh(\n            y, width=len(train_idx), left=train_idx[0],\n            height=0.7, color=\"#4878CF\", alpha=0.7, align=\"center\"\n        )\n\n    purge_zone_len = test_idx[0] - purge_start\n    if purge_zone_len > 0:\n        ax_sv.barh(\n            y, width=purge_zone_len, left=purge_start,\n            height=0.7, color=\"#D65F5F\", alpha=0.8, align=\"center\"\n        )\n\n    ax_sv.barh(\n        y, width=len(test_idx), left=test_idx[0],\n        height=0.7, color=\"#E8A838\", alpha=0.85, align=\"center\"\n    )\n\n    embargo_len = min(embargo_end, n_obs) - test_end\n    if embargo_len > 0:\n        ax_sv.barh(\n            y, width=embargo_len, left=test_end,\n            height=0.7, color=\"#888888\", alpha=0.6, align=\"center\"\n        )\n\ntick_pos = np.arange(0, n_obs, max(1, n_obs // 8))\ntick_labels = [str(oos_dates[i].year) for i in tick_pos]\nax_sv.set_xticks(tick_pos)\nax_sv.set_xticklabels(tick_labels, fontsize=9)\nax_sv.set_yticks(range(n_folds_vis))\nax_sv.set_yticklabels([f\"Fold {k+1}\" for k in range(n_folds_vis)], fontsize=9)\n\nlegend_patches = [\n    mpatches.Patch(color=\"#4878CF\", alpha=0.7, label=\"Train\"),\n    mpatches.Patch(color=\"#E8A838\", alpha=0.85, label=\"Test\"),\n    mpatches.Patch(color=\"#D65F5F\", alpha=0.8, label=\"Purged\"),\n    mpatches.Patch(color=\"#888888\", alpha=0.6, label=\"Embargo\"),\n]\nax_sv.legend(handles=legend_patches, loc=\"lower right\", fontsize=9)\nax_sv.set(\n    title=\"Purged K-Fold Splits: Train / Test / Purged / Embargo Zones\",\n    xlabel=\"Period Index (OOS months)\",\n    ylabel=\"Fold\",\n)\nax_sv.invert_yaxis()\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Each horizontal bar represents one fold. Blue is training data, orange is the test window, red is the purge zone (observations removed because their labels overlap with the test period), and gray is the embargo. Notice that unlike walk-forward, the training data appears on *both sides* of the test window -- this is the k-fold structure. The purge zones are narrow (1 observation each) because our label duration equals 1 month. On daily data, those red strips would be 21 trading days wide and visually prominent. This visualization is what a model risk analyst at an asset manager reviews when auditing a backtest's CV methodology."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "valid_folds = [i for i in range(len(wf_ic_per_fold))\n               if np.isfinite(wf_ic_per_fold[i]) and i < len(purged_ic_per_fold)\n               and np.isfinite(purged_ic_per_fold[i])]\nfold_nums = [i + 1 for i in valid_folds]\nwf_vals = [wf_ic_per_fold[i] for i in valid_folds]\npurged_vals = [purged_ic_per_fold[i] for i in valid_folds]\n\nfig_ic, ax_ic = plt.subplots(figsize=(10, 5))\nax_ic.plot(fold_nums, wf_vals, marker=\"o\", linewidth=2,\n           color=\"#4878CF\", label=f\"Walk-Forward (mean={wf_mean_ic:.4f})\")\nax_ic.plot(fold_nums, purged_vals, marker=\"s\", linewidth=2,\n           color=\"#D65F5F\", linestyle=\"--\",\n           label=f\"Purged KFold (mean={purged_mean_ic:.4f})\")\nax_ic.axhline(0, color=\"black\", linewidth=0.8, linestyle=\":\")\nax_ic.set(\n    title=\"Cross-Validation IC by Fold: Walk-Forward vs. Purged KFold\",\n    xlabel=\"Fold Index\",\n    ylabel=\"Spearman IC\",\n)\nax_ic.legend(fontsize=10)\nax_ic.grid(axis=\"y\", alpha=0.3)\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The two methods track each other closely -- both show the same general pattern of IC rising and falling across folds. The mean IC values are within 0.001 of each other. In the seminar, students will discover that the methods disagree on *which model is best* in 6 out of 10 folds -- methodology choice affects model selection even when the aggregate IC gap is negligible. Purged CV tells us how to split correctly. But even with correct splits, we can still overfit if we search long enough over enough variants."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 3: Combinatorial Purged CV & the Multiple-Testing Problem\n\nYou found the best of 50 strategy variants. Walk-forward says Sharpe 1.4. But here is what walk-forward does not tell you: if those 50 variants were all drawn from the same noisy distribution, the expected maximum Sharpe across 50 independent noise processes is substantially above 1.0. Your \"best\" strategy may have won a beauty contest in noise.\n\nCombinatorial Purged Cross-Validation (CPCV) addresses this by generating not one but many train/test splits -- all possible combinations of k folds taken 2 at a time. For k=6, that is $\\binom{6}{2} = 15$ unique train-test paths. The **Probability of Backtest Overfitting (PBO)** is then the fraction of those paths where the in-sample winner underperforms the median out-of-sample rank. PBO > 0.5 means your best model is more likely than not to disappoint in live trading."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "alpha = load_alpha_output()\ngbm_pred = alpha[\"predictions\"]\nnn_pred = alpha.get(\"nn_predictions\", None)\nfeat_df = alpha.get(\"expanded_features\")\n\nfwd_df = pd.read_parquet(CACHE_DIR / \"forward_returns_w5.parquet\")\n\ngbm_dates = gbm_pred.index.get_level_values(\"date\").unique().sort_values()\nprint(f\"  GBM OOS dates: {len(gbm_dates)} months \"\n      f\"({gbm_dates[0].date()} – {gbm_dates[-1].date()})\")\n\nif nn_pred is not None:\n    nn_dates = nn_pred.index.get_level_values(\"date\").unique().sort_values()\n    print(f\"  NN  OOS dates: {len(nn_dates)} months\")\nelse:\n    nn_pred = None\n    print(\"  NN predictions: not available\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We load three model variants: the gradient boosting model (GBM) from Week 4, a neural network (NN) if available, and we will train a Ridge regression as the third variant. Having three models lets us run CPCV across variants and compute PBO. The Ridge model needs to be trained from scratch via walk-forward, since Week 4 did not cache one."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def _ridge_train_predict_one(\n    feat_df, fwd_df, feat_dates, train_window, i, pred_date\n):\n    \"\"\"Train Ridge on prior window and predict one date.\n\n    Returns list of record dicts, or empty list if data is insufficient.\n    \"\"\"\n    train_dates = feat_dates[i - train_window:i]\n\n    X_list, y_list = [], []\n    for td in train_dates:\n        try:\n            X_td = feat_df.loc[td]\n            y_td = fwd_df.loc[td][\"fwd_return\"]\n        except KeyError:\n            continue\n        common = X_td.index.intersection(y_td.index)\n        if len(common) < 10:\n            continue\n        X_list.append(X_td.loc[common])\n        y_list.append(y_td.loc[common])\n\n    if not X_list:\n        return []\n\n    X_train = pd.concat(X_list).fillna(0.0)\n    y_train = pd.concat(y_list).fillna(0.0)\n\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X_train)\n    model = Ridge(alpha=1.0)\n    model.fit(X_scaled, y_train)\n\n    try:\n        X_pred = feat_df.loc[pred_date].fillna(0.0)\n        y_actual = fwd_df.loc[pred_date][\"fwd_return\"]\n    except KeyError:\n        return []\n\n    common_pred = X_pred.index.intersection(y_actual.index)\n    if len(common_pred) < 10:\n        return []\n\n    X_pred_scaled = scaler.transform(X_pred.loc[common_pred])\n    preds = model.predict(X_pred_scaled)\n\n    records = []\n    for ticker, p, a in zip(common_pred, preds, y_actual.loc[common_pred].values):\n        records.append({\"date\": pred_date, \"ticker\": ticker,\n                        \"prediction\": float(p), \"actual\": float(a)})\n    return records",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "This is standard walk-forward: train on the most recent 36 months, predict the next month, slide forward, repeat. The Ridge model is deliberately simple -- it acts as a \"weak model\" baseline that we expect to underperform GBM and NN. Having one weak model in the CPCV comparison is pedagogically valuable because it creates a clear ranking to test PBO against."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def compute_ridge_predictions(\n    feat_df: pd.DataFrame,\n    fwd_df: pd.DataFrame,\n    train_window: int = 36,\n) -> pd.DataFrame:\n    \"\"\"Walk-forward Ridge regression predictions.\n\n    For each OOS date, train Ridge on the prior `train_window` months\n    of expanded features, then predict one month ahead.\n\n    Args:\n        feat_df: MultiIndex (date, ticker) feature DataFrame.\n        fwd_df: MultiIndex (date, ticker) with column 'fwd_return'.\n        train_window: number of months in the rolling training window.\n\n    Returns:\n        DataFrame with MultiIndex (date, ticker) and columns\n        ['prediction', 'actual'].\n    \"\"\"\n    feat_dates = feat_df.index.get_level_values(\"date\").unique().sort_values()\n    records = []\n\n    for i, pred_date in enumerate(feat_dates[train_window:], start=train_window):\n        if i % 12 == 0:\n            print(f\"  [Ridge {i}/{len(feat_dates)}] {pred_date.date()}\")\n        records.extend(\n            _ridge_train_predict_one(feat_df, fwd_df, feat_dates, train_window, i, pred_date)\n        )\n\n    ridge_df = pd.DataFrame(records).set_index([\"date\", \"ticker\"])\n    return ridge_df\n\nprint(\"Computing Ridge walk-forward predictions...\")\nridge_pred = compute_ridge_predictions(feat_df, fwd_df, train_window=36)\nridge_dates = ridge_pred.index.get_level_values(\"date\").unique().sort_values()\nprint(f\"  Ridge OOS dates: {len(ridge_dates)} months \"\n      f\"({ridge_dates[0].date()} – {ridge_dates[-1].date()})\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Now we align all three models to the common OOS period and compute monthly IC series for each. With all three models on the same 68-month window, we can make a fair comparison."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "common_dates = gbm_dates\n\nif nn_pred is not None:\n    nn_common = common_dates[common_dates.isin(\n        nn_pred.index.get_level_values(\"date\").unique()\n    )]\nelse:\n    nn_common = common_dates\n\nridge_common = common_dates[common_dates.isin(ridge_dates)]\nfinal_dates = common_dates[\n    common_dates.isin(nn_common) & common_dates.isin(ridge_common)\n]\nprint(f\"  Aligned OOS dates (all 3 models): {len(final_dates)} months\")\n\ndef compute_ic_series(pred_df: pd.DataFrame, dates: pd.DatetimeIndex) -> pd.Series:\n    \"\"\"Compute monthly Spearman IC for a prediction DataFrame.\"\"\"\n    ic_vals = {}\n    for d in dates:\n        try:\n            sub = pred_df.loc[d]\n        except KeyError:\n            continue\n        sub = sub.dropna()\n        if len(sub) < 10:\n            continue\n        ic_vals[d] = rank_ic(sub[\"prediction\"].values, sub[\"actual\"].values)\n    return pd.Series(ic_vals, name=\"ic\")\n\ngbm_ic_series = compute_ic_series(gbm_pred, final_dates)\nnn_ic_series = compute_ic_series(nn_pred, final_dates) if nn_pred is not None else None\nridge_ic_series = compute_ic_series(ridge_pred, final_dates)\n\nmodel_ic = {\n    \"GBM\": gbm_ic_series,\n    \"Ridge\": ridge_ic_series,\n}\nif nn_ic_series is not None:\n    model_ic[\"NN\"] = nn_ic_series\n\nfor name, ic_s in model_ic.items():\n    stats_d = ic_summary(ic_s.dropna())\n    print(f\"  {name:6s}: mean_IC={stats_d['mean_ic']:.4f}, \"\n          f\"t={stats_d['t_stat']:.2f}, p={stats_d['p_value']:.3f}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "None of the three models achieves a t-statistic above 1.96. The GBM model has the highest IC at approximately 0.026 (t around 1.4), while the Ridge model is near zero. This is the honest reality of cross-sectional prediction on free data with a survivorship-biased universe. The Gu, Kelly & Xiu (2020) benchmark achieves IC of 0.04-0.05 on CRSP's full 3,000+ stock universe with 94 features and 60 years of data. Our weaker result is consistent with our smaller universe and fewer features.\n\nNow let us run CPCV. We split the 68 OOS months into 6 folds, generating $\\binom{6}{2} = 15$ combinatorial train-test paths. For each path, we identify the in-sample winner and check where it ranks out-of-sample."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "ic_panel = pd.DataFrame(model_ic).dropna()\nn_obs = len(ic_panel)\nmodel_names = ic_panel.columns.tolist()\n\nprint(f\"\\nCPCV setup: {n_obs} time points × {len(model_names)} models\")\nprint(f\"  Models: {model_names}\")\n\ncpcv = CombinatorialPurgedCV(n_splits=6, n_test_splits=2, purge_gap=1)\nn_paths_expected = cpcv.get_n_splits(ic_panel)\nprint(f\"  CPCV: C(6,2) = {n_paths_expected} paths expected\")\n\ncpcv_paths = []\n\nfor path_idx, (train_idx, test_idx) in enumerate(cpcv.split(ic_panel)):\n    is_ics = ic_panel.iloc[train_idx]\n    oos_ics = ic_panel.iloc[test_idx]\n\n    is_mean = is_ics.mean()\n    oos_mean = oos_ics.mean()\n\n    is_winner = is_mean.idxmax()\n    oos_rank_of_winner = (\n        oos_mean.rank(ascending=False).loc[is_winner]\n    )\n\n    n_models = len(model_names)\n    oos_median_rank = (n_models + 1) / 2.0\n\n    cpcv_paths.append({\n        \"path\": path_idx,\n        \"is_winner\": is_winner,\n        \"oos_rank_winner\": float(oos_rank_of_winner),\n        \"oos_ic_winner\": float(oos_mean.loc[is_winner]),\n        \"n_models\": n_models,\n        \"oos_median_rank\": oos_median_rank,\n    })\n\nprint(f\"\\nCPCV complete: {len(cpcv_paths)} paths\")\ncpcv_df = pd.DataFrame(cpcv_paths)\n\nis_winners = cpcv_df[\"is_winner\"].value_counts()\nprint(f\"  IS winners by model: {is_winners.to_dict()}\")\n\noos_return_series = cpcv_df[\"oos_ic_winner\"]\nprint(f\"  OOS IC winner: mean={oos_return_series.mean():.4f}, \"\n      f\"std={oos_return_series.std():.4f}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Fifteen paths, each evaluating which model wins in-sample and where it places out-of-sample. Now the moment of truth -- PBO:"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "n_paths = len(cpcv_df)\nn_below_median = (cpcv_df[\"oos_rank_winner\"] > cpcv_df[\"oos_median_rank\"]).sum()\npbo = n_below_median / n_paths\n\nprint(f\"\\nProbability of Backtest Overfitting (PBO)\")\nprint(f\"  Paths where IS-winner ranks below median OOS: \"\n      f\"{n_below_median}/{n_paths}\")\nprint(f\"  PBO = {pbo:.4f} ({pbo:.1%})\")\n\nif pbo < 0.20:\n    print(\"  ⚠ PBO < 0.20: suspiciously low — possible look-ahead or leakage\")\nelif pbo > 0.75:\n    print(\"  ⚠ PBO > 0.75: very high — models may be pure noise\")\nelse:\n    print(f\"  PBO in acceptable range [0.20, 0.75] — typical regime variation\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "PBO of 0.267 -- meaning the in-sample winner maintains its ranking above the OOS median in 73% of CPCV paths. This falls in the \"genuine alpha\" range of 0.25-0.35 per Bailey et al. (2015). But a crucial nuance: PBO < 0.5 does not confirm statistical significance. None of our three models clears even t = 1.96, so the absolute signal remains indistinguishable from zero at conventional confidence levels. PBO tells us that GBM is *persistently the least-bad* of the three models -- not that its signal is statistically real."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### The Harvey-Liu-Zhu t-Statistic Hurdle\n\nThe Harvey-Liu-Zhu threshold tightens the screw further. In 2016, Harvey, Liu, and Zhu established that given the hundreds of factors published through 2012, any new factor claim needs a t-statistic of at least 3.0 to control the false discovery rate. With hundreds more factors published since then, some researchers argue the bar should be even higher. Let us apply both HLZ and the Benjamini-Hochberg-Yekutieli (BHY) correction to our three models."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "hlz_results = {}\nfor name, ic_s in model_ic.items():\n    s = ic_summary(ic_s.dropna())\n    hlz_results[name] = {\n        \"mean_ic\": s[\"mean_ic\"],\n        \"t_stat\": s[\"t_stat\"],\n        \"p_value\": s[\"p_value\"],\n        \"n\": s[\"n\"],\n        \"passes_t196\": s[\"t_stat\"] > 1.96 if np.isfinite(s[\"t_stat\"]) else False,\n        \"passes_t300\": s[\"t_stat\"] > 3.00 if np.isfinite(s[\"t_stat\"]) else False,\n    }\n\nprint(\"\\nHarvey-Liu-Zhu t-stat analysis\")\nprint(f\"  {'Model':8s}  {'t-stat':>8s}  {'p-value':>8s}  {'t>1.96':>7s}  {'t>3.00':>7s}\")\nfor name, res in hlz_results.items():\n    t = res[\"t_stat\"]\n    p = res[\"p_value\"]\n    t_str = f\"{t:.3f}\" if np.isfinite(t) else \"nan\"\n    p_str = f\"{p:.4f}\" if np.isfinite(p) else \"nan\"\n    t196_str = \"YES\" if res[\"passes_t196\"] else \"NO\"\n    t300_str = \"YES\" if res[\"passes_t300\"] else \"NO\"\n    print(f\"  {name:8s}  {t_str:>8s}  {p_str:>8s}  \"\n          f\"{t196_str:>7s}  {t300_str:>7s}\")\n\nn_passing_t300 = sum(1 for r in hlz_results.values() if r[\"passes_t300\"])\nprint(f\"\\n  N models passing t > 3.00: {n_passing_t300}/{len(hlz_results)}\")\nprint(\"  (t > 3.0 recommended for strategies with many trials)\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Zero out of three models clear t = 3.0. Zero clear even t = 1.96. The GBM model's t = 1.387 is the highest, and it does not even reach the traditional 5% significance level. This is the multiple testing reality: a model can maintain its relative rank (low PBO) while producing a signal indistinguishable from noise (low t-stat)."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "raw_pvalues = np.array([\n    hlz_results[m][\"p_value\"]\n    for m in model_names\n    if np.isfinite(hlz_results[m][\"p_value\"])\n])\nmodel_names_finite = [\n    m for m in model_names\n    if np.isfinite(hlz_results[m][\"p_value\"])\n]\n\nreject, p_adj, _, _ = multipletests(raw_pvalues, method=\"fdr_bh\")\n\nprint(\"\\nBHY (Benjamini-Hochberg-Yekutieli) Multiple Testing Correction\")\nprint(f\"  {'Model':8s}  {'Raw p':>8s}  {'BHY p':>8s}  {'p≥raw?':>8s}  {'Reject?':>8s}\")\nfor m, p_raw, p_bhy, rej in zip(model_names_finite, raw_pvalues, p_adj, reject):\n    print(f\"  {m:8s}  {p_raw:>8.4f}  {p_bhy:>8.4f}  \"\n          f\"{'YES' if p_bhy >= p_raw - 1e-12 else 'NO':>8s}  \"\n          f\"{'YES' if rej else 'NO':>8s}\")\n\nrng = np.random.default_rng(42)\nn_fake = 50\nfake_ic = rng.normal(0, 0.07, (len(final_dates), n_fake))\nfake_tstat = (fake_ic.mean(axis=0)\n              / (fake_ic.std(axis=0) / np.sqrt(len(final_dates))))\nfake_pvals = 2 * (1 - stats.norm.cdf(np.abs(fake_tstat)))\n\nreject_nominal, p_adj_bhy, _, _ = multipletests(fake_pvals, method=\"fdr_bh\")\nn_fp_nominal = (fake_pvals < 0.05).sum()\nn_fp_bhy = reject_nominal.sum()\n\nprint(f\"\\n  50-variant FDR simulation (null: IC ~ N(0, 0.07))\")\nprint(f\"  False positives at nominal p<0.05: {n_fp_nominal}/{n_fake}\")\nprint(f\"  False positives after BHY correction: {n_fp_bhy}/{n_fake}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "All BHY-adjusted p-values are inflated relative to the raw p-values -- the correction always makes it harder to reject the null, never easier. The 50-variant simulation confirms the harshness of the environment: with only 68 months of data and an IC standard deviation of 0.07, even random draws rarely produce spurious significance. In a larger universe with longer history, spurious significance becomes far more common, which is precisely why Harvey-Liu-Zhu raised the bar."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "fig_hist, ax_hist = plt.subplots(figsize=(8, 5))\nax_hist.hist(cpcv_df[\"oos_ic_winner\"], bins=15, color=\"#4C72B0\", alpha=0.8,\n             edgecolor=\"white\")\nmedian_val = cpcv_df[\"oos_ic_winner\"].median()\nax_hist.axvline(median_val, color=\"firebrick\", lw=2, linestyle=\"--\",\n                label=f\"Median = {median_val:.4f}\")\nax_hist.axvline(0, color=\"gray\", lw=1.5, linestyle=\":\")\nax_hist.set(\n    title=\"CPCV: OOS IC Distribution for IS-Winner\",\n    xlabel=\"OOS Mean IC\",\n    ylabel=\"Path Count\",\n)\nax_hist.legend()\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The histogram shows visible spread in OOS IC across the 15 paths -- from negative (the IS winner underperformed OOS) to moderately positive. Single-path walk-forward gives you one number with no sense of its variability. CPCV gives you a distribution, and the width tells you how unstable the strategy's performance is across different data partitions."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "oos_ranks = cpcv_df[\"oos_rank_winner\"].values\nmedian_rank = float(np.median(oos_ranks))\n\nfig_pbo, ax_pbo = plt.subplots(figsize=(8, 4))\nax_pbo.hist(oos_ranks, bins=np.arange(0.5, len(model_names) + 1.5, 1),\n            color=\"#55A868\", alpha=0.8, edgecolor=\"white\")\nax_pbo.axvline(median_rank, color=\"firebrick\", lw=2, linestyle=\"--\",\n               label=f\"Median rank = {median_rank:.1f}\")\nbelow_mask = oos_ranks > median_rank\nax_pbo.fill_between(\n    [median_rank, len(model_names) + 0.5],\n    0, ax_pbo.get_ylim()[1] if ax_pbo.get_ylim()[1] > 0 else 5,\n    alpha=0.15, color=\"red\", label=f\"Below median → PBO contribution\"\n)\nax_pbo.set(\n    title=f\"OOS Rank of IS-Winner Across 15 CPCV Paths (PBO = {pbo:.2f})\",\n    xlabel=\"OOS Rank of IS-Winner (1 = best)\",\n    ylabel=\"Number of Paths\",\n)\nax_pbo.legend()\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Most of the mass is at rank 1 -- meaning the in-sample winner usually maintained its lead out-of-sample. The red-shaded region represents paths where the IS winner fell below the median rank. With PBO = 0.267, the IS winner underperforms in about 27% of paths -- a reasonably healthy result indicating the ranking is not purely noise."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "model_labels = list(hlz_results.keys())\nt_stats = [hlz_results[m][\"t_stat\"] for m in model_labels]\n\nfig_hlz, ax_hlz = plt.subplots(figsize=(8, 4))\ny_pos = range(len(model_labels))\ncolors = [\"#4C72B0\" if t <= 3.0 else \"#C44E52\" for t in t_stats]\nax_hlz.barh(y_pos, t_stats, color=colors, alpha=0.85, edgecolor=\"white\")\nax_hlz.axvline(1.96, color=\"orange\", lw=2, linestyle=\"--\", label=\"t = 1.96 (5%)\")\nax_hlz.axvline(3.00, color=\"firebrick\", lw=2, linestyle=\"-\", label=\"t = 3.00 (HLZ)\")\nax_hlz.set_yticks(y_pos)\nax_hlz.set_yticklabels(model_labels)\nax_hlz.set(\n    title=\"Harvey-Liu-Zhu t-stat: 3 Model Variants\",\n    xlabel=\"t-statistic\",\n)\nax_hlz.legend()\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "All three model bars fall well short of both threshold lines. The orange dashed line at t = 1.96 is the traditional significance level; the solid red line at t = 3.0 is the Harvey-Liu-Zhu threshold. This is the visual punchline: the strategy that \"looked promising\" in walk-forward evaluation does not survive even the most basic significance test. We can now evaluate whether a strategy's performance is statistically real. The next question is: even if it is real gross, does it survive the cost of trading?"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 4: Transaction Cost Decomposition -- From Gross to Net Returns\n\nA strategy showing 30% gross annual return commonly survives backtesting. The same strategy delivering 14% net return after costs does not match the fund's hurdle rate. The gap is not \"slippage.\" It is physics: the market impact of moving money, the bid-ask tax on every trade, and the drift between your signal's price and your fill price. Transaction costs decompose into three canonical components: the **bid-ask spread** (the half-spread tax on each side of a round trip), **market impact** (the Almgren-Chriss square-root model: $\\eta \\cdot \\sigma \\cdot \\sqrt{\\text{participation rate}}$), and **slippage** (latency-driven drift). The multiplier that converts per-trade costs into annual costs is **turnover** -- how much of the portfolio changes each month."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "ls = load_ls_portfolio()\ngross_returns = ls[\"gross_returns\"]\nturnover = ls[\"turnover\"]\nweights_df = ls[\"weights\"]\n\ncommon_dates = gross_returns.index.intersection(turnover.index)\ngross_returns = gross_returns.loc[common_dates]\nturnover = turnover.loc[common_dates]\n\nmean_turnover = turnover.mean()\nmax_turnover = turnover.max()\n\nprint(f\"Mean one-way monthly turnover: {mean_turnover:.1%}\")\nprint(f\"Max one-way monthly turnover:  {max_turnover:.1%}\")\n\nif mean_turnover > 0.50:\n    drag_monthly = mean_turnover * 2 * (5 / 10_000)\n    drag_annual = drag_monthly * 12\n    print(f\"⚠ HIGH TURNOVER: {mean_turnover:.0%} one-way — \"\n          f\"TC drag ≈ {drag_annual:.2%}/year at 5 bps (optimistic)\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Mean one-way monthly turnover of 140%. That means nearly every position is replaced every month. This is the \"physics\" of a monthly cross-sectional signal: when alpha scores re-rank 449 stocks, the top and bottom deciles reshuffle substantially. Even at an optimistic 5 bps spread, the annual TC drag is approximately 1.7%. At a more realistic 10-15 bps for a mixed large/mid-cap universe, the drag doubles or triples.\n\nNow let us build the net return series under three cost regimes: zero (the naive backtest), a fixed 5 bps spread, and a market-cap-tiered spread (10 bps large-cap, 20 bps mid-cap, 30 bps small-cap)."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def net_returns_from_spread(gross: pd.Series, to: pd.Series,\n                             cost_bps: float) -> pd.Series:\n    \"\"\"Subtract round-trip spread cost from gross returns.\"\"\"\n    cost_frac = cost_bps / 10_000\n    costs = to * 2 * cost_frac\n    return (gross - costs).rename(\"net_return\")\n\nreturns_zero = gross_returns.rename(\"zero_tc\")\nreturns_fixed = net_returns_from_spread(gross_returns, turnover, cost_bps=5.0)\n\nmcap_tiers = load_mcap_tiers()\nspread_map = {\"large\": 10.0, \"mid\": 20.0, \"small\": 30.0}\n\ntiered_spreads = []\nfor date in common_dates:\n    if date not in weights_df.index:\n        tiered_spreads.append(np.nan)\n        continue\n    w = weights_df.loc[date]\n    active_tickers = w[w.abs() > 1e-8].index\n    spreads = active_tickers.map(\n        lambda t: spread_map.get(mcap_tiers.get(t, \"mid\"), 15.0)\n    )\n    avg_spread = np.mean(spreads) if len(spreads) > 0 else 15.0\n    tiered_spreads.append(avg_spread)\n\ntiered_spread_series = pd.Series(tiered_spreads, index=common_dates)\n\ntiered_costs = turnover * 2 * (tiered_spread_series / 10_000)\nreturns_tiered = (gross_returns - tiered_costs).rename(\"tiered_tc\")\n\nspread_drag_fixed = (turnover * 2 * (5 / 10_000)).mean() * 12",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The formula is straightforward: round-trip cost = turnover times 2 (both legs of the trade) times the half-spread in decimal form. At 140% one-way turnover and 10 bps spread, the monthly drag is $1.40 \\times 2 \\times 0.001 = 0.28\\%$ per month, or about 3.4% per year. These cost estimates (10/20/30 bps by market-cap tier) are consistent with practitioner data from frec.com's 2023 direct indexing analysis and academic studies by Anderson at UCLA."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Market Impact: The Square-Root Law\n\nNow let us add market impact -- the permanent and temporary price shift caused by trades hitting the market. The Almgren-Chriss square-root model says impact scales with volatility and the square root of the participation rate. At 5% participation, impact is moderate. At 20%, it can exceed the spread cost entirely."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "ASSUMED_AUM = 100_000_000\n\ndef _compute_impact_one_period(\n    weights_df, vol_30d, dollar_vol, date, prev_date, i\n):\n    \"\"\"Compute Almgren-Chriss sqrt-law impact for one rebalance period.\n\n    Returns dict with keys 'date' and 'impact_cost'.\n    \"\"\"\n    delta_w = (weights_df.iloc[i] - weights_df.iloc[i - 1]).abs()\n    active = delta_w[delta_w > 1e-8].index\n\n    if len(active) == 0:\n        return {\"date\": date, \"impact_cost\": 0.0}\n\n    available_days = vol_30d.index[vol_30d.index <= date]\n    if len(available_days) == 0:\n        return {\"date\": date, \"impact_cost\": 0.0}\n    nearest_day = available_days[-1]\n\n    sigma = vol_30d.loc[nearest_day, active].dropna()\n    adv = dollar_vol.loc[nearest_day, active].dropna()\n    common_t = sigma.index.intersection(adv.index)\n\n    if len(common_t) == 0:\n        return {\"date\": date, \"impact_cost\": 0.0}\n\n    dw = delta_w[common_t]\n    sig = sigma[common_t]\n    adv_t = adv[common_t].replace(0, np.nan).dropna()\n    common_t2 = dw.index.intersection(sig.index).intersection(adv_t.index)\n\n    if len(common_t2) == 0:\n        return {\"date\": date, \"impact_cost\": 0.0}\n\n    trade_dollars = dw[common_t2] * ASSUMED_AUM\n    participation = (trade_dollars / adv_t[common_t2]).clip(upper=1.0)\n    impact_per_stock = 0.1 * sig[common_t2] * np.sqrt(participation)\n    total_impact = float((impact_per_stock * dw[common_t2]).sum())\n    return {\"date\": date, \"impact_cost\": total_impact}",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Each rebalance period, we compute the participation rate for each stock being traded, apply the square-root impact formula with $\\eta = 0.1$ (lower end of the 0.1-0.2 production range), and sum the impact across all traded stocks. The \\$100M AUM assumption is standard for research-grade analysis -- at institutional scale (\\$1B+), participation rates increase and impact costs can double or triple."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def compute_market_impact(\n    weights_df: pd.DataFrame,\n    ohlcv: pd.DataFrame,\n    impact_coeff: float = 0.1,\n) -> pd.Series:\n    \"\"\"Almgren-Chriss sqrt-law market impact: eta x sigma x sqrt(participation_rate).\n\n    participation_rate = (|dw| x AUM) / ADV_dollars, clipped to [0, 1].\n    Total impact = sum_i impact_per_stock_i x |dw_i|.\n\n    Args:\n        weights_df: (dates x tickers) portfolio weight matrix.\n        ohlcv: MultiIndex DataFrame (field, ticker) daily OHLCV.\n        impact_coeff: eta in the impact formula (default 0.1).\n\n    Returns:\n        Series of total market impact cost per rebalance period.\n    \"\"\"\n    try:\n        close = ohlcv.xs(\"Close\", level=0, axis=1)\n        volume = ohlcv.xs(\"Volume\", level=0, axis=1)\n    except KeyError:\n        return pd.Series(0.0, index=weights_df.index[1:], name=\"impact_cost\")\n\n    dollar_vol = (close * volume).rolling(30).mean()\n\n    daily_ret = close.pct_change()\n    vol_30d = daily_ret.rolling(30).std()\n\n    dates = weights_df.index\n    impact_list = [\n        _compute_impact_one_period(weights_df, vol_30d, dollar_vol,\n                                   dates[i], dates[i - 1], i)\n        for i in range(1, len(dates))\n    ]\n\n    if not impact_list:\n        return pd.Series(dtype=float, name=\"impact_cost\")\n    df = pd.DataFrame(impact_list).set_index(\"date\")\n    df.index = pd.DatetimeIndex(df.index)\n    return df[\"impact_cost\"]\n\nprint(\"Computing market impact costs from OHLCV data...\")\nohlcv = load_ohlcv_data()\nimpact_costs = compute_market_impact(weights_df, ohlcv)\n\nimpact_costs = impact_costs.reindex(common_dates).fillna(0.0)\nmean_monthly_impact = impact_costs.mean()\nprint(f\"Mean monthly market impact cost: {mean_monthly_impact:.4%}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The mean monthly impact cost adds approximately 0.08% per month to the cost burden -- on top of the spread cost. At institutional scale with higher AUM and $\\eta = 0.15$-$0.20$, impact costs would be 2-3x higher and could exceed spread costs entirely.\n\nNow the money plot: three equity curves showing the progressive drag of transaction costs."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "cum_zero = (1 + returns_zero).cumprod()\ncum_fixed = (1 + returns_fixed).cumprod()\ncum_tiered = (1 + returns_tiered).cumprod()\n\nfig1, ax1 = plt.subplots(figsize=(12, 6))\nax1.plot(cum_zero.index, cum_zero.values, label=\"Zero TC (gross)\", linewidth=2)\nax1.plot(cum_fixed.index, cum_fixed.values, label=\"Fixed 5 bps spread (optimistic)\", linewidth=2)\nax1.plot(cum_tiered.index, cum_tiered.values,\n         label=\"Tiered spread (10/20/30 bps by cap)\", linewidth=2)\nax1.set(\n    title=\"Transaction Cost Regimes: Cumulative Returns\",\n    xlabel=\"Date\",\n    ylabel=\"Cumulative Return ($ per $1 invested)\",\n)\nax1.legend()\nax1.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Watch the three curves diverge. The gross return reaches about \\$2.36 per dollar invested. The optimistic 5 bps regime reaches \\$2.15. The tiered regime reaches \\$1.79 -- a 28% reduction in Sharpe from gross to tiered net, and we have not even accounted for market impact yet. In production, firms report 40-60% live Sharpe reduction relative to backtest. Our 28% is at the conservative end because we use $\\eta = 0.1$ (lower bound) and \\$100M AUM (modest)."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "annual_spread_cost = (turnover * 2 * (5 / 10_000)).mean() * 12\nannual_impact_cost = impact_costs.mean() * 12\n\nfig2, ax2 = plt.subplots(figsize=(7, 5))\ncomponents = [\"Spread Cost (5 bps fixed)\", \"Market Impact (η=0.1)\"]\nvalues = [annual_spread_cost * 100, annual_impact_cost * 100]\nbars = ax2.bar(components, values, color=[\"steelblue\", \"darkorange\"])\nax2.bar_label(bars, fmt=\"%.2f%%\")\nax2.set(\n    title=\"Annual TC Decomposition (Average Month × 12)\",\n    xlabel=\"Cost Component\",\n    ylabel=\"Annual Cost (%)\",\n)\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Spread costs dominate in normal markets, but market impact surges during stress periods. March 2020 -- the COVID crash -- is the highest-cost month in our dataset, dominated by impact rather than spread. When volatility spikes, the $\\sigma$ term in the impact formula explodes, and trading becomes dramatically more expensive. An execution quant at a systematic fund runs exactly this kind of Transaction Cost Analysis (TCA) every day, comparing actual fill prices to VWAP and arrival price benchmarks.\n\nThe square-root impact model has a distinctive concave shape: the marginal cost of trading decreases as you trade more, but the total cost still increases. Let us visualize this relationship."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "participation_rates = np.linspace(0.01, 0.20, 50)\ntry:\n    close_prices = ohlcv.xs(\"Close\", level=0, axis=1)\n    median_sigma = close_prices.pct_change().std().median()\nexcept (KeyError, IndexError) as exc:\n    import warnings\n    warnings.warn(f\"Could not extract close prices for participation chart: {exc}\")\n    median_sigma = 0.015\n\neta = 0.1\nimpact_curve = eta * median_sigma * np.sqrt(participation_rates)\n\nfig3, ax3 = plt.subplots(figsize=(8, 5))\nax3.plot(participation_rates * 100, impact_curve * 100, color=\"darkorange\", linewidth=2)\nax3.axvline(x=5, color=\"gray\", linestyle=\"--\", alpha=0.7, label=\"5% participation\")\nax3.set(\n    title=f\"Market Impact vs. Participation Rate (η={eta}, σ={median_sigma:.3f})\",\n    xlabel=\"Participation Rate (%)\",\n    ylabel=\"Market Impact Cost (%)\",\n)\nax3.legend()\nax3.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The curve rises steeply at low participation rates and flattens at higher ones. At 5% participation (marked by the vertical line), impact is moderate. The vertical line at 5% is a common institutional threshold -- beyond that, most execution desks flag the trade as \"large block\" and switch to VWAP or TWAP algorithms. Transaction costs are the most common cause of the live-to-backtest performance gap. Now let us build the tool that quantifies whether a strategy's performance is statistically genuine even before costs: the deflated Sharpe ratio."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 5: The Deflated Sharpe Ratio -- When Significance Is Not Significant\n\nYour strategy has a Sharpe ratio of 0.88. How impressive is that? The answer depends entirely on three things: how long the backtest is, how non-normal the returns are, and how many strategies you tried before selecting this one. The **deflated Sharpe ratio** (DSR) adjusts for all three, and it often reveals that the \"impressive\" number is indistinguishable from a lucky draw from noise.\n\nThe key insight: if you test M strategy variants on the same dataset, the expected maximum Sharpe ratio across M independent noise processes grows with M. Your observed Sharpe of 0.88 needs to exceed this \"noise ceiling\" by enough to clear a statistical threshold. DSR computes the probability that it does, accounting for the number of trials, the backtest length, and the non-normality of returns."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "ls = load_ls_portfolio()\ngross_returns = ls[\"gross_returns\"].dropna()\nturnover = ls[\"turnover\"].dropna()\n\nprint(f\"Gross return series: {len(gross_returns)} months, \"\n      f\"{gross_returns.index[0].date()} – {gross_returns.index[-1].date()}\")\n\ngross_sr = sharpe_ratio(gross_returns, periods_per_year=12)\ngross_monthly_sr = gross_returns.mean() / gross_returns.std()\ngross_skew = float(stats.skew(gross_returns))\ngross_kurt = float(stats.kurtosis(gross_returns, fisher=True))\n\nprint(f\"\\nGross return moments (n={len(gross_returns)}):\")\nprint(f\"  Annualised Sharpe:  {gross_sr:.4f}\")\nprint(f\"  Monthly Sharpe:     {gross_monthly_sr:.4f}\")\nprint(f\"  Skewness:           {gross_skew:.4f}\")\nprint(f\"  Excess kurtosis:    {gross_kurt:.4f}\")\nif abs(gross_skew) > 1.0 or abs(gross_kurt) > 3.0:\n    print(\"  ⚠ Heavy tails / significant skew — non-normality penalises DSR\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Excess kurtosis of 4.23 -- heavy tails. Negative skewness of -0.26 -- occasional large losses. These numbers are typical for monthly equity long-short returns, not extreme. But they have devastating consequences for the DSR. The formula's denominator includes a term that inflates estimation uncertainty when returns are non-normal. With excess kurtosis of 4.23, this term is roughly 50% larger than it would be under Gaussian assumptions, which means you need proportionally more data to achieve the same confidence.\n\nLet us first compute the Probabilistic Sharpe Ratio (PSR) -- the special case with M=1 -- then build the full DSR surface."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "n_total = len(gross_returns)\npsr = deflated_sharpe_ratio(\n    observed_sr=gross_monthly_sr,\n    n_trials=1,\n    n_obs=n_total,\n    skew=gross_skew,\n    excess_kurt=gross_kurt,\n)\n\nprint(f\"\\nProbabilistic Sharpe Ratio (M=1, T={n_total} months):\")\nprint(f\"  PSR = {psr:.4f}  (using monthly SR={gross_monthly_sr:.4f})\")\nprint(f\"  Interpretation: {psr*100:.1f}% probability the SR is genuine (single trial)\")\n\nmonthly_sr = gross_monthly_sr\nret_skew = gross_skew\nret_kurt = gross_kurt\nsr_label = \"Gross (monthly)\"\n\nprint(f\"\\nDSR surface input ({sr_label}):\")\nprint(f\"  Monthly SR = {monthly_sr:.4f}, skew = {ret_skew:.4f}, excess kurt = {ret_kurt:.4f}\")\n\nT_WINDOWS = [24, 36, 48, 60, 84, 120]\nM_VALUES = [1, 5, 10, 20, 50]\n\nsurface_rows = []\n\nfor T in T_WINDOWS:\n    for M in M_VALUES:\n        dsr = deflated_sharpe_ratio(\n            observed_sr=monthly_sr,\n            n_trials=M,\n            n_obs=T,\n            skew=ret_skew,\n            excess_kurt=ret_kurt,\n        )\n        surface_rows.append({\n            \"T\": T,\n            \"M\": M,\n            \"dsr\": dsr if (dsr is not None and np.isfinite(dsr)) else 0.0,\n        })\n\ndsr_df = pd.DataFrame(surface_rows)\ndsr_pivot = dsr_df.pivot(index=\"M\", columns=\"T\", values=\"dsr\")\ndsr_pivot.index.name = \"M (trials)\"\ndsr_pivot.columns.name = \"T (months)\"\n\nprint(\"\\nDSR surface (rows=M, cols=T):\")\nprint(dsr_pivot.round(3).to_string())",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Six track record lengths times five trial counts = 30 cells in the DSR surface. The gradient tells the entire story: more trials or shorter track records push DSR toward zero. At M=50 with only 24 months of data, the strategy's observed Sharpe has only a 16% probability of being genuine. At M=10 with 48 months, the probability is about 56% -- essentially a coin flip."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "fig_heatmap, ax_hm = plt.subplots(figsize=(9, 5))\n\nim = ax_hm.imshow(\n    dsr_pivot.values,\n    aspect=\"auto\",\n    cmap=\"RdYlGn\",\n    vmin=0.0,\n    vmax=1.0,\n    origin=\"lower\",\n)\nplt.colorbar(im, ax=ax_hm, label=\"DSR\")\n\nax_hm.set_xticks(range(len(T_WINDOWS)))\nax_hm.set_xticklabels([f\"{t}m\" for t in T_WINDOWS])\nax_hm.set_yticks(range(len(M_VALUES)))\nax_hm.set_yticklabels([str(m) for m in M_VALUES])\nax_hm.set_xlabel(\"Track record length T (months)\")\nax_hm.set_ylabel(\"Number of strategies tested M\")\nax_hm.set_title(f\"Deflated Sharpe Ratio Surface  [monthly SR={monthly_sr:.3f}, {sr_label}]\")\n\nfor i, m in enumerate(M_VALUES):\n    for j, t in enumerate(T_WINDOWS):\n        val = dsr_pivot.loc[m, t]\n        ax_hm.text(j, i, f\"{val:.2f}\", ha=\"center\", va=\"center\", fontsize=8,\n                   color=\"black\" if 0.25 < val < 0.85 else \"white\")\n\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Red cells mean the strategy is statistically indistinguishable from noise at the corresponding (T, M) combination. Green cells mean it passes. The transition from red to green as you move right (longer track record) or down (fewer trials) makes the tradeoff explicit. A 6-month backtest of the winning strategy out of 50 variants has only a 4.5% probability of being genuine -- you are almost certainly presenting the best-fitting noise process.\n\nNow the sobering calculation: the **Minimum Track Record Length** (MinTRL). How many months of live data would this strategy need to confirm its observed Sharpe ratio at 95% confidence?"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def min_trl(\n    sharpe_monthly: float,\n    n_trials: int,\n    skew: float = 0.0,\n    excess_kurt: float = 0.0,\n    confidence: float = 0.95,\n) -> float:\n    \"\"\"Minimum Track Record Length (Bailey & Lopez de Prado, 2014).\n\n    Returns the minimum number of monthly observations needed for\n    DSR to exceed `confidence` given `n_trials` strategies tested.\n    Uses iterative search over T from 6 to 1200 months.\n\n    Args:\n        sharpe_monthly: per-period (monthly) Sharpe ratio — must match\n            the observation frequency of n_obs passed to DSR.\n    \"\"\"\n    for T in range(6, 1201):\n        dsr_val = deflated_sharpe_ratio(\n            observed_sr=sharpe_monthly,\n            n_trials=n_trials,\n            n_obs=T,\n            skew=skew,\n            excess_kurt=excess_kurt,\n        )\n        if dsr_val is not None and np.isfinite(dsr_val) and dsr_val >= confidence:\n            return float(T)\n    return float(\"inf\")\n\nsharpe_range = np.linspace(0.05, 0.60, 50)\nmintrl_values = [\n    min_trl(sr, n_trials=10, skew=ret_skew, excess_kurt=ret_kurt)\n    for sr in sharpe_range\n]\n\nobs_mintrl = min_trl(monthly_sr, n_trials=10, skew=ret_skew, excess_kurt=ret_kurt)\n\nprint(f\"\\nMinTRL at 95% confidence (M=10 strategies):\")\nprint(f\"  Observed monthly SR = {monthly_sr:.4f}\")\nprint(f\"  MinTRL = {obs_mintrl:.0f} months\")\n\nmintrl_test = min_trl(0.23, n_trials=10, skew=ret_skew, excess_kurt=ret_kurt)\nprint(f\"  MinTRL at monthly SR=0.23 (~ann 0.80) = {mintrl_test:.0f} months\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "MinTRL of 174 months -- 14.5 years -- at M=10 strategies and 95% confidence. Under Gaussian assumptions, this would be approximately 10 months. The excess kurtosis of 4.23 inflated it by roughly 10x. This is the single most powerful result in this section: **non-normality is not a footnote in the DSR formula; it is the dominant term.** Heavy tails in equity returns inflate the minimum track record by an order of magnitude. A strategy that would need 10 months of confirmation under normal returns needs 174 months when you account for the tails actually present in the data."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Implications for Strategy Evaluation Timelines\n\nThis has profound implications for strategy evaluation. Most strategies have 3-5 year track records (36-60 months). At M=10 trials, a MinTRL of 174 months means that nearly all strategies with typical track records cannot confirm their observed Sharpe ratios at 95% confidence. The MinTRL curve below makes this tradeoff explicit across a range of Sharpe ratios."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "fig_mintrl, ax_trl = plt.subplots(figsize=(8, 5))\n\nfinite_mask = np.isfinite(mintrl_values)\nax_trl.plot(sharpe_range[finite_mask], np.array(mintrl_values)[finite_mask],\n            color=\"steelblue\", linewidth=2, label=\"MinTRL (M=10, 95% conf.)\")\nax_trl.axhline(24, color=\"orange\", linestyle=\"--\", alpha=0.7, label=\"24 months (2 years)\")\nax_trl.axhline(36, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"36 months (3 years)\")\nax_trl.axvline(monthly_sr, color=\"green\", linestyle=\":\", alpha=0.9,\n               label=f\"This strategy monthly SR={monthly_sr:.3f}\")\n\nax_trl.set_xlabel(\"Monthly Sharpe Ratio\")\nax_trl.set_ylabel(\"Minimum Track Record Length (months)\")\nax_trl.set_title(\"MinTRL vs. Monthly Sharpe Ratio at 95% Confidence (M=10)\")\nax_trl.legend(fontsize=9)\nax_trl.set_xlim(0.05, 0.60)\nax_trl.set_ylim(0, 600)\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The MinTRL curve drops rapidly with increasing Sharpe. The horizontal lines at 24 and 36 months represent typical fund evaluation horizons. Most realistic strategies (monthly SR in the 0.10-0.25 range) require track records far longer than what most funds have. We now have the full toolkit: CV methodology, TC accounting, statistical correction for search. The final section shows what a responsible, integrated backtest looks like."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 6: The Responsible Backtest -- Putting It All Together\n\nMost backtests you will encounter in the wild violate at least three of the principles covered this week. A responsible backtest is not just \"more conservative\" -- it is a fundamentally different artifact, built to estimate live performance rather than to impress a committee. Here is the discipline pipeline in full: data hygiene check $\\rightarrow$ purged CV $\\rightarrow$ net-of-cost returns $\\rightarrow$ DSR evaluation $\\rightarrow$ sub-period stability check $\\rightarrow$ honest reporting. We are going to run the same strategy through two parallel evaluations -- naive and responsible -- and measure the gap."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "_alpha = load_alpha_output()\n_predictions = _alpha[\"predictions\"]\n_ic_series = _alpha[\"ic_series\"]\n\n_ls = load_ls_portfolio()\n_gross_returns = _ls[\"gross_returns\"]\n_turnover = _ls[\"turnover\"]\n\n_tc = pd.read_parquet(CACHE_DIR / \"tc_results.parquet\")\n_net_returns_tiered = _tc[\"net_return_tiered\"]\n\n_wf_ic_folds = pd.read_parquet(CACHE_DIR / \"wf_ic.parquet\")[\"wf_ic\"]\n_purged_ic_folds = pd.read_parquet(CACHE_DIR / \"purged_ic.parquet\")[\"purged_ic\"]\n\n_oos_start = _gross_returns.index[0]\n_oos_end = _gross_returns.index[-1]\n\n_sub1_end = pd.Timestamp(\"2021-11-30\")\n_sub2_start = pd.Timestamp(\"2021-12-31\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We load everything from the upstream caches: alpha predictions, portfolio returns, transaction cost results, and cross-validation IC series. The sub-period boundary splits the OOS window roughly in half -- the first half dominated by post-2019 momentum and COVID recovery, the second half including the 2022 rate shock and AI-driven concentration.\n\nThe naive evaluation uses gross returns, walk-forward CV (no purging), and no DSR filter. This is what an uncritical researcher reports."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "gross_returns = _gross_returns.copy()\n\nnaive_sharpe = sharpe_ratio(gross_returns, periods_per_year=12)\nnaive_ret_ann = gross_returns.mean() * 12\nnaive_mdd = max_drawdown(gross_returns)\nnaive_skew = float(stats.skew(gross_returns.dropna()))\nnaive_kurt = float(stats.kurtosis(gross_returns.dropna(), fisher=True))\n\ntss_mean_ic = float(_wf_ic_folds.mean())\ntss_std_ic = float(_wf_ic_folds.std())\n\nprint(\"── IS / OOS BOUNDARY ─────────────────────────────────────────\")\nprint(f\"  IS period:  pre-April 2019 (model training in Week 4)\")\nprint(f\"  OOS period: {_oos_start.date()} to {_oos_end.date()} ({len(gross_returns)} months)\")\nprint()\nprint(\"── NAIVE EVALUATION (IS label; gross returns; no purging) ────\")\nprint(f\"  ann_return:      {naive_ret_ann:.4f}\")\nprint(f\"  ann_sharpe:      {naive_sharpe:.4f}\")\nprint(f\"  max_drawdown:    {naive_mdd:.4f}\")\nprint(f\"  skewness:        {naive_skew:.4f}\")\nprint(f\"  excess_kurtosis: {naive_kurt:.4f}\")\nprint(f\"  cv_mean_ic:      {tss_mean_ic:.4f}  (TimeSeriesSplit, no purging)\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Naive Sharpe of 0.877. That looks like a viable strategy -- annualized return in the high single digits, max drawdown of around -32%, IC of 0.02 from walk-forward CV. If you stopped here, you might deploy this strategy. Now the responsible version adds three corrections: net-of-cost returns, purged CV for IC estimation, and DSR at M=10."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "net_ret_series = _net_returns_tiered.reindex(_gross_returns.index).dropna()\nresp_sharpe = sharpe_ratio(net_ret_series, periods_per_year=12)\nresp_ret_ann = net_ret_series.mean() * 12\nresp_mdd = max_drawdown(net_ret_series)\nresp_skew = float(stats.skew(net_ret_series.dropna()))\nresp_kurt = float(stats.kurtosis(net_ret_series.dropna(), fisher=True))\n\npurged_mean_ic = float(_purged_ic_folds.mean())\npurged_std_ic = float(_purged_ic_folds.std())\n\nn_obs = len(net_ret_series)\nresp_monthly_sr = net_ret_series.mean() / net_ret_series.std() if net_ret_series.std() > 0 else 0.0\ndsr_m10 = deflated_sharpe_ratio(\n    observed_sr=resp_monthly_sr,\n    n_trials=10,\n    n_obs=n_obs,\n    skew=resp_skew,\n    excess_kurt=resp_kurt,\n)\n\nsharpe_gap = naive_sharpe - resp_sharpe\n\nprint()\nprint(\"── RESPONSIBLE EVALUATION (OOS label; net-tiered; purged CV) ──\")\nprint(f\"  ann_return:      {resp_ret_ann:.4f}\")\nprint(f\"  ann_sharpe:      {resp_sharpe:.4f}\")\nprint(f\"  monthly_sharpe:  {resp_monthly_sr:.4f}\")\nprint(f\"  max_drawdown:    {resp_mdd:.4f}\")\nprint(f\"  skewness:        {resp_skew:.4f}\")\nprint(f\"  excess_kurtosis: {resp_kurt:.4f}\")\nprint(f\"  cv_mean_ic:      {purged_mean_ic:.4f}  (PurgedKFold)\")\nprint(f\"  dsr_m10:         {dsr_m10:.4f}\")\nprint(f\"  sharpe_gap (naive - resp): {sharpe_gap:.4f}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Responsible Sharpe of 0.575, down from 0.877. The gap of 0.302 -- a 34% reduction -- is the \"backtest-to-live performance gap\" that practitioners describe. The DSR of 0.414 triggers a NO-DEPLOY verdict (below the 0.5 threshold). The strategy that looked viable under naive evaluation does not survive responsible scrutiny. Lopez de Prado documented in 2017 that the average ML fund's live Sharpe ratio is approximately 40-60% of the backtest Sharpe ratio. Our 65.6% retention rate (0.575/0.877) is slightly better than the production benchmark, consistent with our use of research-grade rather than institutional-scale cost assumptions."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Sub-Period Stability: Is the Signal Decaying?\n\nLet us also check whether the signal is stable across time. A practitioner always splits the OOS period and checks for degradation -- a decaying signal is a different problem from a weak signal, and the remedy is different. A weak-but-stable signal may strengthen with better features; a decaying signal suggests the alpha is being arbitraged away or was regime-specific."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "ic_full = _ic_series[\"ic\"]\nic_sub1 = ic_full.loc[_oos_start:_sub1_end]\nic_sub2 = ic_full.loc[_sub2_start:_oos_end]\n\nic_sub1_mean = float(ic_sub1.mean())\nic_sub2_mean = float(ic_sub2.mean())\nic_sub1_std = float(ic_sub1.std())\nic_sub2_std = float(ic_sub2.std())\nic_degradation = ic_sub1_mean - ic_sub2_mean\n\nprint()\nprint(\"── SUB-PERIOD IC SPLIT (OOS stability check) ─────────────────\")\nprint(f\"  Sub-period 1 (OOS: {ic_sub1.index[0].date()} to {ic_sub1.index[-1].date()}):\")\nprint(f\"    n_months: {len(ic_sub1)}\")\nprint(f\"    mean_ic:  {ic_sub1_mean:.4f}  std_ic: {ic_sub1_std:.4f}\")\nprint(f\"  Sub-period 2 (OOS: {ic_sub2.index[0].date()} to {ic_sub2.index[-1].date()}):\")\nprint(f\"    n_months: {len(ic_sub2)}\")\nprint(f\"    mean_ic:  {ic_sub2_mean:.4f}  std_ic: {ic_sub2_std:.4f}\")\nprint(f\"  IC degradation (sub1 - sub2): {ic_degradation:.4f}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "First-half IC of 0.054 dropping to 0.039 in the second half -- a 27% decline, but below the 50% threshold that would trigger a signal decay warning. The signal is weakening modestly, likely driven by the regime shift from momentum-friendly markets (2019-2021) to rate-shock markets (2022-2024). Week 14 will introduce regime-conditional backtesting that handles exactly this kind of non-stationarity.\n\nNow the capstone visualization: naive versus responsible equity curves side by side."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "cum_naive = cumulative_returns(gross_returns)\nnet_aligned = net_ret_series.reindex(gross_returns.index).fillna(0.0)\ncum_resp = cumulative_returns(net_aligned)\n\nfig_curve, ax_curve = plt.subplots(figsize=(10, 5))\nax_curve.plot(cum_naive.index, cum_naive.values, label=\"Naive (gross, WF CV)\", lw=2)\nax_curve.plot(cum_resp.index, cum_resp.values,\n              label=\"Responsible (net-tiered, purged CV)\", lw=2, linestyle=\"--\")\nax_curve.axvline(x=_oos_start, color=\"gray\", lw=1.2, linestyle=\":\", label=\"OOS start\")\nax_curve.axvline(x=_sub1_end, color=\"lightcoral\", lw=1.0, linestyle=\":\",\n                 label=\"Sub-period split\")\nax_curve.set(\n    title=\"Naive vs. Responsible Equity Curve (IS/OOS labeled)\",\n    xlabel=\"Date\",\n    ylabel=\"Cumulative Return\",\n)\nax_curve.legend(fontsize=9)\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The gap between the two curves is the aggregate cost of honest methodology. The naive curve consistently outperforms because the naive evaluation includes gross returns (no costs) and walk-forward IC (no purging correction). Both curves show the same COVID drawdown in March 2020 -- costs do not help you during crashes, they make them worse."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "fold_indices = np.arange(len(_wf_ic_folds))\nbar_width = 0.35\n\nfig_bar, ax_bar = plt.subplots(figsize=(10, 5))\nax_bar.bar(fold_indices - bar_width / 2, _wf_ic_folds.values,\n           width=bar_width, label=\"Walk-Forward IC (naive)\", alpha=0.8)\nax_bar.bar(fold_indices + bar_width / 2, _purged_ic_folds.values,\n           width=bar_width, label=\"Purged CV IC (responsible)\", alpha=0.8)\nax_bar.axhline(0, color=\"black\", lw=0.8)\nax_bar.set(\n    title=\"Fold-Level IC: Walk-Forward vs. Purged CV (OOS folds)\",\n    xlabel=\"Fold Index\",\n    ylabel=\"IC (Spearman)\",\n    xticks=fold_indices,\n)\nax_bar.legend()\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The walk-forward and purged IC bars are nearly identical in height -- confirming that on monthly data with 1-month labels, the CV method gap is negligible in aggregate. But look at the individual folds: in some folds, the walk-forward IC is higher; in others, the purged IC is higher. This fold-level variation is why methodology choice affects which model you select, even when the average IC gap is tiny.\n\nThe gap between the naive and responsible evaluation is not a problem to solve -- it is the information. The DSR of 0.414 means the strategy's observed Sharpe has only a 41% probability of being genuine given that we tested 10 model variants. The verdict is NO-DEPLOY. The honest conclusion: continue paper trading for at least another year, or accumulate 14.5 years of live data to achieve 95% confidence -- a direct consequence of the heavy-tailed return distribution."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary: The Responsible Backtesting Toolkit\n\n| Concept | What it does | Key result |\n|---------|-------------|------------|\n| Look-ahead bias check | Detects future information leaking into signals | IC = 1.0 vs. -0.013 -- a 400 pp annual return gap |\n| Survivorship bias | Quantifies return inflation from survivor-only universe | ~3% annual premium (simulated) |\n| Purged k-fold CV | Removes label-contaminated training observations | IC delta near zero on monthly data; material on daily |\n| CPCV + PBO | Tests if IS winner maintains OOS rank | PBO = 0.267 (positive signal, but weak absolute IC) |\n| Harvey-Liu-Zhu threshold | Raises significance bar for multiple testing | 0/3 models clear t = 3.0 |\n| TC decomposition | Spread + impact + slippage from gross returns | 28% Sharpe reduction (gross 0.877 to net 0.624) |\n| Deflated Sharpe Ratio | Adjusts Sharpe for trials, length, non-normality | DSR = 0.414 at M=10 -- NO-DEPLOY |\n| MinTRL | Minimum months to confirm observed Sharpe | 174 months (14.5 years) -- non-normality is dominant |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Career Connections\n\n**Quantitative researcher at a multi-strategy hedge fund.** The first question asked of any new strategy submission is \"what's the gross and net Sharpe, and what's the DSR?\" Reports without TC accounting or overfitting correction are rejected at first review. The backtest discipline pipeline you just saw -- purged CV, cost decomposition, DSR evaluation, sub-period stability -- is the exact format of a strategy evaluation memo at firms like Two Sigma, D.E. Shaw, and Citadel.\n\n**ML engineer at a systematic fund.** You own the backtesting infrastructure. The purged CV splits, walk-forward engines, and transaction cost modules are production-grade library code that researchers consume. Your job is to ensure that the evaluation framework *cannot* accidentally produce leaky results. The PurgedKFoldDemo class in this lecture is a teaching version of the production-grade splitter you would maintain.\n\n**Execution quant / portfolio analytics.** You run Transaction Cost Analysis -- the post-trade measurement of actual implementation shortfall versus backtest assumptions. The TC decomposition in Section 4 is the pre-trade version of what you measure post-trade, every day, for every strategy the fund runs.\n\n**Model risk / validation analyst.** You audit backtests using exactly this framework -- checking for look-ahead bias, documentation of the search process, TC realism, and statistical significance under multiple testing correction. When a portfolio manager says \"the model shows a Sharpe of 1.4,\" your job is to ask \"what's the DSR, and how many variants did you test?\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Bridge to Next Week\n\nWeek 5 established how to evaluate a strategy honestly. Week 6 asks the next question: given a strategy that has survived rigorous evaluation, how should it be sized and combined with other strategies in a portfolio? Portfolio construction and risk management are the natural downstream of a trustworthy signal. The DSR and MinTRL tell you how much confidence to place in a strategy's forward Sharpe estimate. That confidence level determines the Kelly fraction -- the optimal position size given your uncertainty. A strategy with DSR of 0.414 and MinTRL of 174 months should receive a smaller capital allocation than one with DSR of 0.95 and MinTRL of 24 months. Week 6 makes that sizing decision quantitative.\n\n**Forward reference -- regime conditioning (Week 14).** The Seven Sins taxonomy identifies regime neglect as a failure mode, but this week does not resolve it. Regime-conditional backtesting requires the Hidden Markov Model and change-point detection machinery from Week 14. The backtesting pipeline built here is rigorous within a stationary assumption; Week 14 relaxes that assumption."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Suggested Reading\n\n- **Lopez de Prado (2018), *Advances in Financial Machine Learning*, Chapters 7-12.** The canonical text. Defines purged k-fold CV, CPCV, and the research discipline framework. Read this before any other backtest-related paper.\n\n- **Bailey & Lopez de Prado (2014), \"The Deflated Sharpe Ratio.\"** The DSR formula and the MinTRL calculation. Short, precise, and devastating. After reading this paper, you will never report a Sharpe ratio without its DSR again.\n\n- **Harvey, Liu & Zhu (2016), \"...And the Cross-Section of Expected Returns.\"** The t-stat hurdle of 3.0 for factor discovery. The paper that told the entire field that most of their published results were probably false discoveries.\n\n- **Almgren & Chriss (2000), \"Optimal Execution of Portfolio Transactions.\"** The canonical transaction cost decomposition. The square-root impact model is elegant, empirically validated, and still the industry standard 25 years later.\n\n- **Arian, Norouzi & Seco (2024), \"Backtest Overfitting in the Machine Learning Era.\"** The strongest current academic validation of the Lopez de Prado framework. Shows in controlled synthetic experiments that CPCV outperforms walk-forward in strategy selection."
  }
 ]
}