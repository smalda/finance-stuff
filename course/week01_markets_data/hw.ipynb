{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Week 1 Homework: Build the Fund's Data Pipeline\n\n> *\"Bad data is worse than no data. With no data, you know you're guessing. With bad data, you think you know.\"*\n\n## The Mission\n\nYou've been hired as the first data engineer at a small quantitative fund. There are three portfolio managers and a handful of researchers. They all need data. Right now, everyone downloads their own CSVs, cleans them (or doesn't), and stores them on their laptops. One PM has an Excel sheet with prices going back to 2015 that he swears is \"clean.\" Another researcher just discovered that her TSLA backtest included a phantom 80% crash on the day of the 2020 stock split. Nobody knows whose numbers to trust.\n\nYour job: build the fund's data pipeline \u2014 the single source of truth that every analyst and every model will depend on. This means downloading from a vendor API, cleaning the inevitable mess, running automated quality checks, storing everything in a format that doesn't waste everyone's time, and making it easy to load exactly the slice of data someone needs. If the data is wrong, every model downstream is wrong. Every backtest is wrong. Every risk report is wrong.\n\nThis isn't a software engineering exercise dressed up as finance. It's the actual first task a new quant data engineer faces at firms like Two Sigma, Citadel, or DE Shaw \u2014 except they're doing it for hundreds of thousands of instruments across asset classes, and you're starting with 50 US equities. The pipeline you build tonight is the foundation for every homework in this course. You'll reuse it in Weeks 2 through 6 and extend it further in Week 13.\n\nFour deliverables. One evening. Let's build something real.\n\n## Deliverables\n\n1. **`FinancialDataLoader` class** \u2014 The complete data lifecycle: download, clean, validate, store, and load. Handles 50+ tickers without crashing. This is the big build.\n\n2. **Data quality report** \u2014 Per-ticker metrics (completeness, stale prices, OHLC violations) with letter grades. Universe-level summary. Which tickers are production-ready and which would get you fired?\n\n3. **Storage format benchmark** \u2014 CSV vs. Parquet, pandas vs. Polars, at multiple scales. The numbers will tell you why the industry is migrating.\n\n4. **Return statistics summary** \u2014 Mean, volatility, skewness, kurtosis, tail behavior for 10 representative tickers. This is your bridge to Week 2 \u2014 the numbers here will break every Gaussian assumption your ML models were built on."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Setup \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import scipy.stats as scipy_stats\n",
    "import yfinance as yf\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from pandas.tseries.offsets import CustomBusinessDay\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "plt.rcParams[\"grid.alpha\"] = 0.3\n",
    "\n",
    "CACHE_DIR = Path(\"code/.cache/hw\")\n",
    "CACHE_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Deliverable 1: The `FinancialDataLoader` Class\n",
    "\n",
    "This is the centerpiece. You're building a class that handles the complete data lifecycle \u2014 downloading from yfinance, cleaning the gaps and anomalies, running automated quality checks, saving to Parquet, and loading back with flexible filtering. The class needs to handle at least 50 tickers without crashing, even when individual tickers fail (and some will).\n",
    "\n",
    "Think about what makes this different from a quick `yf.download()` call: error handling, quality scoring, format choice, metadata tracking. Every method should be defensive \u2014 flag problems, don't crash. A production pipeline that dies on one bad ticker out of 500 is useless.\n",
    "\n",
    "**Requirements:**\n",
    "- Download daily OHLCV for a configurable ticker list and date range\n",
    "- Forward-fill missing days up to a configurable limit; flag longer gaps\n",
    "- Compute per-ticker quality scores (completeness, OHLC consistency, stale prices)\n",
    "- Save to Parquet (wide and long formats) with sidecar metadata JSON\n",
    "- Load back with filtering by ticker, date range, and minimum quality grade\n",
    "\n",
    "Step 4 is where it gets interesting \u2014 you'll discover that some \"blue chip\" stocks have surprisingly messy histories, and that the quality gap between large-cap liquid names and everything else is wider than you'd expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Build the FinancialDataLoader class\n# Start with __init__ \u2014 what state does the loader need to track?\n# Then implement: download(), clean(), validate(), store(), load()\n# Hint: think about what `download()` should do when one ticker out of 50 fails. Should it crash? Should it silently drop the ticker? Should it record the failure and keep going? The right answer determines whether your pipeline is usable at scale."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Implement validate() \u2014 what metrics capture data quality?\n# Think: completeness, stale prices, OHLC consistency, long gaps\n\n# For storage, consider: when would you want wide format (tickers as columns) vs. long format (ticker as a row-level field)? Each serves different downstream use cases.\n# TODO: Implement store() and load() with Parquet + metadata\n# TODO: Test the full pipeline on 50 tickers"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## \u2501\u2501\u2501 SOLUTION: Deliverable 1 \u2501\u2501\u2501\n\nWe'll build the `FinancialDataLoader` incrementally \u2014 one method at a time, with design commentary between each piece. This is a class with six responsibilities (init, download, clean, validate, store, load), and each one involves decisions that matter downstream. The monkey-patching pattern lets us discuss each method in isolation before wiring them together.\n\nLet's start with the skeleton: what state does a data loader need to carry?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialDataLoader:\n",
    "    \"\"\"\n",
    "    Download, clean, validate, and store financial data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tickers : list of str\n",
    "        Stock ticker symbols to download.\n",
    "    start_date, end_date : str\n",
    "        Date range in YYYY-MM-DD format.\n",
    "    cache_dir : Path or str\n",
    "        Directory for storing downloaded data.\n",
    "    ffill_limit : int\n",
    "        Maximum consecutive days to forward-fill.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tickers, start_date, end_date,\n",
    "                 cache_dir=None, ffill_limit=5):\n",
    "        self.tickers = tickers\n",
    "        self.start_date = pd.Timestamp(start_date)\n",
    "        self.end_date = pd.Timestamp(end_date)\n",
    "        self.cache_dir = Path(cache_dir) if cache_dir else CACHE_DIR\n",
    "        self.cache_dir.mkdir(exist_ok=True, parents=True)\n",
    "        self.ffill_limit = ffill_limit\n",
    "        self.data = None\n",
    "        self.quality_scores = {}\n",
    "        self.failed_tickers = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three pieces of state worth noting: `data` (the actual DataFrame), `quality_scores` (a dict of per-ticker metrics we'll populate in `validate()`), and `failed_tickers` (the ones yfinance couldn't deliver). The `ffill_limit` parameter is a design choice \u2014 forward-filling 1-2 missing days is standard practice (holidays, data vendor hiccups), but filling a 30-day gap would be fabricating data. Five days is a reasonable default: it covers a week of missing data but won't silently paper over a delisting.\n",
    "\n",
    "Now the first real method: downloading. This is where most toy pipelines break at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def download(self):\n    \"\"\"Download OHLCV data, handling failures gracefully.\"\"\"\n    print(f\"Downloading {len(self.tickers)} tickers...\")\n    try:\n        self.data = yf.download(\n            self.tickers, start=self.start_date,\n            end=self.end_date, auto_adjust=False,\n            progress=False)\n        if len(self.tickers) == 1:\n            self.data = pd.concat(\n                [self.data], axis=1, keys=[self.tickers[0]]\n            ).swaplevel(axis=1)\n        print(f\"Downloaded {self.data.shape[0]} rows\")\n    except Exception as e:\n        print(f\"Download failed: {e}\")\n        raise\n\n    close = self.data[\"Close\"]\n    for t in self.tickers:\n        if t in close.columns and close[t].isna().all():\n            self.failed_tickers.append(t)\n    if self.failed_tickers:\n        print(f\"Failed: {self.failed_tickers}\")\n    return self\n\nFinancialDataLoader.download = download"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two critical design decisions here. First, `auto_adjust=False` \u2014 we want both raw Close and Adjusted Close so we can compute adjustment factors later. If you use `auto_adjust=True`, you lose the ability to detect splits and dividends programmatically. Second, the failure-detection loop: yfinance doesn't throw exceptions for individual ticker failures. It quietly returns all-NaN columns. The loop catches these and records them in `failed_tickers` so the rest of the pipeline knows which tickers to skip.\n",
    "\n",
    "The single-ticker special case is a yfinance quirk \u2014 when you pass one ticker, the DataFrame structure changes (no MultiIndex). The `concat` + `swaplevel` normalizes it back. Small thing, but it would cause a crash at 1 AM when someone runs the pipeline for a single-stock analysis.\n",
    "\n",
    "Next: cleaning. This is where we deal with the reality that financial calendars are messy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(self):\n",
    "    \"\"\"Forward-fill gaps within limit, flag long gaps.\"\"\"\n",
    "    if self.data is None:\n",
    "        raise ValueError(\"Must download() before clean()\")\n",
    "    print(\"Cleaning data...\")\n",
    "\n",
    "    us_bd = CustomBusinessDay(calendar=USFederalHolidayCalendar())\n",
    "    expected_dates = pd.date_range(\n",
    "        start=self.start_date, end=self.end_date, freq=us_bd\n",
    "    )\n",
    "    self.data = self.data.reindex(expected_dates)\n",
    "\n",
    "    for column in self.data.columns:\n",
    "        self.data[column] = self.data[column].ffill(\n",
    "            limit=self.ffill_limit\n",
    "        )\n",
    "    print(f\"Forward-filled gaps up to {self.ffill_limit} days\")\n",
    "    return self\n",
    "\n",
    "FinancialDataLoader.clean = clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reindex` call is doing important work. yfinance returns data on days the market was open \u2014 but \"open\" depends on which exchange, and sometimes there are half-days, early closes, or weather events. We generate the expected trading calendar using `USFederalHolidayCalendar` and reindex to it. Any days that yfinance missed become NaN, and we forward-fill up to the configured limit. Days that remain NaN after forward-filling are genuine long gaps \u2014 potential delistings, trading halts, or data vendor failures. Those stay NaN on purpose: they're signals, not noise.\n",
    "\n",
    "Now the validation engine \u2014 the method that makes this a data *pipeline* rather than a data *downloader*. This one is the longest, so we'll build it in two parts: first the per-ticker metric computation, then the grading function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def validate(self):\n    \"\"\"Compute quality scores for each ticker.\"\"\"\n    if self.data is None:\n        raise ValueError(\"Must download/clean before validate()\")\n    print(\"Validating data quality...\")\n\n    us_bd = CustomBusinessDay(calendar=USFederalHolidayCalendar())\n    expected = pd.date_range(\n        start=self.start_date, end=self.end_date, freq=us_bd)\n    n_exp = len(expected)\n    cols = {f: self.data[f] for f in\n            [\"Close\", \"Open\", \"High\", \"Low\", \"Volume\"]}\n\n    for ticker in self.tickers:\n        if ticker in self.failed_tickers:\n            self.quality_scores[ticker] = {\n                \"completeness\": 0.0, \"stale_days\": 0,\n                \"ohlc_violations\": 0, \"long_gaps\": 0,\n                \"grade\": \"F\"}\n            continue\n        self.quality_scores[ticker] = self._score_ticker(\n            ticker, cols, expected, n_exp)\n\n    print(f\"Scored {len(self.quality_scores)} tickers\")\n    return self\n\nFinancialDataLoader.validate = validate"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `validate()` method delegates the per-ticker work to `_score_ticker()`. This keeps the main loop readable: for each ticker, either mark it as failed or compute its scores. The helper method does the heavy lifting \u2014 four quality dimensions, each catching a different failure mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def _score_ticker(self, ticker, cols, expected, n_exp):\n    \"\"\"Compute quality metrics for a single ticker.\"\"\"\n    close_t = cols[\"Close\"][ticker]\n    n_actual = close_t.dropna().shape[0]\n    completeness = n_actual / n_exp\n\n    prices = close_t.dropna()\n    stale_days = int(\n        (prices.round(4) == prices.round(4).shift(1)).sum())\n\n    idx = prices.index\n    h, l = cols[\"High\"][ticker][idx], cols[\"Low\"][ticker][idx]\n    o, c = cols[\"Open\"][ticker][idx], close_t[idx]\n    v = cols[\"Volume\"][ticker][idx]\n    ohlc_v = int(((h<o)|(h<c)).sum() + ((l>o)|(l>c)).sum()\n                 + (v <= 0).sum())\n\n    td = close_t.reindex(expected)\n    m = td.isna()\n    runs = m.astype(int).groupby((m != m.shift()).cumsum()).sum()\n    long_gaps = int((runs > 5).sum())\n\n    grade = self._compute_grade(\n        completeness, stale_days, ohlc_v, long_gaps)\n    return {\"completeness\": completeness,\n            \"stale_days\": stale_days,\n            \"ohlc_violations\": ohlc_v,\n            \"long_gaps\": long_gaps, \"grade\": grade}\n\nFinancialDataLoader._score_ticker = _score_ticker"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four quality dimensions, each catching a different failure mode. **Completeness** measures how many of the expected trading days actually have data \u2014 anything below 99% means the ticker has missing history (late IPO, early delisting, or vendor gap). **Stale prices** catch consecutive identical closes, which can mean illiquidity or a data feed that stopped updating. **OHLC violations** flag rows where High < Open or Low > Close \u2014 these should be impossible by definition, and their presence means something went wrong in aggregation. **Long gaps** (more than 5 consecutive missing days) flag potential delistings or trading halts that forward-fill can't responsibly cover.\n",
    "\n",
    "The grading function translates these metrics into a letter grade that anyone at the fund can understand at a glance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_grade(self, completeness, stale_days,\n",
    "                   ohlc_violations, long_gaps):\n",
    "    \"\"\"Assign A/B/C/D/F grade based on quality metrics.\"\"\"\n",
    "    if (completeness >= 0.99 and stale_days <= 20\n",
    "            and ohlc_violations == 0 and long_gaps == 0):\n",
    "        return \"A\"\n",
    "    elif (completeness >= 0.95 and stale_days < 50\n",
    "          and ohlc_violations < 5 and long_gaps == 0):\n",
    "        return \"B\"\n",
    "    elif (completeness >= 0.90 and stale_days < 100\n",
    "          and ohlc_violations < 10):\n",
    "        return \"C\"\n",
    "    elif completeness >= 0.80:\n",
    "        return \"D\"\n",
    "    else:\n",
    "        return \"F\"\n",
    "\n",
    "FinancialDataLoader._compute_grade = _compute_grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grading thresholds encode real-world trade-offs. Grade A demands 99%+ completeness and zero OHLC violations \u2014 this is production-quality data you'd use for live trading signals. Grade B (95%+) is fine for backtesting research where small gaps won't materially change results. Grade C/D is \"use with caution\" territory \u2014 maybe acceptable for cross-sectional studies where one noisy ticker in 200 won't dominate the result. Grade F means the data is too broken to trust for anything.\n",
    "\n",
    "These thresholds aren't academic \u2014 at a real fund, a risk analyst would define similar cutoffs and refuse to compute NAV on any ticker below grade B. The exact numbers are tunable; what matters is that the thresholds exist, are documented, and are enforced programmatically.\n",
    "\n",
    "Now let's add the storage layer: saving to Parquet with metadata. We'll split this into two cells \u2014 one for the wide-format case (which is straightforward), and one for the long-format conversion plus the metadata save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store(self, format=\"wide\"):\n",
    "    \"\"\"Save data to Parquet with metadata.\"\"\"\n",
    "    if self.data is None or not self.quality_scores:\n",
    "        raise ValueError(\n",
    "            \"Must download/clean/validate before store()\"\n",
    "        )\n",
    "    print(f\"Storing data in {format} format...\")\n",
    "\n",
    "    if format == \"wide\":\n",
    "        data_path = self.cache_dir / \"data_wide.parquet\"\n",
    "        self.data.to_parquet(data_path)\n",
    "    elif format == \"long\":\n",
    "        data_path = self.cache_dir / \"data_long.parquet\"\n",
    "        self._store_long(data_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown format: {format}\")\n",
    "\n",
    "    self._save_metadata(format, data_path)\n",
    "    return self\n",
    "\n",
    "FinancialDataLoader.store = store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `store()` method delegates to two helpers: `_store_long()` for the wide-to-long conversion, and `_save_metadata()` for the sidecar JSON. This keeps the public API clean while handling the format-specific complexity behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def _store_long(self, data_path):\n    \"\"\"Convert wide data to long format and save.\"\"\"\n    long_data = []\n    for ticker in self.tickers:\n        if ticker in self.failed_tickers:\n            continue\n        cols = {\n            \"date\": self.data.index, \"ticker\": ticker,\n            \"open\": self.data[\"Open\"][ticker].values,\n            \"high\": self.data[\"High\"][ticker].values,\n            \"low\": self.data[\"Low\"][ticker].values,\n            \"close\": self.data[\"Close\"][ticker].values,\n            \"volume\": self.data[\"Volume\"][ticker].values,\n            \"adj_close\": self.data[\"Adj Close\"][ticker].values,\n        }\n        long_data.append(pd.DataFrame(cols))\n    long_df = pd.concat(long_data, ignore_index=True)\n    long_df.to_parquet(data_path, index=False)\n\nFinancialDataLoader._store_long = _store_long"
  },
  {
   "cell_type": "markdown",
   "source": "The long-format conversion iterates through tickers and stacks them vertically \u2014 each row becomes a single date-ticker observation. This is the natural shape for SQL-style queries and for Polars, which expects tidy data. The wide format is better for pandas cross-sectional operations (correlation matrices, relative momentum).\n\nNow the metadata sidecar \u2014 the provenance record that travels with the data.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_metadata(self, format, data_path):\n",
    "    \"\"\"Write sidecar JSON with provenance info.\"\"\"\n",
    "    metadata = {\n",
    "        \"tickers\": self.tickers,\n",
    "        \"start_date\": str(self.start_date.date()),\n",
    "        \"end_date\": str(self.end_date.date()),\n",
    "        \"download_date\": str(pd.Timestamp.now()),\n",
    "        \"source\": \"yfinance\",\n",
    "        \"failed_tickers\": self.failed_tickers,\n",
    "        \"quality_scores\": self.quality_scores,\n",
    "        \"format\": format,\n",
    "    }\n",
    "    meta_path = self.cache_dir / f\"metadata_{format}.json\"\n",
    "    with open(meta_path, \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"Saved to {data_path}\")\n",
    "    print(f\"Metadata: {meta_path}\")\n",
    "\n",
    "FinancialDataLoader._save_metadata = _save_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two format options, each serving a different use case. **Wide format** (tickers as column levels) is what you want for time-series analysis \u2014 slicing `data[\"Close\"]` gives you a DataFrame where each column is a ticker, perfect for cross-sectional computations like correlation matrices or relative momentum. **Long format** (ticker as a regular column) is what databases and Polars prefer \u2014 it's the natural shape for SQL-style queries like \"give me all rows where ticker='AAPL' and date > '2020-01-01'.\"\n",
    "\n",
    "The sidecar JSON metadata is a small but important detail. When someone loads this data in three months, they'll want to know: when was it downloaded? What source? Which tickers failed? What were the quality scores? Embedding this in the file system alongside the data means the provenance travels with the data, not in someone's memory.\n",
    "\n",
    "Last method: loading with flexible filtering. We'll split this into the core load and the filtering logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(self, tickers=None, start_date=None,\n",
    "         end_date=None, format=\"wide\", min_grade=None):\n",
    "    \"\"\"Load data from Parquet with optional filtering.\"\"\"\n",
    "    data_path = self.cache_dir / f\"data_{format}.parquet\"\n",
    "    meta_path = self.cache_dir / f\"metadata_{format}.json\"\n",
    "    if not data_path.exists():\n",
    "        raise FileNotFoundError(f\"No data at {data_path}\")\n",
    "\n",
    "    with open(meta_path) as f:\n",
    "        meta = json.load(f)\n",
    "    self.quality_scores = meta[\"quality_scores\"]\n",
    "    self.failed_tickers = meta[\"failed_tickers\"]\n",
    "\n",
    "    data = pd.read_parquet(data_path)\n",
    "    data = self._apply_filters(\n",
    "        data, tickers, start_date, end_date,\n",
    "        min_grade, format\n",
    "    )\n",
    "    self.data = data\n",
    "    print(f\"Loaded {len(data)} rows from {data_path}\")\n",
    "    return data\n",
    "\n",
    "FinancialDataLoader.load = load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `load()` method reads metadata first (restoring quality scores and failed-ticker info), then delegates all filtering to `_apply_filters()`. This separation makes it easy to add new filter dimensions later \u2014 by date, by sector, by minimum volume \u2014 without touching the core load logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_filters(self, data, tickers, start_date,\n",
    "                   end_date, min_grade, format):\n",
    "    \"\"\"Apply ticker, date, and quality filters.\"\"\"\n",
    "    def _by_tickers(df, tk_list):\n",
    "        if format == \"wide\":\n",
    "            ok = [t for t in tk_list if t in df[\"Close\"].columns]\n",
    "            return df.loc[:, (slice(None), ok)]\n",
    "        return df[df[\"ticker\"].isin(tk_list)]\n",
    "\n",
    "    if tickers:\n",
    "        data = _by_tickers(data, tickers)\n",
    "    for bound, op in [(start_date, \"ge\"), (end_date, \"le\")]:\n",
    "        if bound:\n",
    "            ts = pd.Timestamp(bound)\n",
    "            key = data.index if format == \"wide\" else data[\"date\"]\n",
    "            data = data[key >= ts] if op == \"ge\" else data[key <= ts]\n",
    "    if min_grade:\n",
    "        rank = {\"A\": 5, \"B\": 4, \"C\": 3, \"D\": 2, \"F\": 1}\n",
    "        good = [t for t, s in self.quality_scores.items()\n",
    "                if rank.get(s[\"grade\"], 0) >= rank.get(min_grade, 0)]\n",
    "        data = _by_tickers(data, good)\n",
    "    return data\n",
    "\n",
    "FinancialDataLoader._apply_filters = _apply_filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filtering supports three dimensions: tickers, date range, and quality grade. The quality-grade filter is the one that matters most in practice. Imagine a researcher says: \"Give me all grade-A data for the last 5 years.\" One line: `loader.load(min_grade=\"A\", start_date=\"2020-01-01\")`. No manual ticker lists, no guessing which stocks are clean. The quality metadata, computed once at validation time, serves as a permanent label.\n",
    "\n",
    "The class is complete. Let's run the full pipeline on 50 tickers and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tickers = [\n",
    "    \"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"META\", \"NVDA\", \"TSLA\",\n",
    "    \"NFLX\", \"JPM\", \"BAC\", \"WFC\", \"GS\", \"MS\", \"C\", \"BLK\",\n",
    "    \"JNJ\", \"UNH\", \"PFE\", \"LLY\", \"ABBV\", \"TMO\",\n",
    "    \"WMT\", \"HD\", \"PG\", \"KO\", \"PEP\", \"COST\", \"NKE\",\n",
    "    \"BA\", \"CAT\", \"GE\", \"MMM\", \"HON\",\n",
    "    \"XOM\", \"CVX\", \"COP\", \"SLB\",\n",
    "    \"SPY\", \"QQQ\", \"IWM\", \"DIA\", \"EEM\", \"TLT\", \"GLD\",\n",
    "    \"AMD\", \"INTC\", \"PYPL\", \"ORCL\", \"ZM\", \"UBER\", \"V\"\n",
    "]\n",
    "\n",
    "loader = FinancialDataLoader(\n",
    "    tickers=test_tickers,\n",
    "    start_date=\"2010-01-01\",\n",
    "    end_date=\"2025-01-01\",\n",
    "    ffill_limit=5\n",
    ")\n",
    "\n",
    "loader.download().clean().validate().store(format=\"wide\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chained `.download().clean().validate().store()` call is the intended API \u2014 each method returns `self`, so you can run the full pipeline in one line. If any step fails, the exception includes a clear message about what prerequisite was missing.\n",
    "\n",
    "Look at the output above. Some tickers may have failed \u2014 this is expected and exactly what the pipeline is designed to handle. yfinance is a free API with rate limits and occasional schema changes. A production pipeline at a real fund would have retry logic with exponential backoff, but the principle is the same: download what you can, record what failed, move on.\n",
    "\n",
    "Let's see the quality score distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_df = pd.DataFrame(loader.quality_scores).T\n",
    "quality_df = quality_df.sort_values(\"grade\")\n",
    "\n",
    "print(f\"Total tickers: {len(test_tickers)}\")\n",
    "print(f\"Failed downloads: {len(loader.failed_tickers)}\")\n",
    "print(f\"\\nGrade distribution:\")\n",
    "print(quality_df[\"grade\"].value_counts().sort_index())\n",
    "print(f\"\\nBottom 10 by quality:\")\n",
    "print(quality_df.tail(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the pattern: the mega-cap liquid names (AAPL, MSFT, SPY) almost certainly got grade A \u2014 near-perfect completeness, no OHLC violations, minimal stale prices. But scroll down to the tickers with shorter histories (UBER IPO'd in 2019, ZM IPO'd in 2019, META was FB until 2022) and you'll start seeing lower completeness scores. They don't have data back to 2010 because they didn't exist yet. The grading system correctly catches this: a ticker with 5 years of history out of a 15-year window gets dinged on completeness.\n",
    "\n",
    "This is exactly the kind of thing that bites you silently in a backtest. If you compute a 15-year Sharpe ratio on a ticker that only has 5 years of data, you're not measuring what you think you're measuring.\n",
    "\n",
    "Now let's verify the round-trip: does what we save come back unchanged?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = loader.data.copy()\n",
    "loader.store(format=\"wide\")\n",
    "loader.store(format=\"long\")\n",
    "loaded_wide = loader.load(format=\"wide\")\n",
    "\n",
    "close_orig = original_data[\"Close\"].values\n",
    "close_load = loaded_wide[\"Close\"].values\n",
    "max_diff = np.nanmax(np.abs(close_orig - close_load))\n",
    "\n",
    "print(f\"Original shape: {original_data.shape}\")\n",
    "print(f\"Loaded shape:   {loaded_wide.shape}\")\n",
    "print(f\"Max abs difference: {max_diff:.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That max difference should be zero or negligibly close to machine epsilon. This is one of Parquet's strengths over CSV \u2014 it stores floats in binary, so there's no string-to-float conversion that could introduce rounding artifacts. With CSV, a price of 192.36999999999998 might get truncated during writing and come back as 192.37. Over millions of rows, those micro-differences can compound into visible return discrepancies. Parquet avoids the problem entirely.\n",
    "\n",
    "Final check: does filtering work correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = loader.load(\n",
    "    tickers=[\"AAPL\", \"TSLA\", \"SPY\"], format=\"wide\"\n",
    ")\n",
    "print(f\"3-ticker filter: {subset['Close'].columns.tolist()}\")\n",
    "\n",
    "recent = loader.load(\n",
    "    start_date=\"2020-01-01\", end_date=\"2022-12-31\",\n",
    "    format=\"wide\"\n",
    ")\n",
    "print(f\"Date filter (2020-2022): {len(recent)} rows\")\n",
    "\n",
    "hq = loader.load(min_grade=\"A\", format=\"wide\")\n",
    "n_hq = (hq[\"Close\"].shape[1]\n",
    "        if \"Close\" in hq.columns.get_level_values(0) else 0)\n",
    "print(f\"Grade-A tickers: {n_hq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "All three filters work as expected. The ticker filter returns exactly the 3 requested names. The date filter slices down to 3 years of history. And the grade filter \u2014 the most powerful one \u2014 returns only the tickers that passed production quality standards. In a real workflow, a researcher would always start with `min_grade=\"B\"` at minimum. Using grade-D data for a backtest is like training an ML model on a dataset with 20% corrupted labels: your results might look fine on average, but you'd never trust them for a real decision.\n\n**The aha moment for Deliverable 1:** Building a data pipeline for 5 stocks is trivially easy \u2014 a single `yf.download()` call and you're done. Scaling to 50 reveals an entirely different class of problems: tickers that changed symbols (Facebook to META), IPOs that create incomplete histories (UBER, ZM), ETFs with slightly different trading calendars (EEM tracks emerging markets with occasional holidays the US market ignores), and the infamous BRK-B ticker that yfinance wants as `BRK-B` not `BRK.B`. Each edge case is minor in isolation. Together, they're the reason quant data engineering is a full-time job at every serious fund.\n\n---\n\n## Deliverable 2: Data Quality Report\n\nThe `FinancialDataLoader` computes quality scores. Now turn those scores into a report that a portfolio manager could read and act on. Per-ticker metrics with letter grades. A universe-level summary. A call-out of the problematic names.\n\nThe goal isn't a pretty table \u2014 it's an answer to the question every PM will ask before trusting your pipeline: \"How do I know this data is good?\"\n\n**Requirements:**\n- Per-ticker metrics: completeness, stale days, OHLC violations, long gaps, letter grade\n- Universe summary: how many tickers pass production quality, grade distribution\n- At least one ticker should receive a grade below A (if everything gets an A, your standards are too low)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build a per-ticker report table from loader.quality_scores\n",
    "# TODO: Compute universe-level summary (grade distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about what \"production quality\" means. What completeness threshold separates data you'd trust for a live trading signal from data you'd only use for exploratory research?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Highlight problematic tickers\n",
    "# TODO: Visualize completeness distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## \u2501\u2501\u2501 SOLUTION: Deliverable 2 \u2501\u2501\u2501\n\nWe already computed quality scores in Deliverable 1. The report just needs to present them in a way that's immediately actionable \u2014 no digging through raw dictionaries. A portfolio manager should be able to glance at this and know which tickers to trust."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_scores = loader.quality_scores\n",
    "\n",
    "report_rows = []\n",
    "for ticker, m in quality_scores.items():\n",
    "    report_rows.append({\n",
    "        \"Ticker\": ticker,\n",
    "        \"Completeness\": f\"{m['completeness']:.1%}\",\n",
    "        \"Stale Days\": m[\"stale_days\"],\n",
    "        \"OHLC Violations\": m[\"ohlc_violations\"],\n",
    "        \"Long Gaps (>5d)\": m[\"long_gaps\"],\n",
    "        \"Grade\": m[\"grade\"]\n",
    "    })\n",
    "\n",
    "per_ticker_report = pd.DataFrame(report_rows)\n",
    "per_ticker_report = per_ticker_report.sort_values(\"Grade\")\n",
    "per_ticker_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scan the Grade column. The A-rated tickers are your workhorses \u2014 use them without hesitation for backtesting, model training, and risk analysis. The B-rated ones are acceptable for most research but worth a manual check before putting them in production. Anything below B deserves investigation: why is completeness low? Is it a late IPO (legitimate) or a data gap (concerning)?\n",
    "\n",
    "The grading criteria are documented and reproducible:\n",
    "- **A:** Completeness >= 99%, stale days <= 20, zero OHLC violations, zero long gaps\n",
    "- **B:** Completeness >= 95%, stale days < 50, OHLC violations < 5, no long gaps\n",
    "- **C:** Completeness >= 90%, stale days < 100, OHLC violations < 10\n",
    "- **D:** Completeness >= 80%\n",
    "- **F:** Below all thresholds\n",
    "\n",
    "Now let's aggregate to the universe level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_counts = per_ticker_report[\"Grade\"].value_counts()\n",
    "grade_counts = grade_counts.sort_index()\n",
    "total_tickers = len(per_ticker_report)\n",
    "\n",
    "print(f\"Total tickers analyzed: {total_tickers}\")\n",
    "print(f\"Failed downloads: {len(loader.failed_tickers)}\\n\")\n",
    "print(\"Grade Distribution:\")\n",
    "for grade in [\"A\", \"B\", \"C\", \"D\", \"F\"]:\n",
    "    count = grade_counts.get(grade, 0)\n",
    "    pct = count / total_tickers * 100\n",
    "    print(f\"  {grade}: {count:3d} ({pct:5.1f}%)\")\n",
    "\n",
    "prod = grade_counts.get(\"A\", 0) + grade_counts.get(\"B\", 0)\n",
    "print(f\"\\nProduction-quality (A or B): \"\n",
    "      f\"{prod} ({prod/total_tickers*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The universe summary tells you the overall health of your data pipeline in two numbers: total tickers and the percentage that pass production quality. If you're running a fund with 50 names and only 35 pass grade B or better, you either need a better data vendor for the other 15, or you need to exclude them from analysis. Either way, you need to know \u2014 and this report gives you that answer automatically.\n",
    "\n",
    "Let's call out the worst offenders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic = per_ticker_report[\n",
    "    per_ticker_report[\"Grade\"].isin([\"C\", \"D\", \"F\"])\n",
    "]\n",
    "\n",
    "if len(problematic) > 0:\n",
    "    print(\"Tickers requiring investigation (Grade C or below):\")\n",
    "    print(problematic.to_string(index=False))\n",
    "else:\n",
    "    print(\"All tickers scored B or above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any tickers flagged above deserve a manual look. Low completeness on UBER or ZM is explained by their recent IPO dates (2019) \u2014 that's legitimate, not a data error. But low completeness on a stock that's been trading since 2005 would be a red flag pointing to a data vendor gap. The report surfaces the issue; the human investigates.\n",
    "\n",
    "Let's visualize the completeness distribution to see the overall picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completeness_vals = [\n",
    "    m[\"completeness\"] for m in quality_scores.values()\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist(completeness_vals, bins=20, edgecolor=\"black\",\n",
    "        alpha=0.7)\n",
    "ax.axvline(0.99, color=\"green\", ls=\"--\",\n",
    "           label=\"Production (99%)\")\n",
    "ax.axvline(0.90, color=\"orange\", ls=\"--\",\n",
    "           label=\"Minimum (90%)\")\n",
    "ax.set_xlabel(\"Completeness Ratio\")\n",
    "ax.set_ylabel(\"Number of Tickers\")\n",
    "ax.set_title(\"Data Completeness Distribution Across Universe\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**The aha moment for Deliverable 2:** The histogram makes it visceral. Most tickers cluster at or near 100% completeness \u2014 these are the household names with deep liquidity and robust vendor coverage. But there's a tail on the left: tickers with 30-70% completeness. These are typically recent IPOs being asked for 15 years of history. The real danger, though, isn't the tickers you know are incomplete \u2014 it's the ones at 95% completeness, where 5% of the trading days are silently missing. A 5% gap spread over 15 years means roughly one week of data missing per year. If one of those missing weeks happens to be March 2020 (COVID crash), your volatility estimates are materially understated and your max-drawdown figure is a fiction. Automated quality checks aren't optional \u2014 they're the difference between a pipeline you can trust and one that lies to you with a straight face.\n\n---\n\n## Deliverable 3: Storage Format Benchmark\n\nThe lecture mentioned that Parquet beats CSV and Polars beats pandas. Now prove it \u2014 with numbers, at multiple scales. Save the same dataset as CSV and Parquet, read it back with pandas and Polars, and measure everything: file sizes, write times, read times.\n\nThe interesting part isn't that Parquet wins (it will). It's *how the gap scales*. At 10 tickers, the difference might barely matter. At 50, it's meaningful. Extrapolate to the 500-ticker universe a real fund uses, and you'll understand why quant teams don't store data in CSV.\n\n**Requirements:**\n- Test all 4 combinations: CSV+pandas, CSV+Polars, Parquet+pandas, Parquet+Polars\n- Test at 3 scales (10, 25, all tickers)\n- Report file sizes, read times, and speedup ratios\n- Verify data integrity across all combinations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the full dataset and create subsets at 3 scales\n",
    "# TODO: Benchmark all 4 format+library combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about what \"scales\" means in practice. A 10-ticker subset might fit entirely in L3 cache. A 500-ticker dataset probably won't. The performance characteristics can change dramatically when data spills from cache to main memory to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display results table and plot scaling behavior\n",
    "# TODO: Verify data integrity across formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## \u2501\u2501\u2501 SOLUTION: Deliverable 3 \u2501\u2501\u2501\n\nA benchmark at a single scale is barely a benchmark \u2014 it tells you what's faster *right now* but not what happens when your data grows. We'll test at three scales (10, 25, and all ~50 tickers) to see whether the performance gap stays constant, widens, or narrows. The answer matters for capacity planning: if Polars+Parquet is only 20% faster at 50 tickers but 10x faster at 500, you want to know that before your data outgrows your tools."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = loader.load(format=\"wide\")\n",
    "all_tickers = full_data[\"Close\"].columns.tolist()\n",
    "\n",
    "print(f\"Full dataset: {full_data.shape[0]} rows x \"\n",
    "      f\"{full_data.shape[1]} columns\")\n",
    "print(f\"Available tickers: {len(all_tickers)}\")\n",
    "mem_mb = full_data.memory_usage(deep=True).sum() / 1e6\n",
    "print(f\"Memory usage: {mem_mb:.1f} MB\")\n",
    "\n",
    "bench_dir = CACHE_DIR / \"benchmark\"\n",
    "bench_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory usage number is worth noting. This is a ~50-ticker, 15-year daily dataset \u2014 a tiny fraction of what a real fund works with. If this already occupies meaningful memory, imagine a 5,000-ticker minute-bar dataset. Format and library choices that seem like micro-optimizations at our scale become existential at production scale.\n",
    "\n",
    "Now the benchmark. We write each subset once (CSV and Parquet), then read it back with all four library+format combinations. The key metric is the speedup relative to the slowest combination (pandas+CSV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALES = [10, 25, len(all_tickers)]\n",
    "all_results = []\n",
    "\n",
    "def _time_read(fn, *a, **kw):\n",
    "    t0 = time.perf_counter(); fn(*a, **kw); return time.perf_counter() - t0\n",
    "\n",
    "for n in SCALES:\n",
    "    sub = full_data.loc[:, (slice(None), all_tickers[:n])]\n",
    "    csv_p, pq_p = bench_dir / f\"bench_{n}.csv\", bench_dir / f\"bench_{n}.parquet\"\n",
    "    sub.to_csv(csv_p); sub.to_parquet(pq_p)\n",
    "    csv_mb, pq_mb = csv_p.stat().st_size / 1e6, pq_p.stat().st_size / 1e6\n",
    "    pd_csv = _time_read(pd.read_csv, csv_p, index_col=0,\n",
    "                        parse_dates=True, header=[0, 1])\n",
    "    pd_pq = _time_read(pd.read_parquet, pq_p)\n",
    "    pl_csv = _time_read(pl.read_csv, csv_p)\n",
    "    pl_pq = _time_read(pl.read_parquet, pq_p)\n",
    "    for lib, fmt, rd in [\n",
    "        (\"pandas\", \"CSV\", pd_csv), (\"pandas\", \"Parquet\", pd_pq),\n",
    "        (\"Polars\", \"CSV\", pl_csv), (\"Polars\", \"Parquet\", pl_pq),\n",
    "    ]:\n",
    "        all_results.append({\"Tickers\": n, \"Library\": lib,\n",
    "            \"Format\": fmt, \"Read (s)\": rd, \"Speedup\": pd_csv / rd})\n",
    "    print(f\"{n} tickers: CSV={csv_mb:.2f}MB, \"\n",
    "          f\"Parquet={pq_mb:.2f}MB ({csv_mb/pq_mb:.1f}x)\")\n",
    "\n",
    "results_df = pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at those file size ratios. Parquet should be 3-5x smaller than CSV at every scale. That compression comes from two sources: Parquet stores numbers in binary (no string conversion overhead), and it uses columnar compression that exploits the fact that a column of float64 prices has much less entropy than the same numbers as ASCII text. At our toy scale, the absolute savings are a few megabytes. At production scale \u2014 think 5,000 tickers with minute bars for 20 years \u2014 the difference is hundreds of gigabytes.\n",
    "\n",
    "Now let's look at the read-time comparison in table form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in SCALES:\n",
    "    print(f\"\\n--- {n} tickers ---\")\n",
    "    s = results_df[results_df[\"Tickers\"] == n]\n",
    "    cols = [\"Library\", \"Format\", \"Read (s)\", \"Speedup\"]\n",
    "    print(s[cols].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Speedup\" column is the one to watch. At 10 tickers, Polars+Parquet might only be 2-3x faster \u2014 noticeable but not life-changing. At 50 tickers, the gap should widen. The reason is architectural: CSV reading is fundamentally serial (parse each character, detect delimiters, convert types), while Parquet reading is parallelizable (the columnar layout lets you read independent column chunks simultaneously). Polars exploits this parallelism aggressively; pandas is more conservative.\n",
    "\n",
    "Let's visualize the scaling behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for (lib, fmt), grp in results_df.groupby([\"Library\", \"Format\"]):\n",
    "    axes[0].plot(grp[\"Tickers\"], grp[\"Read (s)\"],\n",
    "                 marker=\"o\", label=f\"{lib}+{fmt}\", lw=1.5)\n",
    "axes[0].set_xlabel(\"Number of Tickers\")\n",
    "axes[0].set_ylabel(\"Read Time (s)\")\n",
    "axes[0].set_title(\"Read Time vs. Dataset Scale\")\n",
    "axes[0].legend(fontsize=9)\n",
    "\n",
    "for (lib, fmt), grp in results_df.groupby([\"Library\", \"Format\"]):\n",
    "    if not (lib == \"pandas\" and fmt == \"CSV\"):\n",
    "        axes[1].plot(grp[\"Tickers\"], grp[\"Speedup\"],\n",
    "                     marker=\"o\", label=f\"{lib}+{fmt}\", lw=1.5)\n",
    "axes[1].set_xlabel(\"Number of Tickers\")\n",
    "axes[1].set_ylabel(\"Speedup vs pandas+CSV\")\n",
    "axes[1].set_title(\"Speedup Scaling\")\n",
    "axes[1].legend(fontsize=9)\n",
    "axes[1].axhline(1.0, color=\"gray\", ls=\"--\", lw=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left panel shows raw read times. If the lines are diverging as ticker count increases, that's the scaling story: the gap between the fastest combination (Polars+Parquet) and the slowest (pandas+CSV) grows with data size. The right panel normalizes everything to the pandas+CSV baseline, making the speedup factor explicit.\n",
    "\n",
    "This is why the industry is migrating. At 50 tickers the time savings might be seconds. At 5,000 tickers with intraday data, those seconds become minutes \u2014 and when a researcher iterates on a strategy 50 times a day, minutes become hours.\n",
    "\n",
    "One more thing: let's verify that switching formats doesn't corrupt the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_p = bench_dir / f\"bench_{len(all_tickers)}.csv\"\n",
    "pq_p = bench_dir / f\"bench_{len(all_tickers)}.parquet\"\n",
    "\n",
    "df_csv = pd.read_csv(\n",
    "    csv_p, index_col=0, parse_dates=True, header=[0, 1]\n",
    ")\n",
    "df_pq = pd.read_parquet(pq_p)\n",
    "\n",
    "c_csv = df_csv.iloc[\n",
    "    :, df_csv.columns.get_level_values(0) == \"Close\"\n",
    "].values\n",
    "c_pq = df_pq.iloc[\n",
    "    :, df_pq.columns.get_level_values(0) == \"Close\"\n",
    "].values\n",
    "\n",
    "diff = np.nanmax(np.abs(c_csv - c_pq))\n",
    "print(f\"Max diff (CSV vs Parquet Close): {diff:.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**The aha moment for Deliverable 3:** The size and speed advantages of Parquet over CSV are well-known in the abstract \u2014 everyone has heard \"Parquet is faster.\" What this multi-scale benchmark reveals is that the advantage is *super-linear* with data size. At 10 tickers, Parquet might be 3x smaller and 2x faster to read. At 50, it's 4x smaller and 4x faster. Extrapolate to the 10,000-instrument universe at a large quant fund with 20 years of minute bars, and the difference is between a 2 TB storage bill and a 500 GB one, between a 10-minute data load and a 60-second one. The format choice that seems like a minor engineering detail at toy scale becomes a defining infrastructure decision at production scale. This is why every quant fund built in the last 5 years uses Parquet as its default storage format \u2014 and why teams still on CSV are actively migrating.\n\n---\n\n## Deliverable 4: Return Statistics Summary (Bridge to Week 2)\n\nThis is the bridge. Compute basic return statistics for 10 representative tickers spanning different sectors and volatility profiles. The numbers you produce here will motivate everything in Week 2: stylized facts, fat tails, volatility clustering, non-stationarity.\n\nThe key question: are returns normally distributed? Your ML training has probably assumed yes (or at least close enough). The numbers will say otherwise \u2014 and the magnitude of the violation will surprise you.\n\n**Requirements:**\n- 10 tickers spanning sectors: tech, finance, healthcare, energy, consumer, industrials, plus ETFs\n- Compute: mean daily return, annualized volatility, skewness, kurtosis, min/max, and the number of >3-sigma days\n- Compare the actual tail behavior to what a Gaussian would predict\n- Present in a clean summary table"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Select 10 representative tickers across sectors\n",
    "# TODO: Compute daily log returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose tickers that span the volatility spectrum: at least one defensive name (utility or consumer staple), one high-volatility name (growth tech), and one non-equity (gold ETF or bond ETF). If the normality violations show up across all of these, the conclusion is structural, not anecdotal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute statistics and build summary table\n",
    "# TODO: Visualize distributions vs. Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## \u2501\u2501\u2501 SOLUTION: Deliverable 4 \u2501\u2501\u2501\n\nWe want diversity in our 10 tickers \u2014 not 10 tech stocks that all move together. The selection below spans sectors, volatility profiles, and asset types: defensive healthcare (JNJ), volatile tech (TSLA), cyclical energy (XOM), the broad market benchmark (SPY), and a non-equity asset (GLD). If the normality violations appear across all of these, we'll know it's a structural property of financial returns, not a quirk of one sector."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAT_TICKERS = [\n",
    "    \"AAPL\",   # Tech, large-cap\n",
    "    \"TSLA\",   # Tech/auto, high volatility\n",
    "    \"JPM\",    # Financials\n",
    "    \"JNJ\",    # Healthcare, defensive\n",
    "    \"XOM\",    # Energy\n",
    "    \"WMT\",    # Consumer staples, defensive\n",
    "    \"NFLX\",   # Media/entertainment\n",
    "    \"BA\",     # Industrials\n",
    "    \"SPY\",    # S&P 500 ETF (market benchmark)\n",
    "    \"GLD\"     # Gold ETF (alternative asset)\n",
    "]\n",
    "\n",
    "stat_data = loader.load(tickers=STAT_TICKERS, format=\"wide\")\n",
    "close = stat_data[\"Adj Close\"]\n",
    "returns = np.log(close / close.shift(1)).dropna()\n",
    "\n",
    "print(f\"Computing stats for {len(STAT_TICKERS)} tickers\")\n",
    "print(f\"Date range: {returns.index[0].date()} to \"\n",
    "      f\"{returns.index[-1].date()}\")\n",
    "print(f\"Observations per ticker: ~{len(returns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using log returns here, not simple returns. For statistical analysis \u2014 especially when testing for normality \u2014 log returns are the standard choice because they're additive across time and approximately symmetric for small moves. Simple returns would be the right choice if we were computing dollar P&L. The distinction matters: a stock that goes up 10% and then down 10% has a simple return of -1% (the compounding trap from the lecture), but log returns of +9.53% and -10.54% that honestly add up to a net negative.\n",
    "\n",
    "Now let's compute the full statistics panel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_rows = []\n",
    "for ticker in STAT_TICKERS:\n",
    "    ret = returns[ticker].dropna()\n",
    "    std_d = ret.std()\n",
    "    extreme = int((ret.abs() > 3 * std_d).sum())\n",
    "    ext_pct = extreme / len(ret)\n",
    "\n",
    "    stats_rows.append({\n",
    "        \"Ticker\": ticker,\n",
    "        \"Mean (daily)\": ret.mean(),\n",
    "        \"Vol (annual)\": std_d * np.sqrt(252),\n",
    "        \"Skewness\": scipy_stats.skew(ret),\n",
    "        \"Kurtosis\": scipy_stats.kurtosis(ret) + 3,\n",
    "        \"Min\": ret.min(), \"Max\": ret.max(),\n",
    "        \"> 3s days\": extreme,\n",
    "        \"> 3s %\": ext_pct,\n",
    "        \"vs Normal\": ext_pct / 0.0027,\n",
    "    })\n",
    "\n",
    "stats_df = pd.DataFrame(stats_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick note on the kurtosis computation: `scipy.stats.kurtosis` returns *excess* kurtosis (Gaussian = 0). We add 3 to convert to absolute kurtosis (Gaussian = 3), which is the more common convention in finance. When someone at a fund says \"TSLA has kurtosis 20,\" they mean absolute kurtosis. The \"vs Normal\" column divides the actual percentage of extreme days by the Gaussian expectation of 0.27% \u2014 a ratio of 3x means you're seeing three times more tail events than a Gaussian model would predict.\n",
    "\n",
    "Let's format this into a readable table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = stats_df.copy()\n",
    "disp[\"Mean (daily)\"] = disp[\"Mean (daily)\"].map(\"{:.4f}\".format)\n",
    "disp[\"Vol (annual)\"] = disp[\"Vol (annual)\"].map(\"{:.1%}\".format)\n",
    "disp[\"Skewness\"] = disp[\"Skewness\"].map(\"{:+.2f}\".format)\n",
    "disp[\"Kurtosis\"] = disp[\"Kurtosis\"].map(\"{:.1f}\".format)\n",
    "disp[\"Min\"] = disp[\"Min\"].map(\"{:.1%}\".format)\n",
    "disp[\"Max\"] = disp[\"Max\"].map(\"{:.1%}\".format)\n",
    "disp[\"> 3s %\"] = disp[\"> 3s %\"].map(\"{:.2%}\".format)\n",
    "disp[\"vs Normal\"] = disp[\"vs Normal\"].map(\"{:.1f}x\".format)\n",
    "\n",
    "print(\"DAILY RETURN STATISTICS (2010-2025)\")\n",
    "print(\"=\" * 100)\n",
    "print(disp.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read this table row by row and let the numbers sink in. Every single ticker has kurtosis well above 3. If returns were Gaussian, kurtosis would be 3.0 for all of them. Instead, you're seeing values ranging from maybe 4-5 for the defensives (JNJ, WMT) up to 15-25+ for the volatile names (TSLA, BA). The \"vs Normal\" column is the punchline: for most tickers, extreme events (beyond 3 standard deviations) happen 2-5x more often than a Gaussian model predicts. For the most volatile names, it can be 5-10x more frequent.\n",
    "\n",
    "The skewness column tells a parallel story: most tickers have negative skewness, meaning the left tail (crashes) is fatter than the right tail (rallies). Losses are not just more frequent than Gaussian models predict \u2014 they're also larger than the corresponding gains. This asymmetry is one reason options markets price puts more expensively than calls.\n",
    "\n",
    "Let's visualize the distributions against their Gaussian counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "plot_tickers = [\"JNJ\", \"SPY\", \"TSLA\"]\n",
    "\n",
    "for ax, ticker in zip(axes, plot_tickers):\n",
    "    ret = returns[ticker].dropna()\n",
    "    ax.hist(ret, bins=50, density=True, alpha=0.7,\n",
    "            label=\"Empirical\", edgecolor=\"black\")\n",
    "    mu, sig = ret.mean(), ret.std()\n",
    "    x = np.linspace(ret.min(), ret.max(), 100)\n",
    "    ax.plot(x, scipy_stats.norm.pdf(x, mu, sig),\n",
    "            \"r-\", lw=2, label=\"Gaussian\")\n",
    "    ax.set_title(f\"{ticker} Daily Returns\")\n",
    "    ax.set_xlabel(\"Return\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Three tickers, three volatility regimes, one universal pattern. JNJ is the quietest of the three \u2014 a defensive healthcare stock that barely moves most days. Even so, its empirical distribution has a sharper peak and fatter tails than the Gaussian overlay. SPY, the broad market, shows the same pattern more clearly: the red Gaussian curve underestimates the peak and wildly underestimates the tails. TSLA is where it gets dramatic \u2014 the empirical distribution has tails stretching to daily moves of 15-20%, events that a Gaussian model would assign probability approximately zero.\n\nThe visual makes the statistics table tangible. A kurtosis number is abstract. Seeing the Gaussian curve hugging zero while the empirical histogram extends far beyond it in both directions \u2014 that's what fat tails actually look like.\n\n**The aha moment for Deliverable 4:** Compare JNJ and TSLA side by side. JNJ's kurtosis might be around 5-6 \u2014 elevated above Gaussian, but not terrifyingly so. TSLA's could be 15-25+ \u2014 a factor of 4-5x higher. They're both in the S&P 500, both trade billions of dollars a day, but their tail risk profiles are in different universes. If your risk model uses a single distributional assumption for the whole portfolio (as many standard VaR models do), it will dramatically understate risk for the TSLA-like names and marginally overstate it for the JNJ-like names. This heterogeneity isn't a nuisance \u2014 it's a fundamental feature of financial returns that demands asset-specific distributional modeling. Week 2 will give you the tools: GARCH for time-varying volatility, EVT for tail modeling, and stylized facts that apply universally despite this cross-sectional variation.\n\n---\n\n## What You Built Tonight\n\nFour deliverables, one pipeline, and a collection of numbers that should make you permanently skeptical of any dataset that arrives without quality documentation.\n\n- **The `FinancialDataLoader` handles 50+ tickers** without crashing on individual failures. It downloads, cleans, validates, stores to Parquet, and loads with flexible filtering \u2014 the complete lifecycle that every production data pipeline needs, even though most toy projects skip everything after the download step.\n\n- **Data quality varies dramatically across tickers**, even within a curated universe of well-known US stocks. Recent IPOs, corporate restructurings, and ticker changes all create gaps that a naive pipeline would silently ignore. The quality grading system (A through F) makes reliability visible at a glance \u2014 and reveals that \"download from yfinance\" is not the same as \"have clean data.\"\n\n- **Parquet beats CSV by 3-5x on file size and the gap in read performance widens with scale.** At 10 tickers, the format choice barely matters. At 50, it's meaningful. Extrapolate to the thousands of instruments a real fund tracks, and the format choice becomes an infrastructure-defining decision. Polars adds another multiplier on top.\n\n- **Every ticker in the universe has fat tails** \u2014 kurtosis well above the Gaussian value of 3, extreme events occurring 2-10x more often than normal models predict, and negative skewness meaning crashes are larger than rallies. This isn't a property of volatile stocks \u2014 even defensive names like JNJ and the broad market index SPY violate normality.\n\n- **The kurtosis gap between defensive and volatile stocks** (JNJ at ~5 vs. TSLA at ~20+) means a one-size-fits-all distributional assumption is dangerously wrong. Asset-specific modeling isn't a luxury \u2014 it's a necessity.\n\n- **Building for 5 tickers is trivially easy; building for 50 reveals an entirely different class of problems.** Ticker changes, short histories, vendor quirks, calendar mismatches \u2014 these are the reasons quant data engineering is a full-time job, not a pre-processing afterthought.\n\nThe pipeline you built tonight is the foundation for everything that follows. You'll load data from it in Week 2 (stylized facts), Week 3 (feature engineering), Week 4 (portfolio construction), and beyond. Next week, we dig into those kurtosis numbers \u2014 what causes them, what they break, and what to do about them."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}