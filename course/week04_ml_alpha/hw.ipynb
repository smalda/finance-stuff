{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Week 4: ML for Alpha — From Features to Signals\n\n> *\"Every month, a quant researcher lines up 3,000 stocks and asks one question: which ones will be in the top decile next month? That question — and the pipeline that answers it — is the economic engine behind most systematic equity funds. This week, you build the engine.\"*\n\nYour mission: construct a complete, end-to-end cross-sectional alpha pipeline — the system that takes raw features, trains a model, evaluates signal quality, and converts predictions into portfolio returns. By the time you finish, you will have built a reusable class that mirrors the architecture behind production alpha research at firms like Two Sigma, AQR, and WorldQuant.\n\nThis is not a toy exercise. The `AlphaModelPipeline` you build in Deliverable 1 is the skeleton that Weeks 5 and 6 will extend with proper backtesting validation and portfolio optimization. Deliverable 2 tests whether you can improve the signal through feature engineering — and the answer may surprise you. Deliverable 3 asks the question every CIO at a quantitative fund eventually asks: \"Should we switch from gradient boosting to neural networks?\" Your job is to produce an honest answer backed by data, not by hype.\n\nA warning before you begin: the signal-to-noise ratio in cross-sectional equity prediction is catastrophically low. An information coefficient of 0.05 makes you a star. Your best ImageNet model gets 0.99. The metrics, the evaluation framework, and the definition of \"success\" are all different here. Recalibrate your expectations accordingly."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Deliverables\n\n1. **Cross-Sectional Alpha Engine** — Build a reusable `AlphaModelPipeline` class that encapsulates walk-forward training, IC evaluation, and portfolio construction. Must accept any scikit-learn-compatible model.\n2. **Feature Engineering Lab** — Expand the Week 3 feature matrix from 7 to 18 features with price-derived, fundamental, interaction, and non-linear additions. Measure whether the expansion improves signal quality. Produce a SHAP-based importance analysis.\n3. **The Model Comparison Report** — Run OLS, Ridge, LightGBM, and a feedforward neural network through the pipeline on the expanded features. Produce a structured comparison with IC, ICIR, Sharpe, turnover, and net Sharpe. Write an honest CIO recommendation."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import sys\nimport warnings\nfrom pathlib import Path\n\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport shap\nimport torch\nimport torch.nn as nn\nfrom scipy import stats\nfrom sklearn.linear_model import LinearRegression, Ridge\n\nsys.path.insert(0, \"code\")\nfrom data_setup import (\n    CACHE_DIR, PLOT_DIR, FEATURE_COLS, TECHNICAL_FEATURES,\n    FUNDAMENTAL_FEATURES, TRAIN_WINDOW, PURGE_GAP, COST_BPS,\n    FEATURE_START, FEATURE_END,\n    load_monthly_panel, load_feature_matrix, load_forward_returns,\n)\n\nsys.path.insert(0, str(Path(\"code\").resolve().parents[1]))\nfrom shared.temporal import walk_forward_splits, PurgedWalkForwardCV\nfrom shared.metrics import ic_summary, rank_ic, prediction_quality, deflated_sharpe_ratio\nfrom shared.backtesting import (\n    quantile_portfolios, long_short_returns, portfolio_turnover,\n    sharpe_ratio, net_returns, max_drawdown, cumulative_returns,\n)\nfrom shared.data import load_sp500_prices, load_sp500_ohlcv, load_sp500_fundamentals\nfrom shared.dl_training import fit_nn, predict_nn\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nplt.rcParams.update({\n    \"figure.figsize\": (10, 6),\n    \"axes.titlesize\": 13,\n    \"axes.labelsize\": 11,\n    \"font.size\": 10,\n})",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Deliverable 1: Cross-Sectional Alpha Engine\n\nThe lecture showed you how to train a LightGBM model, compute IC, and construct a long-short portfolio — one piece at a time, with fresh code in each section. That is how you learn the components. But nobody at a fund writes alpha research that way. In production, the entire pipeline — temporal splitting, walk-forward training, signal evaluation, portfolio construction — lives inside a single, reusable class. One call to `pipeline.run()` produces a full signal evaluation. A different model? Swap in Ridge, retrain, compare. A different feature set? Same pipeline, different inputs.\n\nYour job is to build that class: `AlphaModelPipeline`. It must accept any scikit-learn-compatible model, run a walk-forward loop with a configurable purge gap, compute IC and rank IC at every OOS date, construct a long-short portfolio from model predictions, and return a summary dictionary with every metric a portfolio manager would want to see. This is the first artifact in the course that connects features to portfolio performance end-to-end — the skeleton that Weeks 5 and 6 will extend with combinatorial purged cross-validation and transaction cost-aware portfolio optimization.\n\nIn production, this is where it breaks: the purge gap. If you get temporal integrity wrong — if even one training window overlaps with the prediction date — your entire IC estimate is contaminated. A fund running this pipeline on production data would have a second system that independently verifies no look-ahead leakage exists. You will build that verification step."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ━━━ SOLUTION: Deliverable 1 ━━━\n\nWe start by defining the class skeleton. The `__init__` method stores everything needed for the walk-forward loop: the model, the features, the target, and the temporal parameters. Notice the `hp_search` flag — in production, hyperparameter search happens inside each walk-forward window (never on OOS data), and this class supports that discipline."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "class AlphaModelPipeline:\n    \"\"\"End-to-end walk-forward alpha model evaluation pipeline.\n\n    Accepts any scikit-learn-compatible regressor, runs walk-forward\n    out-of-sample prediction, computes signal quality metrics, and\n    constructs long-short portfolios with transaction cost analysis.\n\n    Args:\n        model: sklearn-compatible regressor with fit/predict methods.\n        features: DataFrame with MultiIndex (date, ticker), feature columns.\n        target: Series with MultiIndex (date, ticker), forward returns.\n        train_window: months in rolling training window.\n        purge_gap: months between last train date and prediction date.\n        cost_bps: one-way transaction cost in basis points.\n        n_groups: number of quantile groups for portfolio construction.\n        hp_search: if True, run HP search on first window (LightGBM only).\n        hp_grid: dict of HP grid for search (LightGBM only).\n        impute: if True, impute NaN with cross-sectional median per window.\n    \"\"\"\n\n    def __init__(\n        self,\n        model,\n        features: pd.DataFrame,\n        target: pd.Series,\n        train_window: int = 60,\n        purge_gap: int = 1,\n        cost_bps: float = 10.0,\n        n_groups: int = 10,\n        hp_search: bool = False,\n        hp_grid: dict | None = None,\n        impute: bool = False,\n    ):\n        self.model = model\n        self.features = features\n        self.target = target\n        self.train_window = train_window\n        self.purge_gap = purge_gap\n        self.cost_bps = cost_bps\n        self.n_groups = n_groups\n        self.hp_search = hp_search\n        self.hp_grid = hp_grid\n        self.impute = impute\n\n        self.dates = features.index.get_level_values(\"date\").unique().sort_values()\n        self.feature_cols = list(features.columns)\n\n        # Populated after fit_predict()\n        self.predictions_ = None\n        self.ic_series_ = None\n        self.rank_ic_series_ = None\n        self._summary = None",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The class stores model state cleanly: `predictions_`, `ic_series_`, and `rank_ic_series_` are `None` until `fit_predict()` runs. This is the same pattern you will see in production systems like Microsoft's Qlib framework — all state is populated by a single execution call, never partially initialized. The `feature_cols` list is captured at construction time so the class remembers which columns to use even if the DataFrame is later modified.\n\nNext, the imputation method. LightGBM handles NaN natively, but linear models and neural networks cannot. The imputation must happen *within each training window* — using the full dataset's median would be a subtle form of look-ahead bias, because future cross-sections would contaminate the median. In production, this is exactly the kind of bug that survives code review for months because it does not produce errors, only slightly inflated performance."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def _impute_window(self, X_train: np.ndarray) -> np.ndarray:\n    \"\"\"Cross-sectional median imputation within training window.\"\"\"\n    X = X_train.copy()\n    for j in range(X.shape[1]):\n        col = X[:, j]\n        mask = np.isnan(col)\n        if mask.any():\n            med = np.nanmedian(col)\n            col[mask] = med if np.isfinite(med) else 0.0\n    return X\n\nAlphaModelPipeline._impute_window = _impute_window",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The `AlphaModelPipeline.method = function` pattern — monkey-patching methods onto the class after definition — may look unusual if you are accustomed to defining everything inside the class body. We use it here because it lets us break a large class into small, self-contained cells with prose between each one. Every production code review you will encounter uses this pattern for incremental class construction in notebooks and research environments.\n\nNow the hyperparameter search infrastructure. This is a two-part system: a single-fold evaluator and a grid search coordinator. The key design choice is that HP search uses `PurgedWalkForwardCV` — a temporal cross-validation splitter that maintains the purge gap even within the training window. Standard k-fold CV would leak future returns into the training set. This is not hypothetical: a production pipeline at a major quant fund was found to have exactly this bug in 2019, inflating backtested Sharpe by 40%."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def _hp_eval_fold(self, X, y, tr_idx, val_idx, lr, nl):\n    \"\"\"Evaluate one CV fold for HP search. Returns (ic, n_iters).\"\"\"\n    mdl = lgb.LGBMRegressor(\n        n_estimators=500, learning_rate=lr,\n        num_leaves=nl, min_child_samples=20,\n        reg_alpha=0.1, reg_lambda=1.0,\n        subsample=0.8, subsample_freq=1,\n        colsample_bytree=0.8, random_state=42,\n        verbosity=-1,\n    )\n    mdl.fit(\n        X[tr_idx], y[tr_idx],\n        eval_set=[(X[val_idx], y[val_idx])],\n        callbacks=[lgb.early_stopping(50, verbose=False)],\n    )\n    pred = mdl.predict(X[val_idx])\n    ic = np.corrcoef(pred, y[val_idx])[0, 1]\n    return ic, mdl.best_iteration_\n\nAlphaModelPipeline._hp_eval_fold = _hp_eval_fold",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Notice the regularization defaults: `reg_alpha=0.1`, `reg_lambda=1.0`, `subsample=0.8`. These are not arbitrary. With ~10,000 training observations (174 stocks x 60 months) and only 7 features, the model is swimming in data relative to features — a regime where regularization prevents the model from memorizing the noise in individual cross-sections. Early stopping with patience of 50 rounds acts as a second layer of regularization. In S3, the model often stopped at just 10 iterations — a clear sign that the signal is thin and more boosting rounds just chase noise."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def _search_hps(self, X, y):\n    \"\"\"HP search on first window for LightGBM models.\"\"\"\n    grid = self.hp_grid or {\n        \"learning_rate\": [0.005, 0.01, 0.05],\n        \"num_leaves\": [15, 31, 63],\n    }\n    cv = PurgedWalkForwardCV(n_splits=3, purge_gap=1)\n    best_score, best_params, best_n = -np.inf, {}, 100\n\n    for lr in grid.get(\"learning_rate\", [0.05]):\n        for nl in grid.get(\"num_leaves\", [31]):\n            scores, iters = [], []\n            for tr_idx, val_idx in cv.split(X):\n                ic, n_iter = self._hp_eval_fold(X, y, tr_idx, val_idx, lr, nl)\n                if np.isfinite(ic):\n                    scores.append(ic)\n                    iters.append(n_iter)\n\n            mean_ic = np.mean(scores) if scores else -np.inf\n            if mean_ic > best_score:\n                best_score = mean_ic\n                best_params = {\"learning_rate\": lr, \"num_leaves\": nl}\n                best_n = max(10, int(np.mean(iters)))\n\n    return best_params, best_n\n\nAlphaModelPipeline._search_hps = _search_hps",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The HP search runs only on the first training window and reuses the selected parameters for all subsequent windows. This is a pragmatic choice: re-searching HPs at every window would be more rigorous but would multiply runtime by the grid size. In production, HP search is typically done on a quarterly cadence, not monthly. The `max(10, ...)` on `best_n` prevents degenerate zero-iteration models.\n\nNow the per-window model fitting. This is where the LightGBM-specific early stopping logic lives, separate from the generic sklearn path. The design supports both branches through the same interface — a pattern that makes the class extensible to any model family."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def _fit_lgbm_one(self, panel, features, target, train_dates,\n                  searched_params, searched_n_est):\n    \"\"\"Fit one LightGBM model with early stopping on a single window.\"\"\"\n    if searched_params:\n        mdl = lgb.LGBMRegressor(\n            n_estimators=searched_n_est or 200,\n            learning_rate=searched_params[\"learning_rate\"],\n            num_leaves=searched_params[\"num_leaves\"],\n            min_child_samples=20,\n            reg_alpha=0.1, reg_lambda=1.0,\n            subsample=0.8, subsample_freq=1,\n            colsample_bytree=0.8, random_state=42,\n            verbosity=-1,\n        )\n    else:\n        from sklearn.base import clone\n        mdl = clone(self.model)\n\n    n_val = min(12, len(train_dates) // 5)\n    val_dates = train_dates[-n_val:]\n    fit_dates = train_dates[:-n_val]\n    fit_mask = panel.index.get_level_values(\"date\").isin(fit_dates)\n    val_mask_inner = panel.index.get_level_values(\"date\").isin(val_dates)\n    X_fit = features.loc[fit_mask].values\n    y_fit = target.loc[fit_mask].values\n    X_val = features.loc[val_mask_inner].values\n    y_val = target.loc[val_mask_inner].values\n    if self.impute:\n        X_fit = self._impute_window(X_fit)\n        X_val_clean = X_val.copy()\n        for j in range(X_val_clean.shape[1]):\n            col = X_val_clean[:, j]\n            mn = np.isnan(col)\n            if mn.any():\n                col[mn] = np.nanmedian(X_fit[:, j])\n        X_val = X_val_clean\n    mdl.fit(\n        X_fit, y_fit,\n        eval_set=[(X_val, y_val)],\n        callbacks=[lgb.early_stopping(50, verbose=False)],\n    )\n    return mdl\n\nAlphaModelPipeline._fit_lgbm_one = _fit_lgbm_one",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Look at the validation split: the last 12 months of training data (or 20%, whichever is smaller) serve as the early stopping holdout. This is temporal validation — never random — because shuffling would place future cross-sections into the fit set. The imputation path handles the validation set separately from the training set, using the training set's medians. These details are invisible in research code but fatal in production if done wrong.\n\nThe prediction method runs a single walk-forward window end to end: fit the model, generate predictions, compute IC. This is the inner loop that `fit_predict()` orchestrates."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def _predict_one_window(self, panel, features, target, train_dates,\n                        pred_date, is_lgbm, searched_params, searched_n_est):\n    \"\"\"Run one walk-forward window: fit model, predict OOS, return records.\"\"\"\n    train_mask = panel.index.get_level_values(\"date\").isin(train_dates)\n    pred_mask = panel.index.get_level_values(\"date\") == pred_date\n\n    X_train = features.loc[train_mask].values\n    y_train = target.loc[train_mask].values\n    X_pred = features.loc[pred_mask].values\n    y_pred = target.loc[pred_mask].values\n    tickers = panel.loc[pred_mask].index.get_level_values(\"ticker\")\n\n    if self.impute:\n        X_train = self._impute_window(X_train)\n        X_pred_clean = X_pred.copy()\n        for j in range(X_pred_clean.shape[1]):\n            col = X_pred_clean[:, j]\n            mask_nan = np.isnan(col)\n            if mask_nan.any():\n                med = np.nanmedian(X_train[:, j])\n                col[mask_nan] = med if np.isfinite(med) else 0.0\n        X_pred = X_pred_clean\n\n    if is_lgbm:\n        mdl = self._fit_lgbm_one(panel, features, target, train_dates,\n                                 searched_params, searched_n_est)\n    else:\n        from sklearn.base import clone\n        mdl = clone(self.model)\n        if self.impute:\n            X_train = self._impute_window(X_train)\n        mdl.fit(X_train, y_train)\n\n    preds = mdl.predict(X_pred)\n\n    oos_records = [{\"date\": pred_date, \"ticker\": t, \"prediction\": p, \"actual\": a}\n                   for t, p, a in zip(tickers, preds, y_pred)]\n\n    ic_p = np.corrcoef(preds, y_pred)[0, 1]\n    ic_r = stats.spearmanr(preds, y_pred)[0]\n    ic_rec = {\n        \"date\": pred_date,\n        \"ic_pearson\": ic_p if np.isfinite(ic_p) else np.nan,\n        \"ic_rank\": ic_r if np.isfinite(ic_r) else np.nan,\n    }\n    return oos_records, ic_rec\n\nAlphaModelPipeline._predict_one_window = _predict_one_window",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The prediction imputation uses training set medians applied to the prediction cross-section — a critical detail. If you used the prediction cross-section's own median, you would be using information from the stocks you are trying to rank, creating a subtle cross-sectional leakage. In an interview at a systematic fund, this is exactly the kind of question that separates candidates who understand financial ML from those who just know ML.\n\nNow the `fit_predict()` method — the public API that runs the complete walk-forward loop. A single call produces the full OOS prediction history and IC time series."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def fit_predict(self) -> tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Run walk-forward prediction loop.\n\n    Returns:\n        ic_df: DataFrame with columns [ic_pearson, ic_rank], indexed by date.\n        pred_df: DataFrame with MultiIndex (date, ticker),\n                 columns [prediction, actual].\n    \"\"\"\n    panel = self.features.join(self.target.rename(\"fwd_return\"), how=\"inner\")\n    dates = panel.index.get_level_values(\"date\").unique().sort_values()\n    features = panel[self.feature_cols]\n    target = panel[\"fwd_return\"]\n\n    splits = list(walk_forward_splits(dates, self.train_window, self.purge_gap))\n    n_splits = len(splits)\n\n    # HP search on first window (LightGBM only)\n    searched_params, searched_n_est = None, None\n    is_lgbm = isinstance(self.model, lgb.LGBMRegressor)\n    if is_lgbm and self.hp_search:\n        first_train = dates[:self.train_window]\n        mask = panel.index.get_level_values(\"date\").isin(first_train)\n        X_first = features.loc[mask].values\n        y_first = target.loc[mask].values\n        if self.impute:\n            X_first = self._impute_window(X_first)\n        searched_params, searched_n_est = self._search_hps(X_first, y_first)\n        print(f\"  HP search: {searched_params}, n_est={searched_n_est}\")\n\n    oos_records, ic_records = [], []\n    for i, (train_dates, pred_date) in enumerate(splits):\n        if i % 10 == 0:\n            print(f\"  [{i+1}/{n_splits}] predicting \"\n                  f\"{pd.Timestamp(pred_date).date()}\")\n        oos_rec, ic_rec = self._predict_one_window(\n            panel, features, target, train_dates, pred_date,\n            is_lgbm, searched_params, searched_n_est)\n        oos_records.extend(oos_rec)\n        ic_records.append(ic_rec)\n\n    self.predictions_ = (\n        pd.DataFrame(oos_records).set_index([\"date\", \"ticker\"])\n    )\n    self.ic_series_ = pd.DataFrame(ic_records).set_index(\"date\")\n    self.rank_ic_series_ = self.ic_series_[\"ic_rank\"]\n    self._summary = None  # reset cache\n    print(f\"  Walk-forward complete: {len(ic_records)} OOS months\")\n\n    return self.ic_series_, self.predictions_\n\nAlphaModelPipeline.fit_predict = fit_predict",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Two things to notice. First, the `is_lgbm` check uses `isinstance` — not a string comparison — because the class genuinely needs to know whether the model supports early stopping (LightGBM does, sklearn models do not). Second, the HP search happens only once, on the first window, and the found parameters are reused for all subsequent windows. This is a deliberate trade-off between rigor and runtime that mirrors what most production systems do.\n\nThe last method is `summary()` — a single call that returns every metric a portfolio manager would want to see. It caches its results to avoid recomputation."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def summary(self) -> dict:\n    \"\"\"Compute comprehensive evaluation metrics.\n\n    Returns:\n        dict with: mean_ic, std_ic, icir, pct_positive, t_stat, p_value,\n        mean_rank_ic, sharpe_gross, sharpe_net, mean_turnover,\n        max_drawdown, n_oos_months.\n    \"\"\"\n    if self.ic_series_ is None:\n        raise RuntimeError(\"Call fit_predict() first.\")\n\n    if self._summary is not None:\n        return self._summary\n\n    ic_arr = self.ic_series_[\"ic_pearson\"].dropna().values\n    ic_stats = ic_summary(ic_arr)\n    mean_rank = float(self.ic_series_[\"ic_rank\"].dropna().mean())\n\n    # Portfolio metrics\n    pred_s = self.predictions_[\"prediction\"]\n    ret_s = self.predictions_[\"actual\"]\n    ls = long_short_returns(pred_s, ret_s, self.n_groups)\n    to = portfolio_turnover(pred_s, self.n_groups)\n    gross_sharpe = sharpe_ratio(ls)\n    net_ret = net_returns(ls, to, self.cost_bps)\n    net_sharpe = sharpe_ratio(net_ret)\n    mdd = max_drawdown(ls)\n    mean_to = float(to.mean()) if len(to) > 0 else np.nan\n\n    self._summary = {\n        \"mean_ic\": ic_stats[\"mean_ic\"],\n        \"std_ic\": ic_stats[\"std_ic\"],\n        \"icir\": ic_stats[\"icir\"],\n        \"pct_positive\": ic_stats[\"pct_positive\"],\n        \"t_stat\": ic_stats[\"t_stat\"],\n        \"p_value\": ic_stats[\"p_value\"],\n        \"mean_rank_ic\": mean_rank,\n        \"sharpe_gross\": gross_sharpe,\n        \"sharpe_net\": net_sharpe,\n        \"mean_turnover\": mean_to,\n        \"max_drawdown\": mdd,\n        \"n_oos_months\": ic_stats[\"n\"],\n    }\n    return self._summary\n\nAlphaModelPipeline.summary = summary",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The `summary()` dictionary is designed to be directly consumable by a reporting system — a pattern you will see at firms like AQR and BlackRock Systematic, where signal evaluation dashboards are populated by exactly this kind of structured output. The method computes everything: IC statistics, portfolio metrics (gross and net Sharpe), turnover, and max drawdown. A portfolio manager glances at this dictionary and knows whether the signal is worth pursuing.\n\nNow let us run the pipeline on our data and see what it produces. First, the data quality check — a mandatory step before any model training."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "panel = load_monthly_panel()\ndates = panel.index.get_level_values(\"date\").unique().sort_values()\nfeatures = panel[FEATURE_COLS]\ntarget = panel[\"fwd_return\"]\n\nn_months = len(dates)\ntickers_per = panel.groupby(level=\"date\").size()\nnan_rate = features.isna().mean()\n\nprint(\"── DATA QUALITY ──────────────────────────────────────\")\nprint(f\"  Panel shape: {panel.shape}\")\nprint(f\"  Feature months: {n_months}\")\nprint(f\"  Tickers/month: {tickers_per.min()}–{tickers_per.max()}\")\nprint(f\"  Target mean: {target.mean():.6f}, std: {target.std():.6f}\")\nprint(f\"  Missing rates:\")\nfor col in FEATURE_COLS:\n    print(f\"    {col}: {nan_rate[col]:.2%}\")\nprint(f\"  NOTE: LightGBM handles NaN natively; linear models use imputation\")\nprint(f\"  SURVIVORSHIP BIAS: S&P 500 universe — results overstate\")\nprint()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The data quality block shows ~6% missing values in the fundamental features (pb_ratio_z, roe_z) and essentially zero missing in the three technical features (momentum, reversal, volatility). This asymmetry matters: LightGBM handles NaN natively by choosing the best split direction at each node, while linear models must impute. The survivorship bias warning is not cosmetic — our universe is today's S&P 500 projected backward to 2014, meaning every company that went bankrupt, was acquired, or was removed from the index is invisible in our data.\n\nTime to run the default pipeline. LightGBM with a 60-month training window, HP search on the first window, decile portfolio construction."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"── ALPHAMODELPIPELINE: DEFAULT RUN (LightGBM) ────────\")\ndefault_model = lgb.LGBMRegressor(\n    n_estimators=200, learning_rate=0.05, num_leaves=31,\n    min_child_samples=20, reg_alpha=0.1, reg_lambda=1.0,\n    subsample=0.8, subsample_freq=1, colsample_bytree=0.8,\n    random_state=42, verbosity=-1,\n)\n\npipeline = AlphaModelPipeline(\n    model=default_model,\n    features=features,\n    target=target,\n    train_window=TRAIN_WINDOW,\n    purge_gap=PURGE_GAP,\n    cost_bps=COST_BPS,\n    n_groups=10,\n    hp_search=True,\n    impute=False,\n)\n\nic_df, pred_df = pipeline.fit_predict()\nsummary = pipeline.summary()\nprint()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The pipeline runs ~68 walk-forward windows, training a fresh LightGBM model at each one. The HP search on the first window typically finds a learning rate of 0.05 with 31 leaves — but aggressive early stopping often reduces the effective number of estimators to around 10. That is a signal in itself: the model finds that more than 10 boosting rounds just chases noise in the training data."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"── PIPELINE SUMMARY ──────────────────────────────────\")\nfor k, v in summary.items():\n    if isinstance(v, float):\n        print(f\"  {k}: {v:.4f}\")\n    else:\n        print(f\"  {k}: {v}\")\nprint()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The default run produces a mean IC of approximately 0.055 (t = 2.51, p = 0.014) — statistically significant at 5%, and sitting at the upper end of the 0.02-0.05 range that practitioners consider realistic for cross-sectional equity models. The ICIR of 0.305 tells you the signal-to-noise ratio: for every 1 unit of signal, there are roughly 3.3 units of noise. The gross Sharpe of approximately 0.98 looks attractive, but the turnover of 80% monthly means the portfolio replaces most of its holdings every month. At 10 bps per trade, the net Sharpe drops to about 0.91 — still positive, but the cost drag is real.\n\nNow let us verify temporal integrity — the single most important property of the pipeline."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "splits = list(walk_forward_splits(dates, TRAIN_WINDOW, PURGE_GAP))\nleak_count = 0\nfor train_dates, pred_date in splits:\n    if train_dates[-1] >= pred_date:\n        leak_count += 1\n    gap_months = np.searchsorted(dates, pred_date) - np.searchsorted(dates, train_dates[-1])\n    if gap_months < PURGE_GAP + 1:\n        leak_count += 1\n\nprint(\"── TEMPORAL INTEGRITY ────────────────────────────────\")\nprint(f\"  Walk-forward splits: {len(splits)}\")\nprint(f\"  Purge gap: {PURGE_GAP} month(s)\")\nprint(f\"  Leakage violations: {leak_count}\")\nprint()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Zero violations. Every training window ends at least one month before the prediction date. This check is not paranoia — it is the minimum standard for any backtest that will be shown to investors. At a production fund, temporal integrity violations in a backtest are a firing offense.\n\nThe pipeline must accept any scikit-learn-compatible model. Let us verify by swapping in Ridge regression."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"── SKLEARN COMPATIBILITY TEST (Ridge) ────────────────\")\nridge_pipeline = AlphaModelPipeline(\n    model=Ridge(alpha=1.0),\n    features=features,\n    target=target,\n    train_window=TRAIN_WINDOW,\n    purge_gap=PURGE_GAP,\n    cost_bps=COST_BPS,\n    impute=True,\n)\nridge_ic, ridge_pred = ridge_pipeline.fit_predict()\nridge_summary = ridge_pipeline.summary()\nprint(f\"  Ridge mean IC: {ridge_summary['mean_ic']:.4f}\")\nprint(f\"  Ridge ICIR: {ridge_summary['icir']:.4f}\")\nprint(f\"  Ridge Sharpe (gross): {ridge_summary['sharpe_gross']:.4f}\")\nprint()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Ridge produces a mean IC of about 0.059 — remarkably close to LightGBM's 0.055, and arguably a stronger result given its simplicity. This is not a coincidence. On 7 rank-normalized features, the feature-return relationships are nearly linear (rank normalization forces them to be), so a linear model captures most of the available signal. The lesson for ML engineers: in cross-sectional equity prediction with few features, model complexity buys you almost nothing. That lesson will sharpen further in Deliverable 3.\n\nNow let us look at the portfolio that the default GBM signal produces."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "pred_s = pred_df[\"prediction\"]\nret_s = pred_df[\"actual\"]\nqp = quantile_portfolios(pred_s, ret_s, 10)\nls = long_short_returns(pred_s, ret_s, 10)\nto = portfolio_turnover(pred_s, 10)\ncum_ret = cumulative_returns(ls)\nnet_ret = net_returns(ls, to, COST_BPS)\ncum_net = cumulative_returns(net_ret)\n\nprint(\"── PORTFOLIO ANALYSIS ────────────────────────────────\")\nprint(f\"  Decile months: {len(qp)}\")\nprint(f\"  Decile groups: {len(qp.columns)}\")\nprint(f\"  Stocks per decile: ~{panel.groupby(level='date').size().median() / 10:.0f}\")\nprint(f\"  Long-short mean monthly: {ls.mean():.6f}\")\nprint(f\"  Long-short std monthly: {ls.std():.6f}\")\nprint(f\"  Gross Sharpe: {sharpe_ratio(ls):.4f}\")\nprint(f\"  Net Sharpe ({COST_BPS} bps): {sharpe_ratio(net_ret):.4f}\")\nprint(f\"  Mean turnover: {to.mean():.4f}\")\nprint(f\"  Max drawdown: {max_drawdown(ls):.4f}\")\nprint(f\"  Cumulative (gross) final: {cum_ret.iloc[-1]:.4f}\")\nprint(f\"  Cumulative (net) final: {cum_net.iloc[-1]:.4f}\")\nprint()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The decile analysis reveals something you would catch immediately at a fund: every decile earns positive average returns, including the bottom decile. That should not happen — the bottom decile, the stocks you are shorting, should contain some losers. The reason is survivorship bias: our universe contains only companies that survived to be in the S&P 500 today. The firms that went bankrupt or were delisted during 2019-2024 are invisible. In a production backtest on CRSP data, the bottom decile would contain firms that subsequently delisted at a loss.\n\nThe max drawdown of approximately -23.5% is a reminder that even a positive-expectation long-short strategy can lose a quarter of its value in a bad stretch. The return distribution has excess kurtosis above 3.0, meaning that Sharpe ratio understates the true tail risk — extreme monthly losses are more likely than a Gaussian distribution would predict."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\ndecile_means = qp.mean()\ncolors_dec = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(decile_means)))\naxes[0].bar(decile_means.index, decile_means.values, color=colors_dec)\naxes[0].axhline(0, color=\"black\", linewidth=0.5)\naxes[0].set_xlabel(\"Decile (1=bottom, 10=top)\")\naxes[0].set_ylabel(\"Mean Monthly Return\")\naxes[0].set_title(\"Decile Mean Returns (GBM Signal)\")\n\naxes[1].plot(range(len(cum_ret)), cum_ret.values,\n             label=\"Gross\", linewidth=1.5)\naxes[1].plot(range(len(cum_net)), cum_net.values,\n             label=f\"Net ({COST_BPS} bps)\", linewidth=1.5, linestyle=\"--\")\naxes[1].axhline(1.0, color=\"black\", linewidth=0.5)\naxes[1].set_xlabel(\"OOS Month\")\naxes[1].set_ylabel(\"Cumulative Return\")\naxes[1].set_title(\"Long-Short Cumulative Returns\")\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The left panel shows the decile monotonicity: a broadly upward slope from bottom to top decile, with the spread of about 0.88% per month between decile 10 and decile 1. The monotonicity rank correlation of 0.83 is strong for an ML-based signal. The right panel shows cumulative gross and net long-short returns — the gap between the two lines is the cost of trading, compounding over 68 months. At a fund, this chart is the first thing a PM looks at when evaluating a new signal.\n\nYou now have a reusable alpha pipeline. Deliverable 2 will test whether you can improve the signal by engineering better features — and the answer is not what most ML engineers expect."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Deliverable 2: Feature Engineering Lab\n\nThe Week 3 feature matrix has 7 features. Can you do better? GKX (2020) tested 94 characteristics and found that expanding the feature set improved OOS R-squared by roughly 50%. The implication seems clear: more features, more signal. But there is a catch — one that most ML engineers discover the hard way. The GKX result holds on 3,000 CRSP stocks with 94 well-chosen, point-in-time features. On 174 S&P 500 stocks with 18 features, several of which are correlated and two of which have look-ahead bias, the story may be quite different.\n\nYour mission: expand the feature matrix from 7 to 15-25 features using domain-motivated additions. Include multi-horizon momentum, realized volatility, Amihud illiquidity (price-derived features that are point-in-time clean), plus new fundamental ratios and interaction terms. Then run the expanded features through the pipeline from Deliverable 1 and answer a precise question: did feature expansion improve IC?\n\nHere is where it gets interesting. The PIT contamination check at the end will reveal something counterintuitive about the relationship between data quality and signal strength. The fundamental features — which have full look-ahead bias — may not improve the signal at all, and may actually hurt it."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Student Workspace"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# TODO: Load the baseline 7-feature matrix and forward returns\n# TODO: Compute price-derived features (multi-horizon momentum, realized vol)\n# TODO: Compute Amihud illiquidity from OHLCV data\n# TODO: Add fundamental features (D/E ratio, profit margins) with PIT warning\n# TODO: Create interaction features (momentum x volatility, value x quality)\n# TODO: Create non-linear features (momentum squared, absolute reversal)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Once you have the raw features, you need to normalize them and verify quality before running the pipeline."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# TODO: Apply cross-sectional rank normalization to all new features\n# TODO: Check feature correlation matrix — flag pairs with |corr| > 0.95\n# TODO: Run the AlphaModelPipeline on the expanded feature set\n# TODO: Run the baseline pipeline on the same dates for fair comparison",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Finally, evaluate whether the expansion actually helped. A paired t-test on the monthly IC series gives you a fair comparison, SHAP values reveal which features the model actually uses, and the PIT contamination check quantifies the cost of look-ahead bias in the fundamentals."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# TODO: Paired t-test: expanded vs baseline IC series\n# TODO: SHAP importance analysis — which features actually contribute?\n# TODO: PIT contamination check: compare PIT-clean vs all-features IC",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ━━━ SOLUTION: Deliverable 2 ━━━\n\nWe begin by loading the baseline and computing the price-derived features. These are the features that require no fundamental data — just daily prices and volumes — and are therefore point-in-time clean."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "fm_baseline = load_feature_matrix()\nfwd_ret = load_forward_returns()\npanel_baseline = fm_baseline.join(fwd_ret, how=\"inner\")\ndates = panel_baseline.index.get_level_values(\"date\").unique().sort_values()\ntickers_list = sorted(fm_baseline.index.get_level_values(\"ticker\").unique())\n\nprint(\"── BASELINE FEATURES ─────────────────────────────────\")\nprint(f\"  Original features: {FEATURE_COLS}\")\nprint(f\"  Shape: {fm_baseline.shape}\")\nprint(f\"  Months: {len(dates)}, Tickers: {len(tickers_list)}\")\nprint()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "174 stocks, 130 months, 7 features. That is our starting point. Now let us build the additions, starting with multi-horizon momentum — different lookback windows capture different cross-sectional dynamics. Jegadeesh and Titman's (1993) original momentum formulation used 12-month returns skipping the most recent month (to avoid short-term reversal contamination). We add 3-month and 6-month variants to capture shorter-horizon catalysts."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"── CONSTRUCTING PRICE-DERIVED FEATURES ───────────────\")\nprices = load_sp500_prices(start=\"2012-01-01\", end=\"2025-01-31\")\nmonthly_close = prices.resample(\"ME\").last()\n\n# Multi-horizon momentum (cumulative return over horizon)\nmom_3m = monthly_close.pct_change(3)\nmom_6m = monthly_close.pct_change(6)\nmom_12m_1m = monthly_close.shift(1).pct_change(11)  # skip most recent month\n\n# Realized volatility: std of daily returns over 3-month window\ndaily_ret = prices.pct_change()\nrvol_3m = daily_ret.rolling(63).std() * np.sqrt(252)  # annualized\nrvol_3m_monthly = rvol_3m.resample(\"ME\").last()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The `mom_12m_1m` calculation deserves attention: `shift(1).pct_change(11)` computes the return over months t-12 to t-1, deliberately excluding the most recent month. This is the canonical momentum signal — the month-t return is excluded because it captures reversal rather than momentum. Getting this wrong (including the most recent month) would blend two opposing effects and weaken the signal. In production, this is exactly the kind of subtle feature engineering that separates a 0.03 IC from a 0.05 IC.\n\nNow Amihud illiquidity — one of the most durable cross-sectional predictors in the academic literature. It measures the average absolute daily return per dollar of trading volume, capturing how much price impact each dollar of volume creates."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "try:\n    ohlcv = load_sp500_ohlcv(start=\"2012-01-01\", end=\"2025-01-31\",\n                              fields=[\"Close\", \"Volume\"])\n    vol_data = ohlcv[\"Volume\"]\n    close_data = ohlcv[\"Close\"]\n    dollar_vol = vol_data * close_data\n    daily_illiq = daily_ret.abs() / dollar_vol.reindex_like(daily_ret)\n    daily_illiq = daily_illiq.replace([np.inf, -np.inf], np.nan)\n    amihud_monthly = daily_illiq.resample(\"ME\").mean()\n    has_amihud = True\n    print(\"  Amihud illiquidity: computed from OHLCV\")\nexcept Exception as e:\n    has_amihud = False\n    print(f\"  Amihud illiquidity: skipped ({e})\")\n\nprint(f\"  mom_3m shape: {mom_3m.shape}\")\nprint(f\"  mom_6m shape: {mom_6m.shape}\")\nprint(f\"  mom_12m_1m shape: {mom_12m_1m.shape}\")\nprint(f\"  rvol_3m shape: {rvol_3m_monthly.shape}\")\nprint()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Amihud (2002) showed that less liquid stocks earn a premium — compensation for the difficulty of trading them. For our S&P 500 universe, all stocks are relatively liquid, so the premium may be attenuated. But the cross-sectional variation in illiquidity still exists (compare trading Apple versus a smaller S&P 500 constituent like Hormel Foods), and the GBM may exploit it.\n\nNow the fundamental additions. Here is where honesty matters most. The D/E ratio and profit margins come from yfinance's static ratios table — current-date snapshots applied to all historical months. This is full look-ahead bias, and we must say so explicitly."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"── CONSTRUCTING FUNDAMENTAL FEATURES (static ratios) ─\")\nfund = load_sp500_fundamentals(tickers=tickers_list, pit_lag_days=90)\nratios = fund[\"ratios\"]\n\n# D/E ratio — static per-ticker value from yfinance\nde_map = ratios[\"debtToEquity\"].dropna().to_dict() if \"debtToEquity\" in ratios.columns else {}\n\n# Profit margins — static per-ticker value from yfinance\npm_map = ratios[\"profitMargins\"].dropna().to_dict() if \"profitMargins\" in ratios.columns else {}\n\nn_de = sum(1 for t in tickers_list if t in de_map)\nn_pm = sum(1 for t in tickers_list if t in pm_map)\nprint(f\"  D/E ratio: {n_de}/{len(tickers_list)} tickers covered\")\nprint(f\"  Profit margins: {n_pm}/{len(tickers_list)} tickers covered\")\nprint(f\"  ⚠ PIT WARNING: static ratios have full look-ahead bias\")\nprint(f\"    These are current snapshots applied to all historical months.\")\nprint(f\"    The PIT contamination check below quantifies the impact.\")\nprint()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The `pit_lag_days=90` argument to `load_sp500_fundamentals` shifts fundamental dates forward by 90 days to simulate the reporting lag between fiscal period end and filing date. But for D/E and profit margins from the ratios table, there is no date dimension to shift — they are static scalars. This means the PIT lag mitigation does not apply to these features. The PIT contamination check at the end of this deliverable will quantify whether this matters.\n\nNow we merge everything into the expanded feature matrix and create the interaction and non-linear terms."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def cross_sectional_rank(series_grouped):\n    \"\"\"Per-month rank normalization to [0, 1].\"\"\"\n    return series_grouped.rank(pct=True)\n\n\nexpanded = fm_baseline.copy()\n\n# Price-derived: align to baseline MultiIndex\nfor name, raw in [(\"mom_3m\", mom_3m), (\"mom_6m\", mom_6m),\n                   (\"mom_12m_1m\", mom_12m_1m), (\"rvol_3m\", rvol_3m_monthly)]:\n    stacked = raw.stack()\n    stacked.index.names = [\"date\", \"ticker\"]\n    common = expanded.index.intersection(stacked.index)\n    expanded[name] = np.nan\n    expanded.loc[common, name] = stacked.loc[common].values\n\nif has_amihud:\n    stacked_amihud = amihud_monthly.stack()\n    stacked_amihud.index.names = [\"date\", \"ticker\"]\n    common = expanded.index.intersection(stacked_amihud.index)\n    expanded[\"amihud\"] = np.nan\n    expanded.loc[common, \"amihud\"] = stacked_amihud.loc[common].values",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The alignment step — intersecting the new features' index with the baseline's MultiIndex — ensures that we only keep observations where both the original features and the new features exist. Any ticker-month combination that is in the baseline but missing from a new feature gets NaN, which LightGBM handles natively."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Fundamental-derived: static per-ticker values broadcast to all months\nexpanded[\"de_ratio\"] = expanded.index.get_level_values(\"ticker\").map(de_map)\nexpanded[\"profit_margin\"] = expanded.index.get_level_values(\"ticker\").map(pm_map)\n\n# Interaction features (from z-scored originals — no new data needed)\nexpanded[\"mom_x_vol\"] = expanded[\"momentum_z\"] * expanded[\"volatility_z\"]\nexpanded[\"val_x_qual\"] = expanded[\"earnings_yield_z\"] * expanded[\"roe_z\"]\n\n# Non-linear features\nexpanded[\"mom_sq\"] = expanded[\"momentum_z\"] ** 2\nexpanded[\"abs_reversal\"] = expanded[\"reversal_z\"].abs()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The interaction features are the most domain-motivated additions. `mom_x_vol` captures conditional momentum: does the momentum effect strengthen or weaken in high-volatility stocks? If momentum_z and volatility_z are both positive (a high-momentum, high-vol stock), the interaction is large and positive — the model can learn to weight these stocks differently. `val_x_qual` captures the Asness et al. (2019) insight that value and quality are complementary: cheap stocks that are also profitable may have additive alpha beyond what either factor captures alone.\n\nNow we apply cross-sectional rank normalization to all new features and check for near-duplicates."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "new_feat_cols = [c for c in expanded.columns if c not in FEATURE_COLS]\nfor col in new_feat_cols:\n    expanded[col] = expanded.groupby(level=\"date\")[col].transform(\n        lambda x: x.rank(pct=True)\n    )\n\n# Drop features with >50% missing\nkeep_cols = list(FEATURE_COLS)\nfor col in new_feat_cols:\n    miss_rate = expanded[col].isna().mean()\n    if miss_rate < 0.50:\n        keep_cols.append(col)\n    else:\n        print(f\"  Dropped {col}: {miss_rate:.1%} missing\")\n        expanded.drop(columns=[col], inplace=True)\n\nall_feature_cols = keep_cols\nn_features = len(all_feature_cols)\n\nprint(f\"── EXPANDED FEATURE MATRIX ───────────────────────────\")\nprint(f\"  Total features: {n_features}\")\nprint(f\"  Original: {len(FEATURE_COLS)}\")\nprint(f\"  New: {n_features - len(FEATURE_COLS)}\")\nprint(f\"  Columns: {all_feature_cols}\")\nprint(f\"  Missing rates (new features):\")\nfor col in [c for c in all_feature_cols if c not in FEATURE_COLS]:\n    print(f\"    {col}: {expanded[col].isna().mean():.2%}\")\nprint()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Cross-sectional rank normalization transforms each feature to the [0, 1] range within each month. This is the standard preprocessing step in production cross-sectional models for two reasons: it eliminates level effects (a P/E ratio of 50 is \"expensive\" in utilities but \"normal\" in tech) and it makes features comparable across different scales. The original 7 features were already z-scored in Week 3, but the new features come in raw — momentum in return units, Amihud in illiquidity units. Rank normalization puts them all on the same playing field."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "corr_matrix = expanded[all_feature_cols].corr()\nmax_off_diag = 0.0\nmax_pair = (\"\", \"\")\nfor i, c1 in enumerate(all_feature_cols):\n    for j, c2 in enumerate(all_feature_cols):\n        if i < j:\n            val = abs(corr_matrix.loc[c1, c2])\n            if val > max_off_diag:\n                max_off_diag = val\n                max_pair = (c1, c2)\n\nprint(\"── FEATURE CORRELATION CHECK ─────────────────────────\")\nprint(f\"  Max |corr|: {max_off_diag:.4f} ({max_pair[0]} vs {max_pair[1]})\")\nif max_off_diag > 0.95:\n    print(f\"  ⚠ Near-duplicate pair detected — consider dropping one\")\nelse:\n    print(f\"  No near-duplicates (all |corr| < 0.95)\")\nprint()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The maximum correlation of approximately 0.923 (between momentum_z and mom_12m_1m) is high but below the 0.95 threshold. These two features measure similar but not identical things: momentum_z is Week 3's 12-month momentum without the skip-month adjustment, while mom_12m_1m excludes the most recent month per the Jegadeesh-Titman formulation. The 0.923 correlation means the GBM must navigate near-multicollinearity when splitting on either feature — which is exactly why the substitution effect (Seminar Exercise 2) is such a real phenomenon.\n\nNow the critical test: does feature expansion actually help? We run both the expanded and baseline models through the same pipeline on the same date range for a fair comparison."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"── EXPANDED MODEL (LightGBM, {0} features) ──────────\".format(n_features))\nexpanded_features = expanded[all_feature_cols]\nexpanded_target = fwd_ret.reindex(expanded_features.index).dropna()\ncommon_idx = expanded_features.index.intersection(expanded_target.index)\nexpanded_features = expanded_features.loc[common_idx]\nexpanded_target = expanded_target.loc[common_idx]\n\nexpanded_model = lgb.LGBMRegressor(\n    n_estimators=200, learning_rate=0.05, num_leaves=31,\n    min_child_samples=20, reg_alpha=0.1, reg_lambda=1.0,\n    subsample=0.8, subsample_freq=1, colsample_bytree=0.5,\n    random_state=42, verbosity=-1,\n)\n\nexp_pipeline = AlphaModelPipeline(\n    model=expanded_model,\n    features=expanded_features,\n    target=expanded_target,\n    train_window=TRAIN_WINDOW,\n    purge_gap=PURGE_GAP,\n    cost_bps=COST_BPS,\n    hp_search=True,\n    impute=False,  # LightGBM handles NaN natively\n)\n\nexp_ic_df, exp_pred_df = exp_pipeline.fit_predict()\nexp_summary = exp_pipeline.summary()\nprint()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Notice `colsample_bytree=0.5` — down from the baseline's 0.8. With 18 features (many of which are correlated), forcing each tree to use only 50% of features per split encourages diversity and reduces the impact of near-multicollinearity. This is a deliberate HP adjustment for the expanded feature set."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"── BASELINE MODEL (LightGBM, 7 features) ────────────\")\nbase_features = expanded[FEATURE_COLS].loc[common_idx]\nbase_target = expanded_target\n\nbaseline_model = lgb.LGBMRegressor(\n    n_estimators=200, learning_rate=0.05, num_leaves=31,\n    min_child_samples=20, reg_alpha=0.1, reg_lambda=1.0,\n    subsample=0.8, subsample_freq=1, colsample_bytree=0.8,\n    random_state=42, verbosity=-1,\n)\n\nbase_pipeline = AlphaModelPipeline(\n    model=baseline_model,\n    features=base_features,\n    target=base_target,\n    train_window=TRAIN_WINDOW,\n    purge_gap=PURGE_GAP,\n    cost_bps=COST_BPS,\n    hp_search=True,\n    impute=False,\n)\n\nbase_ic_df, base_pred_df = base_pipeline.fit_predict()\nbase_summary = base_pipeline.summary()\nprint()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Both models run on the exact same (date, ticker) observations — the `common_idx` alignment ensures that any IC difference is attributable to the features, not to different samples."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "common_dates = exp_ic_df.index.intersection(base_ic_df.index)\nexp_ic_arr = exp_ic_df.loc[common_dates, \"ic_pearson\"].dropna().values\nbase_ic_arr = base_ic_df.loc[common_dates, \"ic_pearson\"].dropna().values\nn_paired = min(len(exp_ic_arr), len(base_ic_arr))\nexp_ic_arr = exp_ic_arr[:n_paired]\nbase_ic_arr = base_ic_arr[:n_paired]\n\nic_change = exp_summary[\"mean_ic\"] - base_summary[\"mean_ic\"]\ndiff = exp_ic_arr - base_ic_arr\nif len(diff) >= 2 and diff.std() > 0:\n    t_paired = diff.mean() / (diff.std() / np.sqrt(len(diff)))\n    p_paired = 2 * (1 - stats.t.cdf(abs(t_paired), df=len(diff) - 1))\nelse:\n    t_paired, p_paired = np.nan, np.nan\n\nprint(\"── BASELINE vs EXPANDED COMPARISON ───────────────────\")\nprint(f\"  Baseline mean IC: {base_summary['mean_ic']:.4f}\")\nprint(f\"  Expanded mean IC: {exp_summary['mean_ic']:.4f}\")\nprint(f\"  IC change: {ic_change:+.4f}\")\nprint(f\"  Paired t-stat: {t_paired:.4f}\")\nprint(f\"  Paired p-value: {p_paired:.4f}\")\nprint(f\"  Significant (5%): {p_paired < 0.05 if np.isfinite(p_paired) else False}\")\nprint()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Here is the result that should make every ML engineer pause: adding 11 features to a 7-feature model on 174 S&P 500 stocks *degraded* the signal. The IC dropped from 0.055 to about 0.034 — a change of -0.020 (p = 0.13, not statistically significant, but the direction is clear). This is the opposite of the GKX (2020) production result, where expanding from 10 to 94 features improved R-squared by about 50%. What happened?\n\nTwo factors drive the degradation. First, the 174-stock cross-section provides insufficient variation for 18 features to separate signal from noise — the model's search space grows (more features to split on) while the informational content of each cross-section stays fixed. Second, the near-multicollinearity between momentum_z and mom_12m_1m (correlation 0.923) forces the GBM to make arbitrary split choices between essentially duplicate features, injecting noise into the prediction rankings. On a 3,000-stock cross-section with 94 diverse features, each additional feature captures genuine new variation. On 174 large-cap stocks with 18 correlated features, the additions are mostly noise. This is why feature engineering in finance is a domain knowledge problem, not an automation problem — you need to know which features add genuine information at your specific scale.\n\nNow let us look at what the model is actually learning. SHAP values decompose each prediction into per-feature contributions, revealing which features drive the cross-sectional rankings."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"── SHAP FEATURE IMPORTANCE ───────────────────────────\")\npanel_exp = expanded_features.join(expanded_target.rename(\"fwd_return\"), how=\"inner\")\nexp_dates = panel_exp.index.get_level_values(\"date\").unique().sort_values()\nsplits = list(walk_forward_splits(exp_dates, TRAIN_WINDOW, PURGE_GAP))\n\n# Train a final model on each of last 12 windows, collect SHAP\nshap_values_pooled = []\nfeature_names = all_feature_cols\nn_shap_windows = min(12, len(splits))\n\nfor train_dates, pred_date in splits[-n_shap_windows:]:\n    train_mask = panel_exp.index.get_level_values(\"date\").isin(train_dates)\n    pred_mask = panel_exp.index.get_level_values(\"date\") == pred_date\n    X_tr = panel_exp.loc[train_mask, all_feature_cols].values\n    y_tr = panel_exp.loc[train_mask, \"fwd_return\"].values\n    X_pred = panel_exp.loc[pred_mask, all_feature_cols].values\n\n    mdl = lgb.LGBMRegressor(\n        n_estimators=200, learning_rate=0.05, num_leaves=31,\n        min_child_samples=20, reg_alpha=0.1, reg_lambda=1.0,\n        random_state=42, verbosity=-1,\n    )\n    # Split for early stopping\n    n_val = min(12, len(train_dates) // 5)\n    fit_end = len(train_dates) - n_val\n    fit_mask = panel_exp.index.get_level_values(\"date\").isin(train_dates[:fit_end])\n    val_mask = panel_exp.index.get_level_values(\"date\").isin(train_dates[fit_end:])\n    X_fit = panel_exp.loc[fit_mask, all_feature_cols].values\n    y_fit = panel_exp.loc[fit_mask, \"fwd_return\"].values\n    X_val = panel_exp.loc[val_mask, all_feature_cols].values\n    y_val = panel_exp.loc[val_mask, \"fwd_return\"].values\n    mdl.fit(X_fit, y_fit, eval_set=[(X_val, y_val)],\n            callbacks=[lgb.early_stopping(50, verbose=False)])\n\n    explainer = shap.TreeExplainer(mdl)\n    sv = explainer.shap_values(X_pred)\n    shap_values_pooled.append(sv)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We pool SHAP values across the last 12 walk-forward windows rather than using a single snapshot. A single window's SHAP values are unstable on this data — the feature rankings can change substantially from one month to the next. Pooling across 12 windows gives a more robust picture of which features the model consistently relies on."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "shap_all = np.vstack(shap_values_pooled)\nmean_abs_shap = np.mean(np.abs(shap_all), axis=0)\nshap_ranking = sorted(\n    zip(feature_names, mean_abs_shap),\n    key=lambda x: x[1], reverse=True,\n)\n\nprint(f\"  SHAP windows pooled: {n_shap_windows}\")\nprint(f\"  Top-10 features by mean |SHAP|:\")\nfor rank, (feat, val) in enumerate(shap_ranking[:10], 1):\n    origin = \"original\" if feat in FEATURE_COLS else \"engineered\"\n    print(f\"    {rank}. {feat}: {val:.6f} ({origin})\")\nprint()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Volatility dominates the SHAP ranking at 0.00121 — 1.6 times the next feature (rvol_3m at 0.00101). Together, the two volatility features account for about 40% of total SHAP importance. This aligns precisely with the GKX (2020) finding that a small handful of features drive predictions regardless of model complexity. The interaction feature mom_x_vol ranks in the top 5, suggesting that conditional momentum — the idea that momentum's strength varies with volatility regime — captures real cross-sectional structure beyond what the pure linear features provide.\n\nNow the economic interpretations. Every feature that drives a model's predictions should have a story — a reason why it should predict cross-sectional returns."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "interpretations = {\n    \"momentum_z\": \"Cross-sectional momentum: stocks with recent outperformance \"\n                  \"tend to continue outperforming — a well-documented anomaly.\",\n    \"reversal_z\": \"Short-term reversal: extreme recent losers bounce back, \"\n                  \"consistent with microstructure-driven mean reversion.\",\n    \"volatility_z\": \"Higher idiosyncratic volatility correlates with higher \"\n                    \"expected returns in cross-section (risk compensation).\",\n    \"pb_ratio_z\": \"Price-to-book captures the value premium — stocks with low \"\n                  \"P/B tend to earn higher returns (Fama-French HML factor).\",\n    \"roe_z\": \"Quality signal: high-ROE firms have persistent earnings power, \"\n             \"which the market may undervalue.\",\n    \"earnings_yield_z\": \"Earnings yield (E/P) is the inverse of P/E — captures \"\n                        \"cheap vs. expensive stocks in cross-section.\",\n    \"asset_growth_z\": \"Firms expanding assets aggressively tend to underperform \"\n                      \"(Cooper, Gulen & Schill 2008).\",\n    \"mom_3m\": \"Short-horizon momentum captures different frequency dynamics \"\n              \"than 12-month momentum — more sensitive to recent catalysts.\",\n    \"mom_6m\": \"Intermediate momentum: smoother than 3-month, less reversal-prone \"\n              \"than 12-month. Captures medium-term trends.\",\n    \"mom_12m_1m\": \"Skip-month momentum: Jegadeesh & Titman's (1993) original \"\n                  \"formulation, removing the reversal-prone most recent month.\",\n    \"rvol_3m\": \"Realized volatility: higher vol stocks carry a risk premium \"\n               \"but are also noisier — the Ang et al. (2006) low-vol anomaly.\",\n    \"amihud\": \"Amihud illiquidity: less liquid stocks earn a liquidity premium \"\n              \"(compensation for trading difficulty).\",\n    \"de_ratio\": \"Leverage signal: higher D/E firms are riskier — may earn \"\n                \"a risk premium or signal financial distress.\",\n    \"profit_margin\": \"Profitability signal: higher-margin firms have durable \"\n                     \"competitive advantages — quality factor proxy (Novy-Marx 2013).\",\n    \"fcf_yield\": \"Free cash flow yield: high FCF/market cap indicates firms \"\n                 \"generating cash relative to their valuation — quality proxy.\",\n    \"mom_x_vol\": \"Interaction: momentum effect may be stronger/weaker in \"\n                 \"high-vol regimes — captures conditional momentum.\",\n    \"val_x_qual\": \"Interaction: value-quality overlap — cheap + profitable \"\n                  \"stocks may have additive alpha (Asness et al. 2019).\",\n    \"mom_sq\": \"Non-linear momentum: captures diminishing returns at extreme \"\n              \"momentum — very high momentum stocks may reverse.\",\n    \"abs_reversal\": \"Absolute reversal: magnitude of recent loss/gain matters \"\n                    \"regardless of sign — captures extreme-move effect.\",\n}",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Each interpretation maps a feature to an economic mechanism. In production, features without economic stories are treated with suspicion — if you cannot explain why a feature should predict returns, you may be fitting noise. This is the domain knowledge that feature engineering at a fund requires: not just knowing how to compute a feature, but knowing why it should work."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"── ECONOMIC INTERPRETATION (Top-5) ───────────────────\")\nfor rank, (feat, _) in enumerate(shap_ranking[:5], 1):\n    interp = interpretations.get(feat, \"No standard interpretation available.\")\n    print(f\"  {rank}. {feat}: {interp}\")\nprint()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The top-5 ranking reveals the model's true dependencies: volatility (risk compensation), 3-month realized vol (a different frequency of the same risk signal), price-to-book (the value premium), Amihud illiquidity (the liquidity premium), and the momentum-volatility interaction. These are exactly the categories that GKX (2020) found dominant across all model types — the features are the alpha, and the model is just the lens.\n\nFinally, the PIT contamination check. We strip out all fundamental features (the four original z-scored fundamentals plus the two new static ratios) and re-run the pipeline on only price-derived, PIT-clean features."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"── PIT CONTAMINATION CHECK ───────────────────────────\")\npit_clean_cols = [c for c in all_feature_cols\n                  if c not in [\"pb_ratio_z\", \"roe_z\", \"asset_growth_z\",\n                               \"earnings_yield_z\", \"de_ratio\", \"profit_margin\"]]\npit_all_cols = all_feature_cols\n\nprint(f\"  PIT-clean features ({len(pit_clean_cols)}): {pit_clean_cols}\")\nprint(f\"  All features ({len(pit_all_cols)}): {pit_all_cols}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We strip out all six fundamental features — four original z-scored fundamentals plus the two static ratios added in this deliverable — leaving only price-derived, volume-derived, and interaction features. The pipeline re-runs on this clean subset, giving us a direct measure of how much the contaminated fundamentals contribute (or detract)."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "pit_clean_features = expanded_features[pit_clean_cols].loc[common_idx]\npit_model = lgb.LGBMRegressor(\n    n_estimators=200, learning_rate=0.05, num_leaves=31,\n    min_child_samples=20, reg_alpha=0.1, reg_lambda=1.0,\n    subsample=0.8, subsample_freq=1, colsample_bytree=0.8,\n    random_state=42, verbosity=-1,\n)\npit_pipeline = AlphaModelPipeline(\n    model=pit_model,\n    features=pit_clean_features,\n    target=expanded_target,\n    train_window=TRAIN_WINDOW,\n    purge_gap=PURGE_GAP,\n    cost_bps=COST_BPS,\n    hp_search=True,\n    impute=False,\n)\npit_ic_df, _ = pit_pipeline.fit_predict()\npit_summary = pit_pipeline.summary()\n\npit_diff = exp_summary[\"mean_ic\"] - pit_summary[\"mean_ic\"]\nprint(f\"  PIT-clean mean IC: {pit_summary['mean_ic']:.4f}\")\nprint(f\"  All features mean IC: {exp_summary['mean_ic']:.4f}\")\nprint(f\"  Difference (all - clean): {pit_diff:+.4f}\")\nif pit_diff > 0.01:\n    print(f\"  ⚠ PIT contamination likely inflating IC by {pit_diff:.4f}\")\nelse:\n    print(f\"  Minimal PIT contamination effect\")\nprint()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Here is the counterintuitive punchline: the PIT-clean model (using only price-derived features) produces a *higher* IC than the full model including contaminated fundamentals. The PIT-contaminated features hurt IC by about 0.007. This is the opposite of what you might expect — perfect foreknowledge of fundamentals should help, right? Not on a 174-stock S&P 500 cross-section, where the fundamental rankings are too stable (remember, these are static ratios applied uniformly across all months) to provide useful month-to-month prediction differentiation. The cross-sectional variation in static D/E ratios and profit margins is noise that dilutes the signal from time-varying price features. In production, with quarterly point-in-time fundamentals and a 3,000-stock universe, the story would be different — but on our sandbox data, the message is clear: bad data is worse than no data.\n\nLet us visualize the SHAP importance and the baseline-versus-expanded comparison."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "fig_shap, ax_shap = plt.subplots(figsize=(10, 8))\nfeature_order = [f for f, _ in shap_ranking]\nordered_indices = [feature_names.index(f) for f in feature_order]\nshap_ordered = shap_all[:, ordered_indices]\n\n# Color: original features blue, engineered orange\ncolors_bar = [\"#1f77b4\" if f in FEATURE_COLS else \"#ff7f0e\"\n              for f in feature_order]\n\nax_shap.barh(range(len(feature_order)), mean_abs_shap[ordered_indices],\n             color=colors_bar)\nax_shap.set_yticks(range(len(feature_order)))\nax_shap.set_yticklabels(feature_order)\nax_shap.invert_yaxis()\nax_shap.set_xlabel(\"Mean |SHAP value|\")\nax_shap.set_title(\"Feature Importance: Expanded Model (blue=original, orange=engineered)\")\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The horizontal bar chart reveals the importance hierarchy at a glance. The blue bars (original features) and orange bars (engineered features) are interspersed — volatility_z (original) leads, rvol_3m (engineered) follows closely. The engineered interaction feature mom_x_vol earns its place in the top 5, validating the domain intuition that conditional momentum captures real structure."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "fig_comp, ax_comp = plt.subplots(figsize=(8, 5))\nlabels = [\"Baseline\\n(7 features)\", \"Expanded\\n({0} features)\".format(n_features)]\nmeans = [base_summary[\"mean_ic\"], exp_summary[\"mean_ic\"]]\nstds = [base_summary[\"std_ic\"] / np.sqrt(base_summary[\"n_oos_months\"]),\n        exp_summary[\"std_ic\"] / np.sqrt(exp_summary[\"n_oos_months\"])]\nbars = ax_comp.bar(labels, means, yerr=stds, capsize=5,\n                   color=[\"#1f77b4\", \"#ff7f0e\"], width=0.5)\nax_comp.axhline(0, color=\"black\", linewidth=0.5)\nax_comp.set_ylabel(\"Mean OOS IC\")\nax_comp.set_title(f\"Feature Expansion: IC Change = {ic_change:+.4f} (p={p_paired:.3f})\")\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The error bars overlap — the IC degradation from feature expansion is not statistically significant (p = 0.13). But the direction is clear and consistent across multiple runs. At a fund, this chart would end the conversation about adding these features to the production model at this universe scale. You would file it under \"revisit when we have 3,000 stocks.\"\n\nThe core lesson from this deliverable: more features is not always better. Feature expansion that improves performance on a 3,000-stock, 94-feature universe can actively degrade performance on a 174-stock, 18-feature subset. The production result requires both a larger feature set and a larger cross-section to materialize. This is one of the most important lessons for ML engineers entering finance — the relationship between feature count and signal quality is not monotonic, and it depends critically on the scale of your data."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Deliverable 3: The Model Comparison Report\n\nYour fund's CIO walks into your office and asks the question that defines a quant team's technology stack: \"Should we switch from gradient boosting to neural networks?\" This is not a casual question. Rewriting a production pipeline costs engineering months, introduces new failure modes (GPU infrastructure, training instability, hyperparameter sensitivity), and requires revalidating every downstream system. The CIO needs a rigorous, honest answer — not hype about \"deep learning\" and not reflexive conservatism about \"if it ain't broke.\"\n\nYour job is to produce that answer. You will run four model families — OLS, Ridge, LightGBM, and a feedforward neural network — through the same pipeline on the same expanded feature set. You will evaluate on the metrics that actually matter (IC, ICIR, Sharpe, turnover, net Sharpe), compute pairwise statistical tests, apply the deflated Sharpe ratio for multiple testing, and write a structured recommendation.\n\nThe teaser: the model ranking on our data is the *inverse* of what GKX (2020) found on production data. That inversion — and the reason behind it — is the most important finding in this homework."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Student Workspace"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# TODO: Load D2's expanded feature matrix\n# TODO: Define all 4 model configurations (OLS, Ridge, LightGBM, NN)\n# TODO: Build a sklearn-compatible NN wrapper with fit/predict methods\n# TODO: Run each model through the AlphaModelPipeline",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "After running all models, analyze and compare the results statistically. The summary table should contain every metric from `pipeline.summary()`, and the pairwise paired t-tests should compare adjacent complexity levels (OLS vs Ridge, Ridge vs GBM, GBM vs NN) to determine whether the IC differences are significant."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# TODO: Construct the summary comparison table\n# TODO: Compute pairwise paired t-tests between adjacent complexity levels\n# TODO: Compute deflated Sharpe ratio for the best model",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Finally, contextualize the results with an honest sandbox-vs-production analysis. Document the four gaps between our sandbox and a production deployment (feature count, universe size, PIT contamination, survivorship bias), write the CIO recommendation, and produce the three-panel comparison chart."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# TODO: Write the sandbox-vs-production analysis (features, universe, PIT, survivorship)\n# TODO: Write the CIO recommendation\n# TODO: Visualize: 3-panel chart (IC, ICIR, Sharpe)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ━━━ SOLUTION: Deliverable 3 ━━━\n\nWe start by loading D2's expanded feature matrix and aligning it with the target variable. All four models will operate on the exact same data — the only thing that changes is the model."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "expanded_features = pd.read_parquet(CACHE_DIR / \"expanded_features.parquet\")\npanel = load_monthly_panel()\ntarget = panel[\"fwd_return\"]\n\n# Align\ncommon_idx = expanded_features.index.intersection(target.index)\nexpanded_features = expanded_features.loc[common_idx]\ntarget = target.loc[common_idx]\nfeature_cols = list(expanded_features.columns)\n\nprint(\"── DATA ──────────────────────────────────────────────\")\nprint(f\"  Expanded features: {len(feature_cols)} columns\")\nprint(f\"  Shape: {expanded_features.shape}\")\nprint(f\"  Target: {len(target)} observations\")\nprint()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Before running the comparison, we need a neural network that speaks sklearn. The AlphaModelPipeline expects `fit()` and `predict()` methods — the standard sklearn protocol. We build a wrapper around a two-layer PyTorch network that provides this interface."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "class AlphaNet(nn.Module):\n    \"\"\"Two-layer feedforward for cross-sectional return prediction.\"\"\"\n\n    def __init__(self, n_features, hidden=32, dropout=0.3):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_features, hidden),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden, hidden // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden // 2, 1),\n        )\n\n    def forward(self, x):\n        return self.net(x).squeeze(-1)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The architecture — 32 neurons in the first hidden layer, 16 in the second, ReLU activation, 30% dropout — matches the GKX NN1/NN2 specification. This is deliberately small. On ~10,000 training observations with 18 features, a larger network would overfit catastrophically. The dropout rate of 0.3 is the regularization mechanism, serving the same role as early stopping in LightGBM. In production, firms like Man Group AHL use similar shallow networks for tabular cross-sectional data; deep architectures (5+ layers) are reserved for unstructured inputs like text and images."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "class NNRegressor:\n    \"\"\"Sklearn-compatible wrapper for PyTorch NN.\"\"\"\n\n    def __init__(self, n_features, hidden=32, dropout=0.3, lr=1e-3,\n                 epochs=30, batch_size=256, device=\"cpu\"):\n        self.n_features = n_features\n        self.hidden = hidden\n        self.dropout = dropout\n        self.lr = lr\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.device = device\n        self.model_ = None",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The wrapper stores hyperparameters in its constructor but delays model creation until `fit()` — a fresh network is initialized for every walk-forward window. This prevents weight leakage across windows, which would be a subtle but serious form of temporal contamination."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def _nn_fit(self, X, y, **kwargs):\n    self.model_ = AlphaNet(self.n_features, self.hidden, self.dropout)\n    X = X.copy()\n    y = y.copy()\n    # Use last 20% as validation for early stopping\n    n_val = max(1, len(X) // 5)\n    X_tr, y_tr = X[:-n_val], y[:-n_val]\n    X_val, y_val = X[-n_val:], y[-n_val:]\n    # Impute NaN for NN (can't handle natively)\n    for j in range(X_tr.shape[1]):\n        med = np.nanmedian(X_tr[:, j])\n        X_tr[np.isnan(X_tr[:, j]), j] = med if np.isfinite(med) else 0.0\n        X_val[np.isnan(X_val[:, j]), j] = med if np.isfinite(med) else 0.0\n    fit_nn(\n        self.model_, X_tr, y_tr,\n        x_val=X_val, y_val=y_val,\n        epochs=self.epochs, lr=self.lr,\n        batch_size=self.batch_size,\n        patience=10, device=self.device,\n    )\n    return self\n\nNNRegressor.fit = _nn_fit",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The NaN imputation happens inside `fit()` using training set medians — the same pattern as the pipeline's imputation, but necessary here because PyTorch tensors cannot contain NaN. The validation split for early stopping uses the last 20% of the training window (temporal, never shuffled). The `fit_nn` function from `shared/dl_training.py` handles the training loop, gradient clipping, and learning rate scheduling."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def _nn_predict(self, X):\n    # Impute NaN\n    X = X.copy()\n    for j in range(X.shape[1]):\n        med = np.nanmedian(X[:, j])\n        X[np.isnan(X[:, j]), j] = med if np.isfinite(med) else 0.0\n    return predict_nn(self.model_, X, device=self.device)\n\nNNRegressor.predict = _nn_predict",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The `predict()` method handles its own NaN imputation using column medians from the prediction cross-section. This is a pragmatic choice — ideally you would store the training medians from `fit()` and apply them at prediction time, but for walk-forward evaluation where each window retrains from scratch, the difference is negligible. The final piece of the sklearn protocol is the parameter introspection methods that `clone()` needs."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def _nn_get_params(self, deep=True):\n    return dict(n_features=self.n_features, hidden=self.hidden,\n                dropout=self.dropout, lr=self.lr, epochs=self.epochs,\n                batch_size=self.batch_size, device=self.device)\n\ndef _nn_set_params(self, **params):\n    for k, v in params.items():\n        setattr(self, k, v)\n    return self\n\ndef _nn_sklearn_clone(self):\n    return NNRegressor(**self.get_params())\n\nNNRegressor.get_params = _nn_get_params\nNNRegressor.set_params = _nn_set_params\nNNRegressor.__sklearn_clone__ = _nn_sklearn_clone",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The `get_params`, `set_params`, and `__sklearn_clone__` methods complete the sklearn protocol. Without these, the pipeline's `clone(self.model)` call would fail. This is the kind of plumbing that is invisible in research notebooks but essential for production compatibility. At a fund, your NNRegressor must be a drop-in replacement for any sklearn model — if it is not, the infrastructure team will reject it.\n\nNow the four model configurations. Each one is defined with its specific settings."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "n_feat = len(feature_cols)\n\nmodel_configs = {\n    \"OLS\": {\n        \"model\": LinearRegression(),\n        \"hp_search\": False,\n        \"impute\": True,\n    },\n    \"Ridge\": {\n        \"model\": Ridge(alpha=1.0),\n        \"hp_search\": False,\n        \"impute\": True,\n    },\n    \"LightGBM\": {\n        \"model\": lgb.LGBMRegressor(\n            n_estimators=200, learning_rate=0.05, num_leaves=31,\n            min_child_samples=20, reg_alpha=0.1, reg_lambda=1.0,\n            subsample=0.8, subsample_freq=1, colsample_bytree=0.8,\n            random_state=42, verbosity=-1,\n        ),\n        \"hp_search\": True,\n        \"impute\": False,\n    },\n    \"NN\": {\n        \"model\": NNRegressor(\n            n_features=n_feat, hidden=32, dropout=0.3,\n            lr=1e-3, epochs=30, batch_size=256, device=\"cpu\",\n        ),\n        \"hp_search\": False,\n        \"impute\": False,  # NNRegressor handles NaN internally\n    },\n}",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "OLS and Ridge use `impute=True` because they cannot handle NaN. LightGBM handles it natively. The NN wrapper imputes inside its own `fit()` and `predict()` methods, so the pipeline's imputation is off. This design — each model managing its own NaN strategy — mirrors production systems where different model families coexist in the same evaluation framework.\n\nTime to run all four models. This is the most compute-intensive step in the homework — each model runs ~68 walk-forward windows, and the NN has the additional overhead of PyTorch training at each window."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "results = {}\nic_series_all = {}\n\nfor name, config in model_configs.items():\n    print(f\"── MODEL: {name} ─────────────────────────────────────\")\n    pipeline = AlphaModelPipeline(\n        model=config[\"model\"],\n        features=expanded_features,\n        target=target,\n        train_window=TRAIN_WINDOW,\n        purge_gap=PURGE_GAP,\n        cost_bps=COST_BPS,\n        hp_search=config[\"hp_search\"],\n        impute=config[\"impute\"],\n    )\n    ic_df, pred_df = pipeline.fit_predict()\n    summary = pipeline.summary()\n    results[name] = summary\n    ic_series_all[name] = ic_df[\"ic_pearson\"].dropna()\n    print(f\"  mean IC: {summary['mean_ic']:.4f}, \"\n          f\"ICIR: {summary['icir']:.4f}, \"\n          f\"Sharpe: {summary['sharpe_gross']:.4f}\")\n    print()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The same `AlphaModelPipeline` class — built in Deliverable 1 — handles all four models without modification. That is the point of building a reusable pipeline: one piece of infrastructure, any model family. A quantitative researcher at Millennium or Citadel uses exactly this pattern — the evaluation framework is fixed, and models are swapped in and out as experiments.\n\nNow the comprehensive comparison table."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"── MODEL COMPARISON SUMMARY TABLE ────────────────────\")\nheader = (f\"{'Model':<10} {'IC':>7} {'σ(IC)':>7} {'ICIR':>7} \"\n          f\"{'pct+':>7} {'t-stat':>7} {'p-val':>7} {'rIC':>7} \"\n          f\"{'SR_g':>7} {'SR_n':>7} {'TO':>7} {'MDD':>7}\")\nprint(f\"  {header}\")\nprint(f\"  {'─' * len(header)}\")\nfor name in model_configs:\n    s = results[name]\n    row = (f\"  {name:<10} \"\n           f\"{s['mean_ic']:>7.4f} {s['std_ic']:>7.4f} {s['icir']:>7.4f} \"\n           f\"{s['pct_positive']:>7.4f} {s['t_stat']:>7.4f} {s['p_value']:>7.4f} \"\n           f\"{s['mean_rank_ic']:>7.4f} \"\n           f\"{s['sharpe_gross']:>7.4f} {s['sharpe_net']:>7.4f} \"\n           f\"{s['mean_turnover']:>7.4f} {s['max_drawdown']:>7.4f}\")\n    print(row)\nprint()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Study this table carefully. The model ranking is OLS/Ridge > LightGBM > NN — the *inverse* of the GKX (2020) production ranking (NN >= GBRT > Ridge > OLS). OLS and Ridge both achieve IC around 0.044, LightGBM at 0.034, and the NN at only 0.018. But here is the critical finding: *no individual model's IC reaches 5% significance* on the expanded feature set (all p-values above 0.11). The entire comparison operates on signals that cannot be statistically distinguished from zero.\n\nThe Sharpe comparison is even more stark: Ridge at approximately 0.83 versus NN at 0.27 — a 3x difference. This is not because neural networks are bad models; it is because on 18 correlated features and 174 S&P 500 stocks, the additional flexibility of a neural network finds noise instead of signal. The linear models cannot overfit as badly, so they produce more stable (albeit equally weak) predictions.\n\nNow the pairwise statistical tests."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "pairs = [(\"Ridge\", \"OLS\"), (\"LightGBM\", \"Ridge\"), (\"NN\", \"LightGBM\")]\n\nprint(\"── PAIRWISE PAIRED T-TESTS ───────────────────────────\")\nfor m1, m2 in pairs:\n    ic1 = ic_series_all[m1].values\n    ic2 = ic_series_all[m2].values\n    n = min(len(ic1), len(ic2))\n    diff = ic1[:n] - ic2[:n]\n    if n >= 2 and diff.std() > 0:\n        t = diff.mean() / (diff.std() / np.sqrt(n))\n        p = 2 * (1 - stats.t.cdf(abs(t), df=n - 1))\n    else:\n        t, p = np.nan, np.nan\n    sig = \"YES\" if (np.isfinite(p) and p < 0.05) else \"no\"\n    print(f\"  {m1} vs {m2}: t={t:+.4f}, p={p:.4f} [{sig}]\")\nprint()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "No pairwise comparison reaches 5% significance (all p-values above 0.26). The IC spread of 0.025 across all four models is within the noise of monthly IC estimation (standard error approximately 0.02). This means we cannot statistically distinguish any model from any other — the comparison is uninformative at this scale. This is itself the most important finding: with 18 features and 174 stocks, model choice does not matter.\n\nThe deflated Sharpe ratio adjusts for multiple testing — the fact that we tried 4 models and picked the best one."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "n_trials = len(model_configs)\nbest_model = max(results, key=lambda m: results[m][\"sharpe_gross\"])\nbest_sharpe = results[best_model][\"sharpe_gross\"]\nbest_n_oos = results[best_model][\"n_oos_months\"]\n\n# Compute return skewness and kurtosis for deflated SR\nbest_ic_series = ic_series_all[best_model]\nskew_val = float(stats.skew(best_ic_series.values))\nkurt_val = float(stats.kurtosis(best_ic_series.values))\n\ndsr = deflated_sharpe_ratio(\n    best_sharpe, n_trials, best_n_oos,\n    skew=skew_val, excess_kurt=kurt_val,\n)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The DSR computation uses the IC series' skewness and excess kurtosis because return distributions in cross-sectional equity strategies are non-Gaussian. Ignoring these higher moments would overstate the statistical significance of the Sharpe ratio — a common mistake in backtesting research. Now let us see whether the best model survives the adjustment."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"── MULTIPLE TESTING AWARENESS ────────────────────────\")\nprint(f\"  Models compared: {n_trials}\")\nprint(f\"  Best model: {best_model} (gross Sharpe = {best_sharpe:.4f})\")\nprint(f\"  Deflated Sharpe Ratio probability: {dsr:.4f}\")\nif dsr > 0.95:\n    print(f\"  Best SR survives deflation (DSR > 0.95)\")\nelse:\n    print(f\"  Best SR does NOT survive deflation (DSR < 0.95)\")\n    print(f\"  With only {n_trials} trials, this mainly indicates a low SR\")\nprint()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The deflated Sharpe ratio (Bailey and Lopez de Prado, 2014) adjusts for the selection bias of picking the best model from multiple trials. With only 4 trials, the adjustment is small — the main driver of the DSR result is the absolute level of the Sharpe ratio, not the number of trials. A DSR above 0.95 means the best model's Sharpe is robust to the multiple-testing adjustment. At a fund that tests hundreds of signals, the DSR correction would be much more severe.\n\nNow the sandbox-vs-production analysis — the section that separates honest research from marketing."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"── SANDBOX vs PRODUCTION ─────────────────────────────\")\nprint(\"  (a) FEATURE GAP:\")\nprint(f\"      Sandbox: {len(feature_cols)} features\")\nprint(f\"      Production (GKX 2020): 94 firm characteristics\")\nprint(f\"      → More features capture richer cross-sectional variation.\")\nprint(f\"         GKX found ~50% OOS R² improvement going from 10 to 94 features.\")\nprint()\nprint(\"  (b) UNIVERSE GAP:\")\nprint(f\"      Sandbox: ~174 S&P 500 stocks\")\nprint(f\"      Production (GKX 2020): ~3,000 CRSP stocks\")\nprint(f\"      → Larger cross-section provides more power to distinguish models\")\nprint(f\"         and includes less-efficient small/mid-caps where alpha is larger.\")\nprint()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The feature and universe gaps compound: fewer features on fewer stocks means the non-linear models have both less information to work with and less statistical power to exploit it. The next two gaps — data quality issues — affect absolute performance more than relative model rankings."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"  (c) PIT CONTAMINATION:\")\nprint(f\"      Sandbox: 4 original + 2 new fundamental features use static ratios\")\nprint(f\"      Production: quarterly filings with proper point-in-time handling\")\nprint(f\"      → Our fundamental features have look-ahead bias. IC may be inflated\")\nprint(f\"         for models that rely on fundamentals. Price-derived features are clean.\")\nprint()\nprint(\"  (d) SURVIVORSHIP BIAS:\")\nprint(f\"      Sandbox: current S&P 500 members applied retroactively\")\nprint(f\"      Production: point-in-time index constituents from CRSP\")\nprint(f\"      → Survivorship bias inflates returns by ~1-2% annualized (Brown 1992).\")\nprint(f\"         All models are equally affected, so relative rankings may be valid.\")\nprint()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Each gap has a directional impact. The feature gap means our models are starved — 18 features versus the 94 that GKX used. The universe gap means our cross-section is too small for non-linear models to find the complex interactions they are designed to exploit. The PIT contamination means our fundamental features are unreliable. And the survivorship bias inflates all returns equally, making the absolute Sharpe numbers unreliable while leaving relative rankings approximately valid.\n\nAt a fund, this analysis would be the first page of the model comparison memo — before the results, before the tables. The CIO needs to know what the results can and cannot tell them before they see a single number.\n\nNow the recommendation — the punchline."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "model_names = list(results.keys())\nmean_ics = [results[m][\"mean_ic\"] for m in model_names]\nbest_ic_model = model_names[np.argmax(mean_ics)]\nworst_ic_model = model_names[np.argmin(mean_ics)]\nic_range = max(mean_ics) - min(mean_ics)\n\nprint(\"── CIO RECOMMENDATION ────────────────────────────────\")\nprint(f\"  RECOMMENDATION: The model comparison does NOT support switching\")\nprint(f\"  from gradient boosted trees to neural networks on this data.\")\nprint()\nprint(f\"  The IC spread across all {n_trials} models is {ic_range:.4f} — within\")\nprint(f\"  the noise of monthly IC estimation (SE ~0.02). No pairwise comparison\")\nprint(f\"  reaches 5% significance. The best model ({best_ic_model}) outperforms\")\nprint(f\"  the worst ({worst_ic_model}) by {ic_range:.4f} in IC, but this is not\")\nprint(f\"  statistically distinguishable from zero.\")\nprint()\nprint(f\"  The case for neural networks requires: (a) unstructured data inputs\")\nprint(f\"  (text, images) that GBM cannot natively handle, (b) a substantially\")\nprint(f\"  larger feature set (94+ characteristics per GKX 2020), or (c) a larger\")\nprint(f\"  cross-section (~3,000 stocks) where non-linear interactions have more\")\nprint(f\"  statistical power. On 174 S&P 500 stocks with 18 features, model\")\nprint(f\"  choice is dominated by noise, not signal.\")\nprint()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "This recommendation is honest, specific, and actionable. It does not say \"neural networks are bad\" — it says \"neural networks are not justified on *this* data, at *this* scale.\" The conditions under which neural networks would be justified are explicit: unstructured data, larger feature sets, larger cross-sections. A CIO reading this knows exactly what to invest in (more data, broader universe) rather than what model to buy.\n\nFinally, the net Sharpe comparison and visualization."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"── NET SHARPE COMPARISON (10 bps one-way) ────────────\")\nfor name in model_configs:\n    s = results[name]\n    print(f\"  {name:<10}: gross={s['sharpe_gross']:.4f}, \"\n          f\"net={s['sharpe_net']:.4f}, \"\n          f\"turnover={s['mean_turnover']:.4f}\")\nany_positive_net = any(results[m][\"sharpe_net\"] > 0 for m in model_configs)\nif any_positive_net:\n    print(f\"  At least one model shows positive net Sharpe at 10 bps\")\nelse:\n    print(f\"  All models show negative net Sharpe — sandbox limitations dominate\")\nprint()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The net Sharpe comparison at 10 bps one-way cost is the metric that matters for production viability. At institutional S&P 500 execution costs (5-15 bps per Frazzini, Israel & Moskowitz, 2018), the linear models' signals survive — but only barely. The NN's net Sharpe of approximately 0.20 is marginal at best. In production, the portfolio construction step (Week 6) would add turnover constraints that reduce costs but also reduce signal utilization — a trade-off that is the daily bread of portfolio management."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\nmodel_labels = list(model_configs.keys())\nx = np.arange(len(model_labels))\nwidth = 0.6\n\n# Panel A: Mean IC\nics = [results[m][\"mean_ic\"] for m in model_labels]\nses = [results[m][\"std_ic\"] / np.sqrt(results[m][\"n_oos_months\"])\n       for m in model_labels]\ncolors = plt.cm.Set2(np.linspace(0, 0.8, len(model_labels)))\naxes[0].bar(x, ics, width, yerr=ses, capsize=4, color=colors)\naxes[0].set_xticks(x)\naxes[0].set_xticklabels(model_labels)\naxes[0].set_ylabel(\"Mean OOS IC\")\naxes[0].set_title(\"Mean IC (±SE)\")\naxes[0].axhline(0, color=\"black\", linewidth=0.5)\n\n# Panel B: ICIR\nicirs = [results[m][\"icir\"] for m in model_labels]\naxes[1].bar(x, icirs, width, color=colors)\naxes[1].set_xticks(x)\naxes[1].set_xticklabels(model_labels)\naxes[1].set_ylabel(\"ICIR\")\naxes[1].set_title(\"Information Coefficient IR\")\naxes[1].axhline(0, color=\"black\", linewidth=0.5)\n\n# Panel C: Gross vs Net Sharpe\ngross = [results[m][\"sharpe_gross\"] for m in model_labels]\nnet = [results[m][\"sharpe_net\"] for m in model_labels]\naxes[2].bar(x - 0.15, gross, 0.3, label=\"Gross\", color=\"#1f77b4\")\naxes[2].bar(x + 0.15, net, 0.3, label=f\"Net ({COST_BPS}bps)\", color=\"#ff7f0e\")\naxes[2].set_xticks(x)\naxes[2].set_xticklabels(model_labels)\naxes[2].set_ylabel(\"Ann. Sharpe\")\naxes[2].set_title(\"Sharpe Ratio: Gross vs Net\")\naxes[2].legend()\naxes[2].axhline(0, color=\"black\", linewidth=0.5)\n\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The three-panel chart tells the complete story at a glance. Panel A shows the IC error bars overlapping heavily — no model is statistically distinguishable from another. Panel B shows ICIR telling a similar story, though LightGBM and the linear models are tightly clustered while the NN falls behind. Panel C shows the economic bottom line: gross Sharpe is positive for all models, but the gap between gross and net bars — the cost of trading — is substantial. Ridge leads in both gross and net Sharpe, not because it is a \"better\" model, but because its linear predictions produce more stable portfolio rankings month to month, resulting in lower turnover."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## What You Built\n\nThe pipeline you constructed over these three deliverables — `AlphaModelPipeline`, the feature expansion, and the model comparison — is a simplified version of what a quantitative researcher at a systematic fund operates every day. The core architecture is the same: walk-forward temporal splitting, signal evaluation via IC, portfolio construction via quantile sorts, and cost-adjusted performance measurement.\n\nThe surprising findings tell a story about scale dependence. On our 174-stock, 18-feature sandbox, feature expansion degraded IC by 0.020 (baseline 0.055 vs. expanded 0.034), the model ranking inverted relative to published results (linear > GBM > NN), and no individual model produced a statistically significant signal on the expanded feature set. These results are not failures — they are the honest outcomes of running production-grade methodology at sandbox scale. The GKX (2020) result, where neural networks marginally outperform gradient boosting on 94 features and 3,000 stocks, requires exactly the scale our sandbox lacks. Understanding *when* complexity helps — and when it hurts — is the meta-lesson that separates a practitioner from a Kaggle competitor.\n\nWhat is still missing? Our pipeline uses a simple quantile sort for portfolio construction — Week 6 will replace this with mean-variance optimization and turnover constraints. Our backtesting has no combinatorial purged cross-validation — Week 5 will add that. Our transaction cost model is a flat per-trade assumption — production models use market-impact functions that scale with order size. And our fundamental features have full look-ahead bias — production systems use point-in-time databases like Compustat's Xpressfeed or Refinitiv's DataScope."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Career Connection\n\nThe `AlphaModelPipeline` you built is a pedagogically transparent version of what Microsoft Qlib and similar open-source frameworks provide for production alpha research. A quantitative researcher at a multi-manager pod shop like Millennium or Citadel owns a pipeline exactly like this — their performance is measured by net Sharpe after costs, which is literally the number your `summary()` method returns. The feature engineering process in Deliverable 2 is the daily work of the feature engineering team at a systematic fund, which often has more headcount than the modeling team. And the CIO recommendation in Deliverable 3 is the kind of memo that junior quants write when their team evaluates new technology — honest, data-driven, and specific about what would change the answer. The skills you exercised here — temporal integrity, domain-motivated feature engineering, honest model comparison — are the ones that get you past the first round of interviews."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Bridge to Next Week\n\nYou now have a cross-sectional alpha model that produces signals and constructs portfolios. But how do you know the results are real? Week 5 takes this pipeline and stress-tests it — purged cross-validation, combinatorial purged CV, the deflated Sharpe ratio (which you briefly touched in D3), and transaction cost modeling that goes far beyond our simple spread estimate. The question shifts from \"can I build a signal?\" to \"should I believe it?\""
  }
 ]
}