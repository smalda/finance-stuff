{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Week 4: ML for Alpha -- From Features to Signals\n\n> *\"All their models -- from Ridge to five-layer networks -- agreed on the same top features. The model was the lens; the features were the light.\"*\n> -- paraphrasing Gu, Kelly & Xiu (2020)\n\nIn 2020, a team at Chicago Booth fed 94 firm characteristics into every ML model they could find -- from Ridge regression to five-layer neural networks -- and discovered something that should unsettle every ML engineer: all the models agreed. Not on the predictions, but on which features mattered. Momentum, liquidity, volatility -- the same handful of signals dominated regardless of model complexity. The implication is disquieting: in cross-sectional equity prediction, the features ARE the alpha. Your model is just the lens.\n\nThis week, we build the complete alpha pipeline: features measured today, returns observed next month, models trained on rolling windows, signals evaluated with metrics the industry actually uses, and a first long-short portfolio constructed from predictions. Along the way, we will confront three uncomfortable truths for ML engineers entering finance. First, signal-to-noise is catastrophically low -- an information coefficient of 0.05 makes you a star. Second, gradient boosting stubbornly matches neural networks on tabular financial data. Third, the features you engineer matter more than the model you choose.\n\nThe goal is not to build the best model. It is to build the judgment to know what \"best\" means in this domain -- and to understand exactly where our sandbox results diverge from institutional reality."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import json\nimport sys\nimport warnings\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nsys.path.insert(0, \"code\")\nfrom data_setup import (\n    load_feature_matrix,\n    load_forward_returns,\n    load_monthly_panel,\n    CACHE_DIR,\n    PLOT_DIR,\n    FEATURE_COLS,\n    TRAIN_WINDOW,\n    PURGE_GAP,\n    COST_BPS,\n)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 1: The Cross-Sectional Prediction Problem\n\nEvery month, a quant researcher lines up thousands of stocks in a row and asks one question: which ones will be in the top decile next month? That single question -- cross-sectional return prediction -- is the economic engine behind most systematic equity funds. Unlike time-series forecasting, where you predict whether the *market* goes up or down, cross-sectional prediction asks which stocks will outperform *relative to each other*. The target is not the level of returns; it is the ranking.\n\nThe framework we follow is the one established by Gu, Kelly and Xiu (2020) in their landmark paper \"Empirical Asset Pricing via Machine Learning.\" They tested dozens of models on 94 firm characteristics measured monthly across the full CRSP universe. We replicate the structure -- features at time *t*, forward returns at *t+1* -- but on a smaller canvas: 7 z-scored features on roughly 174 S&P 500 stocks over 130 months. Every limitation of our sandbox (fewer features, survivorship-biased universe, no point-in-time fundamentals) matters, and we will track each one explicitly."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "fm = load_feature_matrix()\nfwd = load_forward_returns()\npanel = load_monthly_panel()\n\ndates = panel.index.get_level_values(\"date\").unique().sort_values()\nfm_tickers_per_month = fm.groupby(level=\"date\").size()\nfm_dates = fm.index.get_level_values(\"date\").unique().sort_values()\n\nprint(\"── DATA QUALITY ──────────────────────────────────\")\nprint(f\"  Feature matrix shape: {fm.shape}\")\nprint(f\"  Features: {list(fm.columns)}\")\nprint(f\"  Unique months: {fm.index.get_level_values('date').nunique()}\")\nprint(f\"  Tickers per month: {fm_tickers_per_month.min()}–{fm_tickers_per_month.max()}\")\nprint(f\"  Date range: {fm_dates.min().strftime('%Y-%m')} to {fm_dates.max().strftime('%Y-%m')}\")\nmissing_pct = fm.isna().mean()\nprint(f\"  Missing values:\")\nfor col in fm.columns:\n    print(f\"    {col}: {missing_pct[col]:.1%}\")\nprint(f\"  Forward return months: {fwd.index.get_level_values('date').nunique()}\")\nprint(f\"  Forward return mean: {fwd.mean():.6f}\")\nprint(f\"  Forward return std:  {fwd.std():.6f}\")\nprint()\nprint(\"  ⚠ SURVIVORSHIP BIAS: Universe is current S&P 500 constituents\")\nprint(\"    projected backward. Stocks that were removed from the index\")\nprint(\"    (due to decline, acquisition, bankruptcy) are excluded. This\")\nprint(\"    upward-biases mean returns and likely inflates any cross-\")\nprint(\"    sectional signal that correlates with survival probability.\")\nprint(\"    Production systems require point-in-time membership data.\")\nprint(\"──────────────────────────────────────────────────\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We have roughly 174 stocks per month, 130 months of features (March 2014 through December 2024), and 7 z-scored characteristics. The fundamental features (pb_ratio_z, roe_z) have approximately 6% missing values -- a direct consequence of not all S&P 500 companies having complete fundamental data in every period. The technical features (momentum, reversal, volatility) are computed from price data and have near-zero missingness. The survivorship bias warning is not a footnote: our universe consists of *current* S&P 500 constituents projected backward, excluding companies removed due to decline, acquisition, or bankruptcy. This upward-biases all returns and inflates any signal that correlates with survival probability. Production alpha systems use point-in-time index membership data to avoid this contamination."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Cross-Sectional Excess Returns and the Feature-Return Relationship\n\nThe prediction target in cross-sectional models is not raw returns but *excess* returns -- each stock's return relative to the cross-sectional mean that month. This removes common market exposure and isolates the stock-specific component we are trying to predict. By construction, the cross-sectional mean of excess returns is zero in every month. To visualize what cross-sectional prediction looks like, we take a representative month and scatter momentum (our strongest single feature) against excess returns."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "excess_returns = fwd.groupby(level=\"date\").transform(lambda x: x - x.mean())\nexcess_returns.name = \"excess_return\"\n\nmid_date = dates[len(dates) // 2]\ncs = panel.loc[mid_date].dropna(subset=[\"momentum_z\", \"fwd_return\"]).copy()\ncs[\"excess_return\"] = cs[\"fwd_return\"] - cs[\"fwd_return\"].mean()\n\nfig_scatter, ax_scatter = plt.subplots(figsize=(8, 6))\nax_scatter.scatter(cs[\"momentum_z\"], cs[\"excess_return\"], alpha=0.5, s=20, c=\"steelblue\")\nslope, intercept = np.polyfit(cs[\"momentum_z\"], cs[\"excess_return\"], 1)\nx_line = np.linspace(cs[\"momentum_z\"].min(), cs[\"momentum_z\"].max(), 100)\nax_scatter.plot(x_line, slope * x_line + intercept, \"r-\", linewidth=2, label=f\"OLS slope={slope:.4f}\")\nax_scatter.axhline(0, color=\"grey\", linewidth=0.5, linestyle=\"--\")\nax_scatter.axvline(0, color=\"grey\", linewidth=0.5, linestyle=\"--\")\nax_scatter.set(\n    title=f\"Cross-Sectional: Momentum vs Excess Return ({mid_date.strftime('%Y-%m')})\",\n    xlabel=\"Momentum (z-score)\",\n    ylabel=\"Excess return (demeaned)\",\n)\nax_scatter.legend(fontsize=9)\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "This scatter is the defining image of cross-sectional alpha research. The relationship between momentum and next-month returns exists -- the regression line has a positive slope -- but the noise is overwhelming. Any single stock can fall anywhere regardless of its momentum rank. This is not ImageNet, where the correct label is unambiguous. Here, the \"signal\" is a faint statistical tendency visible only across many stocks and many months."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "fig_hist, ax_hist = plt.subplots(figsize=(8, 5))\nax_hist.hist(cs[\"fwd_return\"], bins=30, alpha=0.7, color=\"steelblue\", edgecolor=\"white\")\nax_hist.axvline(cs[\"fwd_return\"].mean(), color=\"red\", linewidth=1.5, linestyle=\"--\", label=\"Mean\")\nax_hist.axvline(cs[\"fwd_return\"].median(), color=\"orange\", linewidth=1.5, linestyle=\"--\", label=\"Median\")\nax_hist.set(\n    title=f\"Cross-Sectional Return Distribution ({mid_date.strftime('%Y-%m')})\",\n    xlabel=\"Forward 1-month return\",\n    ylabel=\"Count\",\n)\nax_hist.legend()\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The roughly bell-shaped distribution with fat tails is typical: extreme returns are more likely than a Gaussian would predict. These outliers are both a source of potential alpha (if you can predict them) and a source of estimation error (if they contaminate model training). Rather than relying on a single month, the Spearman rank correlation between momentum and forward returns across *all* months gives us a time series preview of the information coefficient concept formalized in Section 2."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "monthly_spearman = []\nfor d in dates:\n    cross = panel.loc[d].dropna(subset=[\"momentum_z\", \"fwd_return\"])\n    if len(cross) < 20:\n        continue\n    r, _ = stats.spearmanr(cross[\"momentum_z\"], cross[\"fwd_return\"])\n    monthly_spearman.append({\"date\": d, \"spearman\": r})\n\nspearman_df = pd.DataFrame(monthly_spearman).set_index(\"date\")\nmedian_spearman = spearman_df[\"spearman\"].median()\nmean_spearman = spearman_df[\"spearman\"].mean()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The median monthly Spearman of approximately 0.035 is tiny in absolute terms. But the fundamental law of active management shows how even a faint per-stock correlation, multiplied by the square root of the number of stocks traded, can produce economically meaningful portfolio-level performance. That is the core insight we formalize next."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 2: The Language of Signal Quality\n\nYour model has an R-squared of 0.001. In any other ML domain, you would delete it. In cross-sectional equity prediction, you might have just found a signal worth hundreds of millions of dollars. The reason is that R-squared is the wrong metric. The industry uses a different language: *information coefficient* (IC), *IC information ratio* (ICIR = mean(IC)/std(IC)), and the *fundamental law of active management* (IR = IC x sqrt(BR)). This section introduces all three.\n\nThe IC is simply the correlation between your predicted cross-sectional returns and the realized returns, computed month by month. We compute both Pearson IC and Rank IC (Spearman) -- the latter is more robust to outliers. The Fama-MacBeth procedure provides our baseline signal: cross-sectional OLS each month, accumulating coefficients, predicting with the historical average -- a genuinely out-of-sample linear signal. Features are rank-transformed within each cross-section before regression, removing outlier leverage."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def _fm_one_month(panel, features, target, dt, beta_history, min_history):\n    \"\"\"Run OLS for one cross-section; return (pred_series|None, beta|None).\"\"\"\n    cross = panel.loc[dt].dropna(subset=features + [target])\n    if len(cross) < 10:\n        return None, None\n    X_rank = cross[features].rank(pct=True).values\n    y = cross[target].values\n    y_dm = y - y.mean()\n    X_const = np.column_stack([np.ones(len(X_rank)), X_rank])\n    try:\n        beta = np.linalg.lstsq(X_const, y_dm, rcond=None)[0]\n    except np.linalg.LinAlgError:\n        return None, None\n    pred_series = None\n    if len(beta_history) >= min_history:\n        avg_beta = np.mean(beta_history, axis=0)\n        fitted = X_const @ avg_beta\n        pred_series = pd.Series(fitted, index=cross.index, name=\"prediction\")\n        pred_series = pred_series.to_frame()\n        pred_series[\"date\"] = dt\n        pred_series = pred_series.set_index(\"date\", append=True).swaplevel()\n        pred_series = pred_series[\"prediction\"]\n    return pred_series, beta\n\n\ndef fama_macbeth_predict(panel, features, target, min_history=12):\n    \"\"\"Expanding-window Fama-MacBeth signal: historical betas predict next month.\"\"\"\n    predictions = []\n    month_dates = panel.index.get_level_values(\"date\").unique().sort_values()\n    beta_history = []\n    for i, dt in enumerate(month_dates):\n        if i % 20 == 0:\n            print(f\"  [{i+1}/{len(month_dates)}] Fama-MacBeth OLS for {dt.date()}\")\n        pred_series, beta = _fm_one_month(panel, features, target, dt, beta_history, min_history)\n        if beta is not None:\n            beta_history.append(beta)\n        if pred_series is not None:\n            predictions.append(pred_series)\n    return pd.concat(predictions)\n\n\nfeatures = FEATURE_COLS\nTARGET = \"fwd_return\"\nfm_predictions = fama_macbeth_predict(panel, features, TARGET, min_history=12)\nn_pred_months = fm_predictions.index.get_level_values(\"date\").nunique()\nprint(f\"  Fama-MacBeth predictions: {len(fm_predictions)} obs, \"\n      f\"{n_pred_months} months (after 12-month burn-in)\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "After the 12-month burn-in, we have out-of-sample predictions for over 100 months. Now we compute the IC series and its summary statistics."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def _ic_one_month(pred_dt, act_dt, method):\n    \"\"\"Compute IC for a single month cross-section.\"\"\"\n    if len(pred_dt) < 10:\n        return None\n    if method == \"spearman\":\n        corr, _ = stats.spearmanr(pred_dt.values, act_dt.values)\n    else:\n        corr = np.corrcoef(pred_dt.values, act_dt.values)[0, 1]\n    return corr if np.isfinite(corr) else None\n\n\ndef compute_ic_series(predictions, actuals, method=\"pearson\"):\n    \"\"\"Compute monthly cross-sectional IC (information coefficient).\"\"\"\n    common = predictions.index.intersection(actuals.index)\n    pred = predictions.loc[common]\n    act = actuals.loc[common]\n    dates_all = pred.index.get_level_values(\"date\").unique().sort_values()\n    ic_vals = {}\n    for dt in dates_all:\n        corr = _ic_one_month(pred.loc[dt], act.loc[dt], method)\n        if corr is not None:\n            ic_vals[dt] = corr\n    return pd.Series(ic_vals, name=f\"IC_{method}\")\n\n\ndef ic_summary(ic_series):\n    \"\"\"Compute IC summary: mean, std, ICIR, t-stat, p-value, pct_positive.\"\"\"\n    n = len(ic_series)\n    mean_ic = ic_series.mean()\n    std_ic = ic_series.std(ddof=1)\n    icir = mean_ic / std_ic if std_ic > 0 else 0.0\n    t_stat = mean_ic / (std_ic / np.sqrt(n)) if std_ic > 0 else 0.0\n    p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df=n - 1))\n    pct_positive = (ic_series > 0).mean()\n    return dict(mean_ic=mean_ic, std_ic=std_ic, icir=icir,\n                t_stat=t_stat, p_value=p_value, pct_positive=pct_positive, n_months=n)\n\n\nactuals = panel[TARGET]\npearson_ic = compute_ic_series(fm_predictions, actuals, method=\"pearson\")\nrank_ic = compute_ic_series(fm_predictions, actuals, method=\"spearman\")\npearson_summary = ic_summary(pearson_ic)\nrank_summary = ic_summary(rank_ic)\n\nprint(f\"\\n  Pearson IC summary:\")\nfor k, v in pearson_summary.items():\n    print(f\"    {k}: {v:.6f}\" if isinstance(v, float) else f\"    {k}: {v}\")\nprint(f\"\\n  Rank IC summary:\")\nfor k, v in rank_summary.items():\n    print(f\"    {k}: {v:.6f}\" if isinstance(v, float) else f\"    {k}: {v}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The mean Pearson IC of approximately 0.047 is statistically significant (t = 2.54, p = 0.013) and sits at the upper end of the 0.02-0.05 production range documented in GKX (2020). But the IC standard deviation of roughly 0.199 -- four times the mean -- tells the real story: any individual month's IC is almost meaningless. The ICIR of 0.234 is the stability measure that matters operationally. The signal is positive about 55% of the time -- barely better than a coin flip in any given month, but statistically significant over many months."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### The Fundamental Law of Active Management\n\nGrinold and Kahn (2000) formalized the relationship: **IR = IC x sqrt(BR)**. The predicted IR of approximately 0.615 is nearly 2.6x the actual ICIR of 0.234. This gap exists because the fundamental law assumes independent bets -- stocks with uncorrelated returns. In reality, S&P 500 stocks are heavily correlated, so effective breadth is far less than 174."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "tickers_per_month = panel.groupby(level=\"date\").size()\nBR = tickers_per_month.median()\nmean_ic = pearson_summary[\"mean_ic\"]\npredicted_ir = mean_ic * np.sqrt(BR)\nactual_ir = pearson_summary[\"icir\"]\n\nprint(f\"\\n  Fundamental Law of Active Management:\")\nprint(f\"    IC  = {mean_ic:.4f}\")\nprint(f\"    BR  = {BR:.0f} (median stocks per month)\")\nprint(f\"    Predicted IR = IC × √BR = {mean_ic:.4f} × √{BR:.0f} = {predicted_ir:.4f}\")\nprint(f\"    Actual ICIR  = {actual_ir:.4f}\")\nprint(f\"    Ratio (predicted / actual) = {predicted_ir / actual_ir:.2f}\"\n      if actual_ir != 0 else \"    Actual ICIR is zero — cannot compute ratio\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### IC Visualization\n\nThe IC bar chart below is the most important visualization in cross-sectional alpha research. Individual months swing from approximately -0.48 to +0.67, yet the mean IC (dashed line) is barely visible in the noise. If you are accustomed to ML domains where a good model produces consistently correct predictions, this chart is the moment of recalibration. The three-panel decomposition shows the IC distribution straddling zero, the Pearson-vs-Rank scatter hugging the diagonal, and the ICIR components."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "fig_ic, ax_ic = plt.subplots(figsize=(12, 5))\ncolors = [\"steelblue\" if v >= 0 else \"salmon\" for v in pearson_ic.values]\nax_ic.bar(range(len(pearson_ic)), pearson_ic.values, color=colors, width=1.0)\nax_ic.axhline(0, color=\"black\", linewidth=0.5)\nax_ic.axhline(\n    pearson_summary[\"mean_ic\"], color=\"navy\", linewidth=1.5,\n    linestyle=\"--\", label=f'Mean IC = {pearson_summary[\"mean_ic\"]:.4f}',\n)\nax_ic.set(\n    title=\"Monthly Information Coefficient (Fama-MacBeth Linear Signal)\",\n    xlabel=\"Month index\",\n    ylabel=\"IC (Pearson)\",\n)\nax_ic.legend(loc=\"upper right\")\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "IC is the universal currency of the alpha industry. Every signal -- whether from gradient boosting, a neural network, or a fundamental analyst's intuition -- is reduced to this single number. Understanding it is non-negotiable."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "fig_decomp, axes = plt.subplots(1, 3, figsize=(14, 4))\naxes[0].hist(pearson_ic.values, bins=20, color=\"steelblue\", edgecolor=\"white\", alpha=0.8)\naxes[0].axvline(pearson_summary[\"mean_ic\"], color=\"navy\", linestyle=\"--\", linewidth=1.5)\naxes[0].set(title=\"IC Distribution\", xlabel=\"IC\", ylabel=\"Count\")\n\naxes[1].scatter(pearson_ic.values, rank_ic.values, s=12, alpha=0.5, color=\"steelblue\")\ndiag_min = min(pearson_ic.min(), rank_ic.min())\ndiag_max = max(pearson_ic.max(), rank_ic.max())\naxes[1].plot([diag_min, diag_max], [diag_min, diag_max], \"k--\", linewidth=0.8)\naxes[1].set(title=\"Pearson IC vs Rank IC\", xlabel=\"Pearson IC\", ylabel=\"Rank IC\")\n\nic_labels = [\"Pearson\", \"Rank\"]\nic_means = [pearson_summary[\"mean_ic\"], rank_summary[\"mean_ic\"]]\nic_stds = [pearson_summary[\"std_ic\"], rank_summary[\"std_ic\"]]\nicirs = [pearson_summary[\"icir\"], rank_summary[\"icir\"]]\nx = np.arange(len(ic_labels))\nwidth = 0.3\naxes[2].bar(x - width, ic_means, width, label=\"Mean IC\", color=\"steelblue\")\naxes[2].bar(x, ic_stds, width, label=\"Std IC\", color=\"salmon\")\naxes[2].bar(x + width, icirs, width, label=\"ICIR\", color=\"seagreen\")\naxes[2].set_xticks(x)\naxes[2].set_xticklabels(ic_labels)\naxes[2].set(title=\"ICIR Decomposition\")\naxes[2].legend(fontsize=8)\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Now we know how to measure signals. Time to build one with gradient boosting."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 3: Gradient Boosting for Cross-Sectional Alpha\n\nAsk a quant at a top systematic fund what model they use for cross-sectional equity prediction, and the honest answer is usually three words: LightGBM or XGBoost. Not a transformer. Not a diffusion model. A gradient boosted tree from 2016. This is not because quant funds are technologically conservative -- it is because gradient boosting dominates tabular cross-sectional data for structural reasons: automatic feature interaction detection, robustness to outliers and missing values (LightGBM handles NaN natively), fast training, and interpretable feature importance.\n\nThe evaluation follows a strict walk-forward protocol: train on a 60-month rolling window, skip one month (the purge gap), then predict the next month. The `PurgedWalkForwardCV` splitter implements temporal cross-validation with a purge gap between train and validation folds -- finance-specific infrastructure that standard sklearn `TimeSeriesSplit` does not provide. Hyperparameters are tuned once on the first window and reused throughout."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import lightgbm as lgb\n\nsys.path.insert(0, str(Path(\"code\").resolve().parents[1]))\nfrom shared.temporal import walk_forward_splits, PurgedWalkForwardCV\n\npanel = load_monthly_panel()\ndates = panel.index.get_level_values(\"date\").unique().sort_values()\nfeatures = panel[FEATURE_COLS]\ntarget = panel[\"fwd_return\"]\n\nn_months = len(dates)\ntickers_per_month = panel.groupby(level=\"date\").size()\nnan_rate = features.isna().mean()\n\nprint(\"── DATA QUALITY ──────────────────────────────────────\")\nprint(f\"  Panel shape: {panel.shape}\")\nprint(f\"  Feature months: {n_months}\")\nprint(f\"  Tickers/month: {tickers_per_month.min()}–{tickers_per_month.max()}\")\nprint(f\"  Target (fwd_return) mean: {target.mean():.6f}\")\nprint(f\"  Target std: {target.std():.6f}\")\nprint(f\"  Missing rates:\")\nfor col in FEATURE_COLS:\n    print(f\"    {col}: {nan_rate[col]:.2%}\")\nprint(f\"  NOTE: LightGBM handles NaN natively (no imputation needed)\")\nprint(f\"  SURVIVORSHIP BIAS: universe is current S&P 500 — results overstate\")\nprint()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Hyperparameter Search and Walk-Forward Training\n\nWe search over learning rate and number of leaves on the first training window, using purged temporal CV. Early stopping determines the optimal number of trees within [10, 500]. On this low-signal data, the resulting `best_n_estimators` is often quite small (around 10), reflecting aggressive regularization -- more trees means more overfitting."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "first_train_dates = dates[:TRAIN_WINDOW]\nfirst_train_mask = panel.index.get_level_values(\"date\").isin(first_train_dates)\nX_first = features.loc[first_train_mask].values\ny_first = target.loc[first_train_mask].values\n\nparam_grid = {\"learning_rate\": [0.005, 0.01, 0.05], \"num_leaves\": [15, 31, 63]}\ncv_splitter = PurgedWalkForwardCV(n_splits=3, purge_gap=1)\nbest_score = -np.inf\nbest_params = {\"learning_rate\": 0.05, \"num_leaves\": 31}\nbest_n_estimators = 100\n\nprint(\"── HYPERPARAMETER SEARCH ─────────────────────────────\")\nprint(f\"  Training window: {TRAIN_WINDOW} months ({len(X_first)} obs)\")\nprint(f\"  CV: PurgedWalkForwardCV(n_splits=3, purge_gap=1)\")\nprint(f\"  Grid: lr={param_grid['learning_rate']}, leaves={param_grid['num_leaves']}\")\n\n\ndef hp_eval_one_fold(X_tr, y_tr, X_val, y_val, lr, nl):\n    \"\"\"Train one LightGBM fold and return (ic, n_iters).\"\"\"\n    mdl = lgb.LGBMRegressor(\n        n_estimators=500, learning_rate=lr, num_leaves=nl, min_child_samples=20,\n        reg_alpha=0.1, reg_lambda=1.0, subsample=0.8, subsample_freq=1,\n        colsample_bytree=0.8, random_state=42, verbosity=-1,\n    )\n    mdl.fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n            callbacks=[lgb.early_stopping(50, verbose=False)])\n    preds_cv = mdl.predict(X_val)\n    return np.corrcoef(preds_cv, y_val)[0, 1], mdl.best_iteration_\n\n\nfor lr in param_grid[\"learning_rate\"]:\n    for nl in param_grid[\"num_leaves\"]:\n        fold_scores, fold_iters = [], []\n        for train_idx, val_idx in cv_splitter.split(X_first):\n            ic_cv, n_iters = hp_eval_one_fold(\n                X_first[train_idx], y_first[train_idx],\n                X_first[val_idx], y_first[val_idx], lr, nl)\n            if np.isfinite(ic_cv):\n                fold_scores.append(ic_cv)\n                fold_iters.append(n_iters)\n        mean_cv_ic = np.mean(fold_scores) if fold_scores else -np.inf\n        if mean_cv_ic > best_score:\n            best_score = mean_cv_ic\n            best_params = {\"learning_rate\": lr, \"num_leaves\": nl}\n            best_n_estimators = max(10, int(np.mean(fold_iters)))\n\nprint(f\"  Best: lr={best_params['learning_rate']}, \"\n      f\"leaves={best_params['num_leaves']}, \"\n      f\"n_est={best_n_estimators}, CV IC={best_score:.4f}\")\nprint()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Now the walk-forward loop: for each of approximately 68 windows, train on 60 months (with the last 12 held out for early stopping), skip the purge gap, and predict the next month."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def train_predict_one_window(panel, features, target, train_dates, pred_date, best_params):\n    \"\"\"Train GBM on one window and predict OOS month.\"\"\"\n    train_mask = panel.index.get_level_values(\"date\").isin(train_dates)\n    n_val_months = min(12, len(train_dates) // 5)\n    val_dates, fit_dates = train_dates[-n_val_months:], train_dates[:-n_val_months]\n    fit_mask = panel.index.get_level_values(\"date\").isin(fit_dates)\n    val_mask = panel.index.get_level_values(\"date\").isin(val_dates)\n\n    X_fit, y_fit = features.loc[fit_mask].values, target.loc[fit_mask].values\n    X_val, y_val = features.loc[val_mask].values, target.loc[val_mask].values\n\n    model = lgb.LGBMRegressor(\n        n_estimators=500, learning_rate=best_params[\"learning_rate\"],\n        num_leaves=best_params[\"num_leaves\"], min_child_samples=20,\n        reg_alpha=0.1, reg_lambda=1.0, subsample=0.8, subsample_freq=1,\n        colsample_bytree=0.8, random_state=42, verbosity=-1,\n    )\n    model.fit(X_fit, y_fit, eval_set=[(X_val, y_val)],\n              callbacks=[lgb.early_stopping(50, verbose=False)])\n\n    pred_mask = panel.index.get_level_values(\"date\") == pred_date\n    X_pred, y_pred = features.loc[pred_mask].values, target.loc[pred_mask].values\n    tickers_pred = panel.loc[pred_mask].index.get_level_values(\"ticker\")\n    preds = model.predict(X_pred)\n\n    pred_df = pd.DataFrame({\"date\": pred_date, \"ticker\": tickers_pred,\n                            \"prediction\": preds, \"actual\": y_pred})\n    oos_ic = np.corrcoef(preds, y_pred)[0, 1]\n    train_ic = np.corrcoef(model.predict(X_fit), y_fit)[0, 1]\n    return pred_df, oos_ic, train_ic, model.best_iteration_\n\n\noos_predictions, oos_ic_list, train_ic_list, early_stop_rounds = [], [], [], []\nsplits = list(walk_forward_splits(dates, TRAIN_WINDOW, PURGE_GAP))\nn_splits = len(splits)\n\nprint(f\"── WALK-FORWARD TRAINING ({n_splits} windows) ───────\")\nfor i, (train_dates, pred_date) in enumerate(splits):\n    if i % 10 == 0:\n        print(f\"  [{i+1}/{n_splits}] predicting {pd.Timestamp(pred_date).date()}\")\n    pred_df, oos_ic, train_ic, n_trees = train_predict_one_window(\n        panel, features, target, train_dates, pred_date, best_params)\n    oos_predictions.append(pred_df)\n    early_stop_rounds.append(n_trees)\n    if np.isfinite(oos_ic):\n        oos_ic_list.append({\"date\": pred_date, \"ic\": oos_ic})\n    if np.isfinite(train_ic):\n        train_ic_list.append(train_ic)\n\nprint(f\"  Walk-forward complete: {len(oos_ic_list)} OOS months\")\nprint()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### OOS Signal Quality, Overfitting, and Baseline Comparison\n\nThe GBM achieves a mean OOS IC of approximately 0.046 (t = 2.15, p = 0.035) -- statistically significant. But the train/OOS IC ratio of approximately 6.7 is a screaming overfitting signal: the model memorizes far more structure than generalizes. The acid test is whether the multi-feature GBM outperforms a trivially simple single-feature momentum signal -- and it does not (paired p = 0.57). The additional complexity buys marginal, statistically undetectable improvement."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "all_preds = pd.concat(oos_predictions, ignore_index=True).set_index([\"date\", \"ticker\"])\nic_series = pd.DataFrame(oos_ic_list).set_index(\"date\")[\"ic\"]\n\nmean_ic = ic_series.mean()\nstd_ic = ic_series.std()\nicir = mean_ic / std_ic if std_ic > 0 else np.nan\npct_positive = (ic_series > 0).mean()\nn_ic = len(ic_series)\nt_stat = mean_ic / (std_ic / np.sqrt(n_ic)) if std_ic > 0 and n_ic >= 2 else np.nan\np_value = 2 * (1 - stats.t.cdf(abs(t_stat), df=n_ic - 1)) if np.isfinite(t_stat) else np.nan\n\nrank_ic_list = []\nfor date, group in all_preds.groupby(level=\"date\"):\n    r = stats.spearmanr(group[\"prediction\"], group[\"actual\"])[0]\n    if np.isfinite(r):\n        rank_ic_list.append(r)\nmean_rank_ic = np.mean(rank_ic_list)\n\nmean_train_ic = np.mean(train_ic_list)\noos_ratio = mean_train_ic / mean_ic if mean_ic != 0 else np.inf\n\nprint(\"── OOS SIGNAL QUALITY ────────────────────────────────\")\nprint(f\"  OOS months: {n_ic}\")\nprint(f\"  Mean Pearson IC: {mean_ic:.4f}\")\nprint(f\"  Mean Rank IC: {mean_rank_ic:.4f}\")\nprint(f\"  Std IC: {std_ic:.4f},  ICIR: {icir:.4f},  pct_positive: {pct_positive:.4f}\")\nprint(f\"  t-stat: {t_stat:.4f},  p-value: {p_value:.4f}\")\nprint(f\"  Mean train IC: {mean_train_ic:.4f},  Train/OOS ratio: {oos_ratio:.2f}\")\nif mean_train_ic > 2 * mean_ic:\n    print(f\"  ⚠ OVERFIT: train IC ({mean_train_ic:.4f}) > 2 × OOS IC ({mean_ic:.4f})\")\nprint()\n\n# Naive baseline comparison\ndef vs_naive_baseline(ic_model, ic_naive):\n    \"\"\"Paired t-test: model IC vs naive.\"\"\"\n    m, b = np.asarray(ic_model, dtype=float), np.asarray(ic_naive, dtype=float)\n    mask = np.isfinite(m) & np.isfinite(b)\n    m, b = m[mask], b[mask]\n    n = len(m)\n    if n < 2: return dict(mean_improvement=np.nan, t_stat=np.nan, p_value=np.nan, significant_5pct=False)\n    diff = m - b\n    se = diff.std() / np.sqrt(n)\n    if se == 0: return dict(mean_improvement=float(diff.mean()), t_stat=np.nan, p_value=np.nan, significant_5pct=False)\n    t = float(diff.mean() / se)\n    p = float(2 * (1 - stats.t.cdf(abs(t), df=n - 1)))\n    return dict(mean_improvement=float(diff.mean()), t_stat=t, p_value=p, significant_5pct=bool(p < 0.05))\n\nnaive_ic_list = []\nfor date in ic_series.index.tolist():\n    group = all_preds.loc[date]\n    fm_mask = panel.index.get_level_values(\"date\") == date\n    fm_slice = panel.loc[fm_mask]\n    common = group.index.intersection(fm_slice.index.get_level_values(\"ticker\"))\n    if len(common) < 10: continue\n    naive_r = np.corrcoef(fm_slice.loc[(date, common), \"momentum_z\"].values,\n                          group.loc[common, \"actual\"].values)[0, 1]\n    if np.isfinite(naive_r): naive_ic_list.append(naive_r)\n\nbaseline_result = vs_naive_baseline(ic_series.values[:len(naive_ic_list)], np.array(naive_ic_list))\nprint(\"── NAIVE BASELINE COMPARISON ─────────────────────────\")\nprint(f\"  Naive (momentum_z) mean IC: {np.mean(naive_ic_list):.4f}\")\nprint(f\"  GBM mean IC: {mean_ic:.4f}\")\nprint(f\"  Paired t-stat: {baseline_result['t_stat']:.4f},  p-value: {baseline_result['p_value']:.4f}\")\nprint(f\"  Significant (5%): {baseline_result['significant_5pct']}\")\nprint()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Monthly IC and Cumulative IC"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "colors = [\"#2ca02c\" if v > 0 else \"#d62728\" for v in ic_series.values]\nfig_ic, ax_ic = plt.subplots(figsize=(12, 5))\nax_ic.bar(range(len(ic_series)), ic_series.values, color=colors, width=0.8)\nax_ic.axhline(0, color=\"black\", linewidth=0.5)\nax_ic.axhline(mean_ic, color=\"blue\", linestyle=\"--\", linewidth=1, label=f\"Mean IC = {mean_ic:.4f}\")\nax_ic.set_xlabel(\"OOS Month\")\nax_ic.set_ylabel(\"Information Coefficient\")\nax_ic.set_title(\"GBM Walk-Forward: Monthly OOS IC\")\nax_ic.legend()\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The monthly IC bar chart shows the same dramatic swings as the Fama-MacBeth version -- both models extract the same underlying cross-sectional structure from the same 7 features. The cumulative IC below confirms the statistical significance despite the month-to-month noise: the upward trend is real, just very weak."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "cumulative_ic = ic_series.cumsum()\nfig_cum, ax_cum = plt.subplots(figsize=(12, 5))\nax_cum.plot(range(len(cumulative_ic)), cumulative_ic.values, color=\"#1f77b4\", linewidth=1.5)\nax_cum.axhline(0, color=\"black\", linewidth=0.5)\nax_cum.set_xlabel(\"OOS Month\")\nax_cum.set_ylabel(\"Cumulative IC\")\nax_cum.set_title(\"GBM Walk-Forward: Cumulative OOS IC\")\nax_cum.fill_between(range(len(cumulative_ic)), 0, cumulative_ic.values, alpha=0.15, color=\"#1f77b4\")\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "This is the production workhorse. If you join a systematic equity fund, your first model will likely be some variant of this. The obvious question: can a neural network do better?"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "all_preds.to_parquet(CACHE_DIR / \"gbm_predictions.parquet\")\nic_series.to_frame(\"ic\").to_parquet(CACHE_DIR / \"gbm_ic_series.parquet\")\nwith open(CACHE_DIR / \"gbm_best_params.json\", \"w\") as f:\n    json.dump(best_params, f, indent=2)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 4: Neural Networks vs. Trees -- The Honest Comparison\n\nKelly, Malamud, and Zhou proved in 2023 that more complex models should outperform simpler ones for return prediction -- theoretically. Their \"Virtue of Complexity\" requires scale: 94+ features, 3,000+ stocks. On our 7 features and 174 S&P 500 stocks, the answer is less satisfying. We train a two-layer feedforward network (AlphaNet) with ReLU and dropout, mirroring GKX (2020) at reduced scale. Unlike LightGBM, the NN cannot handle NaN natively, so missing values are replaced with zero (the cross-sectional median for z-scored features)."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import torch\nimport torch.nn as nn\nfrom shared.dl_training import fit_nn, predict_nn\n\npanel = load_monthly_panel()\ndates = panel.index.get_level_values(\"date\").unique().sort_values()\nfeatures = panel[FEATURE_COLS]\ntarget = panel[\"fwd_return\"]\ngbm_ic_df = pd.read_parquet(CACHE_DIR / \"gbm_ic_series.parquet\")\ngbm_ic_series = gbm_ic_df[\"ic\"]\nn_features = len(FEATURE_COLS)\n\n\nclass AlphaNet(nn.Module):\n    \"\"\"Two-layer feedforward network for cross-sectional alpha prediction.\"\"\"\n    def __init__(self, n_input, hidden_size=32, dropout=0.3):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_input, hidden_size), nn.ReLU(), nn.Dropout(dropout),\n            nn.Linear(hidden_size, hidden_size // 2), nn.ReLU(), nn.Dropout(dropout),\n            nn.Linear(hidden_size // 2, 1),\n        )\n    def forward(self, x):\n        return self.net(x).squeeze(-1)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "With a hidden size of 32, this network has roughly 785 parameters on approximately 10,440 training observations -- a tight but workable ratio with dropout. The HP search and walk-forward loop mirror Section 3 exactly: same windows, same purge gap, same prediction dates."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "device = \"cpu\"\nfirst_train_dates = dates[:TRAIN_WINDOW]\nfirst_train_mask = panel.index.get_level_values(\"date\").isin(first_train_dates)\nX_first = np.nan_to_num(features.loc[first_train_mask].values.astype(np.float32), nan=0.0)\ny_first = target.loc[first_train_mask].values.astype(np.float32)\n\nhp_grid = {\"lr\": [1e-4, 1e-3, 1e-2], \"hidden_size\": [16, 32, 64], \"dropout\": [0.1, 0.3, 0.5]}\ncv_splitter = PurgedWalkForwardCV(n_splits=3, purge_gap=1)\nbest_cv_ic = -np.inf\nbest_hp = {\"lr\": 1e-3, \"hidden_size\": 32, \"dropout\": 0.3}\nn_combos = len(hp_grid[\"lr\"]) * len(hp_grid[\"hidden_size\"]) * len(hp_grid[\"dropout\"])\ncombo_idx = 0\nprint(f\"── NN HYPERPARAMETER SEARCH ({n_combos} combos) ─────\")\n\nfor lr in hp_grid[\"lr\"]:\n    for hs in hp_grid[\"hidden_size\"]:\n        for dp in hp_grid[\"dropout\"]:\n            combo_idx += 1\n            fold_ics = []\n            for train_idx, val_idx in cv_splitter.split(X_first):\n                model_cv = AlphaNet(n_features, hidden_size=hs, dropout=dp)\n                fit_nn(model_cv, X_first[train_idx], y_first[train_idx],\n                       x_val=X_first[val_idx], y_val=y_first[val_idx],\n                       epochs=30, lr=lr, patience=10, batch_size=256, device=device)\n                ic_cv = np.corrcoef(predict_nn(model_cv, X_first[val_idx], device=device),\n                                    y_first[val_idx])[0, 1]\n                if np.isfinite(ic_cv): fold_ics.append(ic_cv)\n            mean_cv = np.mean(fold_ics) if fold_ics else -np.inf\n            if combo_idx % 9 == 0 or combo_idx == n_combos:\n                print(f\"  [{combo_idx}/{n_combos}] lr={lr}, hs={hs}, dp={dp} -> CV IC={mean_cv:.4f}\")\n            if mean_cv > best_cv_ic:\n                best_cv_ic = mean_cv\n                best_hp = {\"lr\": lr, \"hidden_size\": hs, \"dropout\": dp}\n\nprint(f\"  Best: lr={best_hp['lr']}, hidden={best_hp['hidden_size']}, \"\n      f\"dropout={best_hp['dropout']}, CV IC={best_cv_ic:.4f}\")\nprint()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Walk-Forward NN Training and Head-to-Head Comparison"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def nn_train_predict_one(panel, features, target, train_dates, pred_date,\n                         best_hp, n_features, max_epochs, patience, device):\n    \"\"\"Train NN on one window and predict OOS month.\"\"\"\n    n_val_months = min(12, len(train_dates) // 5)\n    fit_dates, val_dates = train_dates[:-n_val_months], train_dates[-n_val_months:]\n    fit_mask = panel.index.get_level_values(\"date\").isin(fit_dates)\n    val_mask = panel.index.get_level_values(\"date\").isin(val_dates)\n    X_fit = np.nan_to_num(features.loc[fit_mask].values.astype(np.float32), nan=0.0)\n    y_fit = target.loc[fit_mask].values.astype(np.float32)\n    X_val = np.nan_to_num(features.loc[val_mask].values.astype(np.float32), nan=0.0)\n    y_val = target.loc[val_mask].values.astype(np.float32)\n\n    model = AlphaNet(n_features, hidden_size=best_hp[\"hidden_size\"], dropout=best_hp[\"dropout\"])\n    info = fit_nn(model, X_fit, y_fit, x_val=X_val, y_val=y_val,\n                  epochs=max_epochs, lr=best_hp[\"lr\"], patience=patience, batch_size=256, device=device)\n\n    pred_mask = panel.index.get_level_values(\"date\") == pred_date\n    X_pred = np.nan_to_num(features.loc[pred_mask].values.astype(np.float32), nan=0.0)\n    y_pred = target.loc[pred_mask].values\n    preds = predict_nn(model, X_pred, device=device)\n    pred_df = pd.DataFrame({\"date\": pred_date, \"ticker\": panel.loc[pred_mask].index.get_level_values(\"ticker\"),\n                            \"prediction\": preds, \"actual\": y_pred})\n    oos_ic = np.corrcoef(preds, y_pred)[0, 1]\n    train_ic = np.corrcoef(predict_nn(model, X_fit, device=device), y_fit)[0, 1]\n    return pred_df, oos_ic, train_ic, info[\"final_epoch\"]\n\n\noos_predictions, oos_ic_list, train_ic_list, stopping_epochs = [], [], [], []\nsplits = list(walk_forward_splits(dates, TRAIN_WINDOW, PURGE_GAP))\nn_splits = len(splits)\nprint(f\"── NN WALK-FORWARD ({n_splits} windows) ─────────────\")\n\nfor i, (train_dates, pred_date) in enumerate(splits):\n    if i % 10 == 0:\n        print(f\"  [{i+1}/{n_splits}] predicting {pd.Timestamp(pred_date).date()}\")\n    pred_df, oos_ic, train_ic, final_epoch = nn_train_predict_one(\n        panel, features, target, train_dates, pred_date,\n        best_hp, n_features, 50, 10, device)\n    oos_predictions.append(pred_df)\n    stopping_epochs.append(final_epoch)\n    if np.isfinite(oos_ic): oos_ic_list.append({\"date\": pred_date, \"ic\": oos_ic})\n    if np.isfinite(train_ic): train_ic_list.append(train_ic)\n\nprint(f\"  Walk-forward complete: {len(oos_ic_list)} OOS months\")\nprint()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The NN achieves a mean IC of approximately 0.046 -- virtually identical to the GBM. But the t-statistic is lower (approximately 1.74, p = 0.087), making the NN borderline at the 5% level while the GBM signal is significant. Same point estimate, different statistical confidence. Interestingly, the NN overfits *less* (train/OOS ratio approximately 2.2 vs. 6.7 for GBM). The paired p-value of approximately 0.99 means the two models are statistically indistinguishable on this data."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "all_nn_preds = pd.concat(oos_predictions, ignore_index=True).set_index([\"date\", \"ticker\"])\nnn_ic_series = pd.DataFrame(oos_ic_list).set_index(\"date\")[\"ic\"]\n\nmean_nn_ic = nn_ic_series.mean()\nstd_nn_ic = nn_ic_series.std()\nnn_icir = mean_nn_ic / std_nn_ic if std_nn_ic > 0 else np.nan\nn_nn = len(nn_ic_series)\nnn_t = mean_nn_ic / (std_nn_ic / np.sqrt(n_nn)) if std_nn_ic > 0 else np.nan\nnn_p = 2 * (1 - stats.t.cdf(abs(nn_t), df=n_nn - 1)) if np.isfinite(nn_t) else np.nan\nmean_train_ic = np.mean(train_ic_list) if train_ic_list else np.nan\n\ncommon_dates = nn_ic_series.index.intersection(gbm_ic_series.index)\nnn_aligned = nn_ic_series.loc[common_dates].values\ngbm_aligned = gbm_ic_series.loc[common_dates].values\nmean_gbm_ic = gbm_aligned.mean()\ndiff_series = gbm_aligned - nn_aligned\nn_paired = len(diff_series)\nse_diff = diff_series.std() / np.sqrt(n_paired) if n_paired >= 2 else np.nan\npaired_t = diff_series.mean() / se_diff if se_diff > 0 else np.nan\npaired_p = 2 * (1 - stats.t.cdf(abs(paired_t), df=n_paired - 1)) if np.isfinite(paired_t) else np.nan\n\nprint(\"── GBM vs NN HEAD-TO-HEAD ────────────────────────────\")\nprint(f\"  GBM mean IC: {mean_gbm_ic:.4f},  NN mean IC: {mean_nn_ic:.4f}\")\nprint(f\"  |Difference|: {abs(mean_gbm_ic - mean_nn_ic):.4f}\")\nprint(f\"  Paired t-stat: {paired_t:.4f},  p-value: {paired_p:.4f}\")\nprint(f\"  NN ICIR: {nn_icir:.4f},  NN t-stat: {nn_t:.4f} (p={nn_p:.4f})\")\nprint(f\"  NN train/OOS ratio: {mean_train_ic / mean_nn_ic:.2f}\" if mean_nn_ic != 0 else \"\")\nprint()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Comparison Visualization\n\nAll three metrics -- mean IC, ICIR, percentage of positive months -- are nearly identical across models. The monthly IC lines intertwine rather than separate, confirming convergence."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "models = [\"GBM\", \"NN\"]\nmean_ics = [mean_gbm_ic, mean_nn_ic]\nicirs = [gbm_aligned.mean() / gbm_aligned.std() if gbm_aligned.std() > 0 else 0, nn_icir]\npct_pos = [(gbm_aligned > 0).mean(), (nn_ic_series > 0).mean()]\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\nfor ax, vals, ylabel, title in zip(axes, [mean_ics, icirs, pct_pos],\n    [\"Mean IC\", \"ICIR\", \"pct_positive\"],\n    [\"Mean OOS IC: GBM vs NN\", \"ICIR: GBM vs NN\", \"Pct Positive IC: GBM vs NN\"]):\n    bars = ax.bar(models, vals, color=[\"#1f77b4\", \"#ff7f0e\"], width=0.5)\n    ax.set_ylabel(ylabel)\n    ax.set_title(title)\n    ax.axhline(0.5 if ylabel == \"pct_positive\" else 0, color=\"gray\", linewidth=0.5, linestyle=\"--\")\n    for bar, val in zip(bars, vals):\n        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),\n                f\"{val:.4f}\" if ylabel == \"Mean IC\" else f\"{val:.3f}\", ha=\"center\", va=\"bottom\", fontsize=9)\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The honest takeaway is not \"trees always win\" -- it is \"trees win HERE, on THIS data, at THIS scale.\" The production story differs: GKX (2020) found NN marginally outperforms GBM on 94 features across 3,000+ CRSP stocks. Knowing when to reach for deep learning versus when to stick with gradient boosting is a career-defining judgment call."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "fig_line, ax_line = plt.subplots(figsize=(14, 5))\nx_range = range(len(common_dates))\nax_line.plot(x_range, gbm_aligned, color=\"#1f77b4\", alpha=0.8, linewidth=1, label=f\"GBM (mean={mean_gbm_ic:.4f})\")\nax_line.plot(x_range, nn_aligned, color=\"#ff7f0e\", alpha=0.8, linewidth=1, label=f\"NN (mean={mean_nn_ic:.4f})\")\nax_line.axhline(0, color=\"black\", linewidth=0.5)\nax_line.set_xlabel(\"OOS Month\")\nax_line.set_ylabel(\"Information Coefficient\")\nax_line.set_title(\"Monthly OOS IC: GBM vs Neural Network\")\nax_line.legend()\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Both models agree: the features matter more than the model. Time to examine what they are learning."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "all_nn_preds.to_parquet(CACHE_DIR / \"nn_predictions.parquet\")\nnn_ic_series.to_frame(\"ic\").to_parquet(CACHE_DIR / \"nn_ic_series.parquet\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 5: Feature Engineering and Importance\n\nGKX tested 94 characteristics. All their models agreed on the same top features. At a fund, the feature engineering team often has more headcount than the modeling team. We extend the feature matrix with interaction terms (e.g., momentum x volatility captures conditional momentum) and non-linear terms (squared momentum, absolute reversal), then use TreeSHAP and permutation importance to discover which features drive predictions."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import shap\nfrom sklearn.inspection import permutation_importance\n\nbaseline_ic = pd.read_parquet(CACHE_DIR / \"gbm_ic_series.parquet\")\nbaseline_ic = baseline_ic[\"ic\"] if \"ic\" in baseline_ic.columns else baseline_ic.iloc[:, 0]\nbaseline_ic.name = \"ic\"\nwith open(CACHE_DIR / \"gbm_best_params.json\") as f:\n    best_params = json.load(f)\n\npanel = load_monthly_panel()\ndates = panel.index.get_level_values(\"date\").unique().sort_values()\nfeatures = panel[FEATURE_COLS].copy()\ntarget = panel[\"fwd_return\"]\n\nfeatures[\"mom_x_vol\"] = features[\"momentum_z\"] * features[\"volatility_z\"]\nfeatures[\"ey_x_pb\"] = features[\"earnings_yield_z\"] * features[\"pb_ratio_z\"]\nfeatures[\"roe_x_ag\"] = features[\"roe_z\"] * features[\"asset_growth_z\"]\nfeatures[\"momentum_sq\"] = features[\"momentum_z\"] ** 2\nfeatures[\"abs_reversal\"] = features[\"reversal_z\"].abs()\n\nEXPANDED_COLS = list(features.columns)\nn_expanded = len(EXPANDED_COLS)\nprint(f\"  Features: {len(FEATURE_COLS)} original -> {n_expanded} expanded\")\nprint(f\"  Columns: {EXPANDED_COLS}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Walk-Forward with Expanded Features and SHAP Analysis\n\nWe reuse S3's best hyperparameters and collect models from the last 12 windows for pooled SHAP analysis. The IC change from expansion is approximately +0.004 -- tiny and not significant. This contrasts with GKX (2020), where expanding from 10 to 94 features improved R-squared by roughly 50%. On 174 stocks, additional features add more noise than signal."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def _wf_expanded_one_window(panel, features, target, best_params, train_dates, pred_date):\n    \"\"\"Fit GBM on one walk-forward window.\"\"\"\n    train_mask = panel.index.get_level_values(\"date\").isin(train_dates)\n    pred_mask = panel.index.get_level_values(\"date\") == pred_date\n    n_val_months = min(12, len(train_dates) // 5)\n    fit_dates, val_dates = train_dates[:-n_val_months], train_dates[-n_val_months:]\n    fit_mask = panel.index.get_level_values(\"date\").isin(fit_dates)\n    val_mask = panel.index.get_level_values(\"date\").isin(val_dates)\n\n    model = lgb.LGBMRegressor(\n        n_estimators=500, learning_rate=best_params[\"learning_rate\"],\n        num_leaves=best_params[\"num_leaves\"], min_child_samples=20,\n        reg_alpha=0.1, reg_lambda=1.0, subsample=0.8, subsample_freq=1,\n        colsample_bytree=0.8, random_state=42, verbosity=-1,\n    )\n    model.fit(features.loc[fit_mask].values, target.loc[fit_mask].values,\n              eval_set=[(features.loc[val_mask].values, target.loc[val_mask].values)],\n              callbacks=[lgb.early_stopping(50, verbose=False)])\n\n    X_pred, y_pred = features.loc[pred_mask].values, target.loc[pred_mask].values\n    preds = model.predict(X_pred)\n    pred_df = pd.DataFrame({\"date\": pred_date, \"ticker\": panel.loc[pred_mask].index.get_level_values(\"ticker\"),\n                            \"prediction\": preds, \"actual\": y_pred})\n    oos_ic = np.corrcoef(preds, y_pred)[0, 1]\n    train_ic = np.corrcoef(model.predict(features.loc[fit_mask].values), target.loc[fit_mask].values)[0, 1]\n    return pred_df, oos_ic, train_ic, model, X_pred, y_pred\n\n\noos_predictions, oos_ic_list, train_ic_list, recent_models = [], [], [], []\nsplits = list(walk_forward_splits(dates, TRAIN_WINDOW, PURGE_GAP))\nn_splits = len(splits)\nN_SHAP_MODELS = 12\n\nprint(f\"── WALK-FORWARD (expanded, {n_splits} windows) ─────────\")\nfor i, (train_dates, pred_date) in enumerate(splits):\n    if i % 10 == 0:\n        print(f\"  [{i+1}/{n_splits}] predicting {pd.Timestamp(pred_date).date()}\")\n    pred_df, oos_ic, train_ic, model, X_pred, y_pred = \\\n        _wf_expanded_one_window(panel, features, target, best_params, train_dates, pred_date)\n    oos_predictions.append(pred_df)\n    if np.isfinite(oos_ic): oos_ic_list.append({\"date\": pred_date, \"ic\": oos_ic})\n    if np.isfinite(train_ic): train_ic_list.append(train_ic)\n    if i >= n_splits - N_SHAP_MODELS: recent_models.append((model, X_pred, y_pred))\n\nlast_model, last_X_pred, last_y_pred = model, X_pred, y_pred\nexpanded_ic = pd.DataFrame(oos_ic_list).set_index(\"date\")[\"ic\"]\ncommon_dates = expanded_ic.index.intersection(baseline_ic.index)\nic_change = expanded_ic.loc[common_dates].mean() - baseline_ic.loc[common_dates].mean()\n\nprint(f\"  Mean IC (expanded): {expanded_ic.mean():.4f}\")\nprint(f\"  Mean IC (baseline): {baseline_ic.loc[common_dates].mean():.4f}\")\nprint(f\"  IC change: {ic_change:+.4f}\")\nprint()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### SHAP Feature Importance\n\nVolatility dominates the SHAP ranking (approximately 1.6x the next feature), with the momentum-volatility interaction and reversal filling out the top spots -- consistent with GKX (2020). The grouped analysis confirms original features are the primary drivers."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "pooled_shap, pooled_X = [], []\nfor mdl, X_oos, _ in recent_models:\n    ex = shap.TreeExplainer(mdl)\n    pooled_shap.append(ex.shap_values(X_oos))\n    pooled_X.append(X_oos)\npooled_shap_arr = np.concatenate(pooled_shap, axis=0)\npooled_X_arr = np.concatenate(pooled_X, axis=0)\n\nmean_abs_shap = np.abs(pooled_shap_arr).mean(axis=0)\nshap_ranking = pd.Series(mean_abs_shap, index=EXPANDED_COLS).sort_values(ascending=False)\n\nparent_map = {\"mom_x_vol\": [\"momentum_z\", \"volatility_z\"], \"ey_x_pb\": [\"earnings_yield_z\", \"pb_ratio_z\"],\n              \"roe_x_ag\": [\"roe_z\", \"asset_growth_z\"], \"momentum_sq\": [\"momentum_z\"], \"abs_reversal\": [\"reversal_z\"]}\ngrouped_shap = {feat: mean_abs_shap[EXPANDED_COLS.index(feat)] for feat in FEATURE_COLS}\nfor eng_feat, parents in parent_map.items():\n    contrib = mean_abs_shap[EXPANDED_COLS.index(eng_feat)] / len(parents)\n    for p in parents: grouped_shap[p] += contrib\ngrouped_ranking = pd.Series(grouped_shap).sort_values(ascending=False)\n\nprint(f\"── SHAP FEATURE IMPORTANCE (pooled, {len(pooled_X_arr)} obs) ──\")\nfor rank, (feat, val) in enumerate(shap_ranking.items(), 1):\n    print(f\"    {rank:2d}. {feat}: {val:.6f} ({'original' if feat in FEATURE_COLS else 'engineered'})\")\nprint(f\"\\n  Grouped by parent:\")\nfor rank, (feat, val) in enumerate(grouped_ranking.items(), 1):\n    print(f\"    {rank}. {feat}: {val:.6f}\")\nprint()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### SHAP Visualizations\n\nThe beeswarm plot reveals directional relationships: high volatility stocks tend to have lower predicted returns (low-volatility anomaly), while high momentum stocks have higher predicted returns (momentum premium). The bar chart shows original features (blue) dominating over engineered features (orange)."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "last_shap_values = shap.TreeExplainer(last_model).shap_values(last_X_pred)\n\nfig_shap, ax_shap = plt.subplots(figsize=(10, 7))\nshap.summary_plot(last_shap_values, features=pd.DataFrame(last_X_pred, columns=EXPANDED_COLS),\n                  show=False, plot_size=None)\nfig_shap = plt.gcf()\nax_shap = fig_shap.axes[0] if fig_shap.axes else ax_shap\nax_shap.set_title(\"SHAP Feature Importance (Last OOS Cross-Section)\")\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The features are the alpha. Engineering adds refinement, not revolution."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "fig_bar, ax_bar = plt.subplots(figsize=(10, 6))\ncolors_bar = [\"#1f77b4\" if f in FEATURE_COLS else \"#ff7f0e\" for f in shap_ranking.index]\nax_bar.barh(range(len(shap_ranking)), shap_ranking.values, color=colors_bar)\nax_bar.set_yticks(range(len(shap_ranking)))\nax_bar.set_yticklabels(shap_ranking.index)\nax_bar.invert_yaxis()\nax_bar.set_xlabel(\"Mean |SHAP value|\")\nax_bar.set_title(\"Feature Importance: Original (blue) vs Engineered (orange)\")\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "As a robustness check, permutation importance shows moderate agreement with SHAP (Spearman rank correlation approximately 0.54 on grouped features). Both methods agree on the top features but differ on mid-ranked ones -- expected since SHAP measures marginal contribution while permutation measures degradation from removal."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "pooled_y = np.concatenate([y for _, _, y in recent_models])\nperm_result = permutation_importance(last_model, pooled_X_arr, pooled_y,\n                                      n_repeats=30, random_state=42, scoring=\"neg_mean_squared_error\")\nperm_imp = pd.Series(perm_result.importances_mean, index=EXPANDED_COLS).sort_values(ascending=False)\n\ngrouped_perm = {feat: perm_imp[feat] for feat in FEATURE_COLS}\nfor eng_feat, parents in parent_map.items():\n    for p in parents: grouped_perm[p] += perm_imp[eng_feat] / len(parents)\ngrouped_perm_ranking = pd.Series(grouped_perm).sort_values(ascending=False)\nrank_corr = stats.spearmanr(grouped_ranking.loc[FEATURE_COLS], grouped_perm_ranking.loc[FEATURE_COLS])[0]\n\nprint(f\"  Rank corr (grouped SHAP vs grouped Perm): {rank_corr:.4f}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 6: From Signal to Portfolio\n\nYour model says AAPL will be in the top decile next month. So you buy AAPL. But what do you sell? And how much? The gap between \"good signal\" and \"profitable strategy\" is where most quant careers are actually spent. We sort stocks into deciles by GBM prediction each month, go long the top and short the bottom, and track returns -- the standard academic approach to evaluating cross-sectional signals."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from shared.backtesting import (quantile_portfolios, long_short_returns, portfolio_turnover,\n                                 sharpe_ratio, net_returns, max_drawdown, cumulative_returns)\n\ngbm_preds = pd.read_parquet(CACHE_DIR / \"gbm_predictions.parquet\")\npredictions, actuals = gbm_preds[\"prediction\"], gbm_preds[\"actual\"]\nn_oos = predictions.index.get_level_values(\"date\").nunique()\n\ndecile_returns = quantile_portfolios(predictions, actuals, n_groups=10)\ndecile_means = decile_returns.mean()\nprint(f\"Decile mean monthly returns:\")\nfor d in decile_means.index:\n    print(f\"  D{d:2d}: {decile_means[d]:+.5f}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Monotonicity and Long-Short Performance\n\nThe monotonicity rank correlation of approximately 0.83 (p = 0.003) confirms a strong pattern, but notice *all* deciles have positive mean returns -- including the bottom decile. This is a survivorship bias artifact: our universe excludes firms that went bankrupt or were delisted. The gross Sharpe of approximately 0.77 is respectable (roughly half the GKX benchmark of 1.5), but monthly turnover of 79% (9.5x annual) is extremely high by institutional standards (typical: 50-100% annual)."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "top_decile = decile_means.iloc[-1]\nbottom_decile = decile_means.iloc[0]\nspread = top_decile - bottom_decile\nrank_corr = stats.spearmanr(decile_means.index, decile_means.values)\nprint(f\"\\nSpread (D10 - D1): {spread:+.5f}\")\nprint(f\"Monotonicity rank corr: {rank_corr.statistic:.3f} (p={rank_corr.pvalue:.3f})\")\n\ndecile_labels = [int(x) for x in decile_means.index]\ndecile_vals = [float(v) * 100 for v in decile_means.values]\nfig_decile, ax_decile = plt.subplots(figsize=(8, 5))\nax_decile.bar(decile_labels, decile_vals,\n              color=[\"#d62728\" if v < 0 else \"#2ca02c\" for v in decile_vals],\n              edgecolor=\"black\", linewidth=0.5)\nax_decile.axhline(0, color=\"black\", linewidth=0.8)\nax_decile.set(title=\"Mean Monthly Return by Prediction Decile (GBM)\",\n              xlabel=\"Decile (1=lowest predicted, 10=highest predicted)\",\n              ylabel=\"Mean Monthly Return (%)\")\nax_decile.set_xticks(decile_labels)\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The staircase pattern from bottom to top decile is what a positive-IC signal should produce. Now let us measure the long-short returns, turnover, costs, and risk."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "ls_returns = long_short_returns(predictions, actuals, n_groups=10)\nls_sharpe_gross = sharpe_ratio(ls_returns)\nturnover = portfolio_turnover(predictions, n_groups=10)\nmean_turnover = turnover.mean()\nls_net = net_returns(ls_returns, turnover, cost_bps=COST_BPS)\nls_sharpe_net = sharpe_ratio(ls_net)\nmdd_gross = max_drawdown(ls_returns)\nskewness = float(stats.skew(ls_returns.dropna().values))\nkurtosis_excess = float(stats.kurtosis(ls_returns.dropna().values))\n\nprint(f\"\\n── PORTFOLIO METRICS ─────────────────────────────────\")\nprint(f\"  Gross Sharpe: {ls_sharpe_gross:.3f},  Net Sharpe ({COST_BPS} bps): {ls_sharpe_net:.3f}\")\nprint(f\"  Monthly turnover: {mean_turnover:.4f},  Annual: {mean_turnover * 12:.2f}\")\nprint(f\"  Max drawdown: {mdd_gross:.4f}\")\nprint(f\"  Skewness: {skewness:+.3f},  Excess kurtosis: {kurtosis_excess:+.3f}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The excess kurtosis of approximately +3.67 means extreme monthly returns are more likely than a Gaussian predicts -- the Sharpe ratio of 0.77 *understates* the true tail risk. A maximum drawdown of approximately -23.5% is substantial: the strategy can lose nearly a quarter of its value before recovering."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "cum_gross = cumulative_returns(ls_returns)\ncum_net = cumulative_returns(ls_net)\n\nfig_cum, ax_cum = plt.subplots(figsize=(10, 5))\nax_cum.plot(cum_gross.index, cum_gross.values, label=\"Gross\", linewidth=1.5, color=\"#1f77b4\")\nax_cum.plot(cum_net.index, cum_net.values, label=f\"Net ({COST_BPS} bps)\",\n            linewidth=1.5, color=\"#ff7f0e\", linestyle=\"--\")\nax_cum.axhline(1.0, color=\"grey\", linewidth=0.8, linestyle=\":\")\nax_cum.set(title=\"Cumulative Long-Short Returns (GBM Signal, Decile Sort)\",\n           xlabel=\"Date\", ylabel=\"Cumulative Return ($1 invested)\")\nax_cum.legend(loc=\"upper left\")\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The gap between gross and net widens over time -- a visual representation of how transaction costs compound. A signal and a strategy are not the same thing. The complete arc is now visible: features to model to signal to portfolio to returns. This is the skeleton of every systematic equity alpha pipeline."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 7: Alternative Data as Alpha Features\n\nThe alpha pipeline we built this week uses traditional features -- data every market participant can access. The industry's fastest-growing edge comes from *alternative data*: non-traditional information sources that may provide an edge before the signal is priced in. The market has grown from approximately $200M in 2015 to over $7B by 2025.\n\nThe taxonomy spans seven major categories. **Sentiment and News** (news articles, social media, earnings calls, SEC filing complexity) with intraday-to-weeks alpha horizons. **Web and App Traffic** (SimilarWeb, Google Trends, app downloads) as leading revenue indicators. **Geolocation and Foot Traffic** (Placer.ai, SafeGraph) for nowcasting same-store sales. **Satellite and Imagery** (oil tank fills, crop health via NDVI) for commodity and macro signals. **Transaction and Payment Data** (credit card panels, e-commerce receipts) for revenue nowcasting. **Government and Regulatory Filings** (FDA pipelines, patent networks, Form 4 insider transactions). **Expert Networks and Surveys** (GLG, Tegus, ISM PMI sub-components).\n\nAlternative data is expensive: the median institutional spend is approximately $1.6M/year (BattleFin 2023, Exabel 2024), with top-decile funds spending over $5M/year. This creates structural barriers -- a $100M fund needs 160 bps of gross alpha just to break even on data costs. Quality challenges include survivorship bias in coverage panels, point-in-time integrity issues from retroactive revisions, sample representativeness concerns, evolving privacy regulations, and signal decay as adoption increases (typical half-life: 2-4 years).\n\nThe key implication: by the time an alternative dataset is widely available, its alpha content has often been arbitraged away. But the *evaluation framework* from this week -- IC, ICIR, paired tests, net-of-cost Sharpe -- applies identically. In Week 7, we add the first alternative data category to this pipeline: NLP-derived sentiment features from earnings call transcripts. The metrics are universal; only the data sources change."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\n| Metric | Value | Context |\n|--------|-------|---------|\n| Universe | ~174 S&P 500 stocks/month | Survivorship-biased; production uses CRSP 3,000+ |\n| Features | 7 z-scored (4 fundamental, 3 technical) | Production: 94+ (GKX 2020) |\n| Sample | 130 months, 68 OOS months | 60-month train window + 1-month purge |\n| Fama-MacBeth IC | 0.047 (t=2.54, p=0.013) | Linear baseline |\n| GBM OOS IC | 0.046 (t=2.15, p=0.035) | Statistically significant |\n| NN OOS IC | 0.046 (t=1.74, p=0.087) | Indistinguishable from GBM (paired p=0.99) |\n| GBM vs naive | p = 0.57 | Does not significantly beat momentum alone |\n| Train/OOS ratio | 6.73 (GBM), 2.2 (NN) | GBM overfits more |\n| Feature expansion | IC +0.004 (7 to 12 features) | Not significant |\n| SHAP top features | volatility_z, mom_x_vol, reversal_z | Technicals dominate |\n| Gross Sharpe | 0.77 | Production benchmark: ~1.5 (GKX 2020) |\n| Net Sharpe (10 bps) | 0.69 | 79% monthly turnover |\n| Max drawdown | -23.5% | Excess kurtosis = 3.67 |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Career Connections\n\n**Quantitative Researcher.** The full pipeline -- feature engineering, walk-forward GBM, IC evaluation, SHAP, decile portfolios -- is the daily workflow. Model complexity does not help on small feature sets; spend time on new features, not architectures.\n\n**ML Engineer (Quant Fund).** The infrastructure challenge is production-grade walk-forward at scale: 3,000+ stocks, 94+ features, daily frequency, distributed compute. Our single-threaded loop takes minutes; production runs nightly on clusters.\n\n**Portfolio Manager.** The signal-to-portfolio gap -- IC of 0.046 becoming Sharpe 0.77 gross but 0.69 net, with 79% turnover and -23.5% drawdown -- is the core evaluation framework for capital allocation.\n\n**Risk Manager.** Excess kurtosis of 3.67 means Sharpe understates tail risk. Survivorship inflates all deciles. Train/OOS ratio of 6.73 signals overfitting. Every number requires risk-adjusted interpretation."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Bridge to Week 5\n\nThis week built the alpha signal. Next week tears it apart. Week 5 formalizes: *how do you know you have not fooled yourself?* Multiple hypothesis testing, non-stationary data, and implementation sensitivity (Sharpe ranged 0.77-0.98 depending on details) all require rigorous treatment. Week 5 introduces deflated Sharpe ratio, combinatorial cross-validation, and the master-backtest framework."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## References\n\n- **Gu, Kelly & Xiu (2020).** \"Empirical Asset Pricing via Machine Learning.\" *Review of Financial Studies*. The canonical reference: 94 characteristics, CRSP universe, all models agree on which features matter.\n\n- **Grinold & Kahn (2000).** *Active Portfolio Management*. McGraw-Hill. The fundamental law of active management: IR = IC x sqrt(BR).\n\n- **Kelly, Malamud & Zhou (2023).** \"The Virtue of Complexity in Return Prediction.\" *Journal of Finance*. Complexity helps at scale; our Section 4 confirms the contrapositive.\n\n- **Lopez de Prado (2018).** *Advances in Financial Machine Learning*. Wiley. Feature importance, backtesting, substitution effects.\n\n- **Frazzini, Israel & Moskowitz (2018).** \"Trading Costs.\" Working paper. Institutional S&P 500 execution at 5-15 bps one-way."
  }
 ]
}