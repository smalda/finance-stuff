{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework \u2014 Week 3: Factor Models & Return Decomposition\n",
    "\n",
    "> *\"By one count, over 400 factors have been proposed to explain stock returns \u2014 more factors than months of out-of-sample data to test them.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Mission\n\nThis week you build the infrastructure that every cross-sectional strategy rests on.\n\nIn the lecture, we showed you *why* factor models matter \u2014 from CAPM's elegant failures to the factor zoo's overfitting epidemic. In the seminar, you tested individual pieces: replicating a single factor, running one Fama-MacBeth regression, decomposing one portfolio. Now you scale up.\n\nYou will build three things:\n\n1. **A Factor Factory** \u2014 a complete pipeline that constructs all six Fama-French factors from raw data and validates them against Ken French's official benchmarks. This is the factory floor of quantitative finance: the data infrastructure that AQR, Dimensional, and every systematic fund builds before anything else.\n\n2. **A Cross-Sectional Feature Matrix** \u2014 a reusable system that transforms messy fundamental and price data into a clean, standardized panel ready for ML consumption. This is the bridge between factor research (this week) and ML-based alpha models (Week 4). The feature matrix you build here is the *direct input* to next week's gradient boosting and neural network models.\n\n3. **A Factor Model Horse Race** \u2014 a head-to-head comparison of CAPM, FF3, and FF5 using Fama-MacBeth regressions on your full stock universe. You will quantify exactly how much cross-sectional variation each model explains, which factors are genuinely priced, and \u2014 most importantly \u2014 how much *residual alpha* remains unexplained. That residual is the opportunity space for ML.\n\nBy the end, you will understand something that takes many junior quants months on the job to grasp: factor models explain only about 8\u201315% of cross-sectional return variation. The remaining 85%+ is noise *and* signal, mixed together. Week 4's ML models will try to separate them \u2014 but only if the feature matrix you build here is clean.\n\n## Deliverables\n\n1. **The Factor Factory** \u2014 `FactorBuilder` class producing 6 monthly factor return series (Mkt-RF, SMB, HML, RMW, CMA, MOM), validated against official Ken French data.\n2. **The Cross-Sectional Feature Matrix** \u2014 `FeatureEngineer` class producing a standardized panel of 7 features (4 fundamental + 3 technical), saved to Parquet for Week 4.\n3. **The Factor Model Horse Race** \u2014 Fama-MacBeth regressions for CAPM, FF3, and FF5 with R-squared progression, significance tests, and residual alpha analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install yfinance getfactormodels linearmodels statsmodels matplotlib numpy pandas tqdm -q\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import yfinance as yf\n",
    "from linearmodels import FamaMacBeth as LMFamaMacBeth\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"figure.figsize\": (12, 6),\n",
    "    \"axes.grid\": True,\n",
    "    \"grid.alpha\": 0.3,\n",
    "    \"font.size\": 11,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Download\n",
    "\n",
    "Everything in this homework starts from the same raw materials: 200 S&P 500 stocks spanning every sector, daily prices from 2014\u20132024, fundamental data from yfinance, and official factor returns from Ken French's data library. We download everything once here and work from memory throughout.\n",
    "\n",
    "A word of warning: the fundamental download loops through 200 tickers sequentially, hitting yfinance's API for balance sheets, income statements, and company info. This takes about 2\u20133 minutes. If you are on a slow connection or yfinance throttles you, some tickers may fail \u2014 the code handles this gracefully and moves on. We only need about 150+ tickers to have enough cross-sectional breadth for all three deliverables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Ticker universe: 200 S&P 500 stocks across all sectors \u2500\u2500\n",
    "TICKERS = [\n",
    "    # Technology\n",
    "    \"AAPL\", \"MSFT\", \"NVDA\", \"AVGO\", \"AMD\", \"CSCO\", \"ADBE\", \"CRM\", \"INTC\", \"TXN\",\n",
    "    \"QCOM\", \"AMAT\", \"LRCX\", \"MU\", \"NOW\", \"INTU\", \"SNPS\", \"CDNS\", \"KLAC\", \"MCHP\",\n",
    "    # Communication Services\n",
    "    \"META\", \"GOOG\", \"NFLX\", \"DIS\", \"CMCSA\", \"T\", \"VZ\", \"TMUS\", \"EA\", \"TTWO\",\n",
    "    # Consumer Discretionary\n",
    "    \"AMZN\", \"TSLA\", \"HD\", \"MCD\", \"NKE\", \"LOW\", \"SBUX\", \"TJX\", \"BKNG\", \"ORLY\",\n",
    "    \"MAR\", \"YUM\", \"DHI\", \"CMG\", \"ROST\", \"LEN\", \"GPC\", \"BBY\", \"POOL\", \"GRMN\",\n",
    "    # Consumer Staples\n",
    "    \"PG\", \"KO\", \"PEP\", \"COST\", \"WMT\", \"PM\", \"MO\", \"CL\", \"KMB\", \"GIS\",\n",
    "    \"SJM\", \"HSY\", \"KHC\", \"MNST\", \"STZ\", \"KR\", \"TSN\", \"SYY\", \"CHD\", \"MKC\",\n",
    "    # Energy\n",
    "    \"XOM\", \"CVX\", \"COP\", \"SLB\", \"EOG\", \"MPC\", \"PSX\", \"VLO\", \"OXY\", \"HES\",\n",
    "    \"DVN\", \"FANG\", \"HAL\", \"BKR\", \"CTRA\", \"APA\", \"TRGP\", \"KMI\", \"WMB\", \"OKE\",\n",
    "    # Financials\n",
    "    \"JPM\", \"BAC\", \"WFC\", \"GS\", \"MS\", \"BLK\", \"SCHW\", \"AXP\", \"USB\", \"PNC\",\n",
    "    \"TFC\", \"AIG\", \"MET\", \"PRU\", \"ALL\", \"CB\", \"MMC\", \"AON\", \"ICE\", \"CME\",\n",
    "    # Healthcare\n",
    "    \"LLY\", \"UNH\", \"JNJ\", \"PFE\", \"ABT\", \"MRK\", \"TMO\", \"AMGN\", \"MDT\", \"ISRG\",\n",
    "    \"BMY\", \"GILD\", \"VRTX\", \"REGN\", \"ZTS\", \"SYK\", \"BSX\", \"BDX\", \"EW\", \"HCA\",\n",
    "    # Industrials\n",
    "    \"CAT\", \"GE\", \"HON\", \"UNP\", \"RTX\", \"BA\", \"DE\", \"LMT\", \"MMM\", \"GD\",\n",
    "    \"WM\", \"ITW\", \"EMR\", \"ETN\", \"FDX\", \"CSX\", \"NSC\", \"PCAR\", \"ROK\", \"FAST\",\n",
    "    # Materials\n",
    "    \"LIN\", \"APD\", \"SHW\", \"ECL\", \"NEM\", \"FCX\", \"NUE\", \"VMC\", \"MLM\", \"DOW\",\n",
    "    # Real Estate\n",
    "    \"PLD\", \"AMT\", \"CCI\", \"EQIX\", \"PSA\", \"SPG\", \"DLR\", \"O\", \"WELL\", \"ARE\",\n",
    "    # Utilities\n",
    "    \"NEE\", \"DUK\", \"SO\", \"D\", \"AEP\", \"SRE\", \"EXC\", \"XEL\", \"WEC\", \"ES\",\n",
    "]\n",
    "\n",
    "START = \"2014-01-01\"\n",
    "END = \"2024-12-31\"\n",
    "\n",
    "# --- Equity prices ---\n",
    "print(\"Downloading equity prices (200 tickers, 2014-2024)...\")\n",
    "raw = yf.download(TICKERS, start=START, end=END, auto_adjust=True, progress=True)\n",
    "prices = raw[\"Close\"]\n",
    "completeness = prices.notna().mean()\n",
    "valid_tickers = completeness[completeness > 0.50].index.tolist()\n",
    "prices = prices[valid_tickers]\n",
    "prices.index = pd.to_datetime(prices.index).tz_localize(None)\n",
    "print(f\"Tickers downloaded: {len(valid_tickers)}/{len(TICKERS)}\")\n",
    "\n",
    "# Monthly returns\n",
    "monthly_prices = prices.resample(\"ME\").last()\n",
    "monthly_returns = monthly_prices.pct_change().dropna(how=\"all\")\n",
    "print(f\"Monthly returns: {monthly_returns.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the fundamental data. This is the slow part \u2014 we loop through each ticker to grab balance sheets, income statements, sector, and market cap from yfinance. The retry logic handles transient API failures, and we track which tickers fall back to Net Income when Operating Income is unavailable (about 4% of tickers, mostly financials like JPM and GS that use a different income statement structure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fundamentals ---\n",
    "bs_records, inc_records, shares_records = [], [], []\n",
    "sector_map, mcap_map = {}, {}\n",
    "failures = []\n",
    "\n",
    "print(f\"Downloading fundamentals for {len(valid_tickers)} tickers...\")\n",
    "for ticker_str in tqdm(valid_tickers, desc=\"Fundamentals\"):\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            tk = yf.Ticker(ticker_str)\n",
    "            bs = tk.balance_sheet\n",
    "            if bs is not None and not bs.empty:\n",
    "                for col_date in bs.columns:\n",
    "                    row = {}\n",
    "                    for field in [\"Stockholders Equity\", \"Total Assets\",\n",
    "                                  \"Ordinary Shares Number\"]:\n",
    "                        if field in bs.index:\n",
    "                            row[field] = bs.loc[field, col_date]\n",
    "                    if row:\n",
    "                        row[\"ticker\"] = ticker_str\n",
    "                        row[\"date\"] = pd.Timestamp(col_date).tz_localize(None)\n",
    "                        bs_records.append(row)\n",
    "                    if \"Ordinary Shares Number\" in bs.index:\n",
    "                        shares_records.append({\n",
    "                            \"ticker\": ticker_str,\n",
    "                            \"date\": pd.Timestamp(col_date).tz_localize(None),\n",
    "                            \"shares\": bs.loc[\"Ordinary Shares Number\", col_date],\n",
    "                        })\n",
    "            inc = tk.income_stmt\n",
    "            if inc is not None and not inc.empty:\n",
    "                for col_date in inc.columns:\n",
    "                    row = {\"ticker\": ticker_str,\n",
    "                           \"date\": pd.Timestamp(col_date).tz_localize(None)}\n",
    "                    for field in [\"Operating Income\", \"Net Income\",\n",
    "                                  \"Total Revenue\", \"Pretax Income\"]:\n",
    "                        if field in inc.index:\n",
    "                            row[field] = inc.loc[field, col_date]\n",
    "                    inc_records.append(row)\n",
    "            info = tk.info\n",
    "            sector_map[ticker_str] = info.get(\"sector\", \"Other\")\n",
    "            mcap_map[ticker_str] = info.get(\"marketCap\", np.nan)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if attempt < 2:\n",
    "                time.sleep(2 ** (attempt + 1))\n",
    "            else:\n",
    "                failures.append(ticker_str)\n",
    "                print(f\"  FAILED {ticker_str}: {e}\")\n",
    "\n",
    "bs_df = pd.DataFrame(bs_records)\n",
    "if not bs_df.empty:\n",
    "    bs_df = bs_df.set_index([\"ticker\", \"date\"]).sort_index()\n",
    "inc_df = pd.DataFrame(inc_records)\n",
    "if not inc_df.empty:\n",
    "    inc_df = inc_df.set_index([\"ticker\", \"date\"]).sort_index()\n",
    "shares_df = pd.DataFrame(shares_records)\n",
    "if not shares_df.empty:\n",
    "    shares_df = shares_df.set_index([\"ticker\", \"date\"]).sort_index()\n",
    "sector_s = pd.Series(sector_map, name=\"sector\")\n",
    "mcap_s = pd.Series(mcap_map, name=\"market_cap\")\n",
    "\n",
    "fundamentals = {\n",
    "    \"balance_sheet\": bs_df,\n",
    "    \"income_stmt\": inc_df,\n",
    "    \"sector\": sector_s,\n",
    "    \"market_cap\": mcap_s,\n",
    "    \"shares\": shares_df,\n",
    "}\n",
    "\n",
    "print(f\"Balance sheet records: {len(bs_df)}\")\n",
    "print(f\"Income statement records: {len(inc_df)}\")\n",
    "print(f\"Sectors: {sector_s.nunique()} unique\")\n",
    "print(f\"Failures: {len(failures)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, official factor returns from Ken French's data library. These are the benchmark we will validate our self-built factors against. The `getfactormodels` library wraps Ken French's public data and returns it in a clean format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getfactormodels import FamaFrenchFactors, CarhartFactors\n",
    "\n",
    "# FF5 factors\n",
    "ff5_raw = FamaFrenchFactors(model=\"5\", frequency=\"M\")\n",
    "ff5 = ff5_raw.to_pandas()\n",
    "if \"date\" in ff5.columns:\n",
    "    ff5[\"date\"] = pd.to_datetime(ff5[\"date\"])\n",
    "    ff5 = ff5.set_index(\"date\")\n",
    "ff5.index = pd.to_datetime(ff5.index)\n",
    "ff5.index.name = \"date\"\n",
    "\n",
    "# FF3 (subset of FF5)\n",
    "ff3 = ff5[[\"Mkt-RF\", \"SMB\", \"HML\", \"RF\"]].copy()\n",
    "\n",
    "# Carhart (includes MOM)\n",
    "cf_raw = CarhartFactors(frequency=\"M\")\n",
    "carhart = cf_raw.to_pandas()\n",
    "if \"date\" in carhart.columns:\n",
    "    carhart[\"date\"] = pd.to_datetime(carhart[\"date\"])\n",
    "    carhart = carhart.set_index(\"date\")\n",
    "carhart.index = pd.to_datetime(carhart.index)\n",
    "carhart.index.name = \"date\"\n",
    "\n",
    "print(f\"FF5 factors: {list(ff5.columns)}, {len(ff5)} months\")\n",
    "print(f\"Carhart factors: {list(carhart.columns)}, {len(carhart)} months\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n# Deliverable 1: The Factor Factory\n\nYour first mission: build a complete factor construction pipeline from scratch.\n\nEvery systematic fund on the planet starts here. Before you can run a backtest, train a model, or pitch a strategy, you need factor returns \u2014 the elemental building blocks of cross-sectional finance. At AQR, Dimensional, and Bridgewater, the team that maintains the factor construction pipeline is foundational infrastructure, like the data engineering team at a tech company.\n\nYou will build a `FactorBuilder` class that takes raw price and fundamental data and produces monthly return series for all six canonical factors: the market (Mkt-RF), size (SMB), value (HML), profitability (RMW), investment (CMA), and momentum (MOM). The class then validates its output against Ken French's official data and reports correlation and tracking error for each factor.\n\nHere is the crucial twist that makes this more than a coding exercise: our universe is 200 S&P 500 stocks \u2014 large-caps only. Ken French's factors are built from the entire NYSE/AMEX/NASDAQ universe of 4,000\u20136,000 stocks, including genuine small-caps with market caps of \\$100M\u2013\\$2B. Our \"small\" stocks have market caps above \\$10B. This structural mismatch means some factors will replicate well and others will not, and the *hierarchy* of replication quality tells you something important about which characteristics have meaningful dispersion within large-caps and which require the full market universe.\n\n### Student Workspace\n\nBuild a `FactorBuilder` class with these capabilities:\n- Download raw data and compute fundamental characteristics (book-to-market, profitability, investment)\n- Perform 2\u00d73 double-sort portfolio formation (size median \u00d7 characteristic terciles)\n- Compute value-weighted monthly returns for each portfolio\n- Construct long-short factor returns (SMB, HML, RMW, CMA, MOM, Mkt-RF)\n- Validate against Ken French official data (correlation, tracking error)\n- Produce a quality report (missing data, fallback tickers, portfolio composition)\n\nTease: the validation results will reveal a clear hierarchy \u2014 some factors replicate well with our data, others do not. The reason *why* each factor fails or succeeds is itself the insight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the FactorBuilder class\n",
    "# - __init__: accept monthly_returns, prices, fundamentals\n",
    "# - compute_characteristics: extract B/M, profitability, investment from fundamentals\n",
    "# - _double_sort: perform 2 x 3 sort on size and a characteristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add the portfolio return computation and factor construction methods\n",
    "# - _compute_vw_returns: value-weighted monthly returns per portfolio\n",
    "# - _build_long_short: SMB and characteristic factor from portfolio returns\n",
    "# - build_all_factors: orchestrate the full 6-factor pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "Once the class skeleton is in place, the factory needs to actually *produce* something. The value-weighted return computation is where most subtle bugs hide \u2014 getting the weight normalization wrong silently breaks every downstream factor return. Think carefully about how to handle months where some tickers in a portfolio have missing returns.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add the validation method\n",
    "# - validate: compare each self-built factor against official Ken French data\n",
    "# - report correlation, tracking error, number of overlapping months"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "With the pipeline built, the final step is execution and validation. Running the builder end-to-end should produce 6 factor return series spanning ~131 months. Validation against Ken French's official data is not optional \u2014 it is how you know whether your construction methodology is correct or subtly broken.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run the pipeline end-to-end\n",
    "# builder = FactorBuilder(monthly_returns, prices, fundamentals)\n",
    "# factor_returns = builder.build_all_factors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Validate and visualize\n",
    "# validation = builder.validate(ff5, carhart)\n",
    "# Plot cumulative factor returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## \u2501\u2501\u2501 SOLUTION: Deliverable 1 \u2501\u2501\u2501\n\nThe `FactorBuilder` class below encapsulates the full pipeline. A few design choices worth noting: the class computes characteristics *once* from the most recent fundamentals and applies those fixed assignments over the entire price window. This is a **static sort** \u2014 a deliberate simplification because yfinance provides only ~4\u20135 annual fundamental periods per ticker (~2021\u20132025), making proper annual rebalancing over 2014\u20132024 impossible. The proper Fama-French methodology rebalances portfolios each June using the prior fiscal year's fundamentals. Our static approach introduces look-ahead bias \u2014 a stock classified as \"value\" using its 2025 book equity may have been \"growth\" in 2014. We will see how this affects validation results shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorBuilder:\n",
    "    \"\"\"Construct Fama-French 5 factors + momentum from raw data.\n",
    "\n",
    "    Attributes:\n",
    "        monthly_returns: DataFrame of monthly returns (date x ticker).\n",
    "        fundamentals: Dict of fundamental DataFrames from data_setup.\n",
    "        factor_returns: DataFrame of monthly self-built factor returns.\n",
    "        quality_report: Dict summarizing data quality issues.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, monthly_returns, prices, fundamentals):\n",
    "        self.monthly_returns = monthly_returns\n",
    "        self.prices = prices\n",
    "        self.fundamentals = fundamentals\n",
    "        self.factor_returns = None\n",
    "        self.quality_report = {\n",
    "            \"missing_equity\": [],\n",
    "            \"missing_income\": [],\n",
    "            \"net_income_fallback\": [],\n",
    "            \"portfolio_counts\": {},\n",
    "        }\n",
    "\n",
    "    def compute_characteristics(self):\n",
    "        \"\"\"Compute book-to-market, profitability, and investment.\"\"\"\n",
    "        bs = self.fundamentals[\"balance_sheet\"]\n",
    "        inc = self.fundamentals[\"income_stmt\"]\n",
    "        mcap = self.fundamentals[\"market_cap\"]\n",
    "\n",
    "        records = []\n",
    "        for ticker in self.monthly_returns.columns:\n",
    "            row = {\"ticker\": ticker}\n",
    "\n",
    "            # Book equity\n",
    "            equity = np.nan\n",
    "            if ticker in bs.index.get_level_values(\"ticker\"):\n",
    "                eq_vals = bs.loc[ticker].sort_index()[\"Stockholders Equity\"].dropna()\n",
    "                if len(eq_vals) > 0 and eq_vals.iloc[-1] > 0:\n",
    "                    equity = eq_vals.iloc[-1]\n",
    "                else:\n",
    "                    self.quality_report[\"missing_equity\"].append(ticker)\n",
    "            else:\n",
    "                self.quality_report[\"missing_equity\"].append(ticker)\n",
    "\n",
    "            # Market cap and B/M\n",
    "            mk = mcap.get(ticker, np.nan)\n",
    "            if not np.isnan(equity) and not np.isnan(mk) and mk > 0:\n",
    "                row[\"book_to_market\"] = equity / mk\n",
    "                row[\"market_cap\"] = mk\n",
    "\n",
    "            # Operating profitability (OP / equity)\n",
    "            if ticker in inc.index.get_level_values(\"ticker\"):\n",
    "                tk_inc = inc.loc[ticker].sort_index()\n",
    "                oi = tk_inc.get(\"Operating Income\", pd.Series(dtype=float)).dropna()\n",
    "                if len(oi) == 0:\n",
    "                    ni = tk_inc.get(\"Net Income\", pd.Series(dtype=float)).dropna()\n",
    "                    if len(ni) > 0:\n",
    "                        oi = ni\n",
    "                        self.quality_report[\"net_income_fallback\"].append(ticker)\n",
    "                if len(oi) > 0 and not np.isnan(equity) and equity > 0:\n",
    "                    row[\"profitability\"] = oi.iloc[-1] / equity\n",
    "            else:\n",
    "                self.quality_report[\"missing_income\"].append(ticker)\n",
    "\n",
    "            # Investment (asset growth)\n",
    "            if ticker in bs.index.get_level_values(\"ticker\"):\n",
    "                assets = bs.loc[ticker].sort_index()[\"Total Assets\"].dropna()\n",
    "                if len(assets) > 1 and assets.iloc[-2] > 0:\n",
    "                    row[\"investment\"] = (assets.iloc[-1] / assets.iloc[-2]) - 1\n",
    "\n",
    "            records.append(row)\n",
    "\n",
    "        self.chars = pd.DataFrame(records).set_index(\"ticker\")\n",
    "        return self.chars\n",
    "\n",
    "    def _double_sort(self, char_col, n_groups=3):\n",
    "        \"\"\"Perform 2 x n_groups sort on size and a characteristic.\n",
    "\n",
    "        Returns dict mapping portfolio name to list of tickers.\n",
    "        \"\"\"\n",
    "        valid = self.chars.dropna(subset=[\"market_cap\", char_col]).copy()\n",
    "\n",
    "        # Size split: median\n",
    "        size_median = valid[\"market_cap\"].median()\n",
    "        valid[\"size_group\"] = np.where(\n",
    "            valid[\"market_cap\"] < size_median, \"S\", \"B\"\n",
    "        )\n",
    "\n",
    "        # Characteristic split\n",
    "        breakpoints = [valid[char_col].quantile(q)\n",
    "                       for q in np.linspace(0, 1, n_groups + 1)]\n",
    "\n",
    "        def assign_char_group(val):\n",
    "            for i in range(n_groups):\n",
    "                if val <= breakpoints[i + 1]:\n",
    "                    return i\n",
    "            return n_groups - 1\n",
    "\n",
    "        valid[\"char_group\"] = valid[char_col].apply(assign_char_group)\n",
    "\n",
    "        portfolios = {}\n",
    "        for size in [\"S\", \"B\"]:\n",
    "            for cg in range(n_groups):\n",
    "                mask = (valid[\"size_group\"] == size) & (valid[\"char_group\"] == cg)\n",
    "                pf_tickers = valid[mask].index.tolist()\n",
    "                pf_name = f\"{size}/{cg}\"\n",
    "                portfolios[pf_name] = pf_tickers\n",
    "\n",
    "        return portfolios, valid\n",
    "\n",
    "    def _compute_vw_returns(self, portfolios, valid_chars):\n",
    "        \"\"\"Compute value-weighted monthly returns for each portfolio.\"\"\"\n",
    "        pf_returns = {}\n",
    "\n",
    "        for date in self.monthly_returns.index:\n",
    "            rets = self.monthly_returns.loc[date].dropna()\n",
    "            date_rets = {}\n",
    "\n",
    "            for pf_name, tickers in portfolios.items():\n",
    "                avail = [t for t in tickers if t in rets.index]\n",
    "                if len(avail) == 0:\n",
    "                    date_rets[pf_name] = np.nan\n",
    "                    continue\n",
    "                weights = valid_chars.loc[avail, \"market_cap\"]\n",
    "                weights = weights / weights.sum()\n",
    "                date_rets[pf_name] = (rets[avail] * weights).sum()\n",
    "\n",
    "            pf_returns[date] = date_rets\n",
    "\n",
    "        return pd.DataFrame(pf_returns).T\n",
    "\n",
    "    def _build_long_short(self, pf_returns, n_groups=3):\n",
    "        \"\"\"Build SMB and characteristic-based long-short factor.\"\"\"\n",
    "        # Factor = avg(S portfolios) - avg(B portfolios)\n",
    "        s_cols = [c for c in pf_returns.columns if c.startswith(\"S/\")]\n",
    "        b_cols = [c for c in pf_returns.columns if c.startswith(\"B/\")]\n",
    "\n",
    "        smb = pf_returns[s_cols].mean(axis=1) - pf_returns[b_cols].mean(axis=1)\n",
    "\n",
    "        # Characteristic factor: high - low\n",
    "        high = n_groups - 1\n",
    "        high_cols = [f\"S/{high}\", f\"B/{high}\"]\n",
    "        low_cols = [\"S/0\", \"B/0\"]\n",
    "        char_factor = pf_returns[high_cols].mean(axis=1) - \\\n",
    "                      pf_returns[low_cols].mean(axis=1)\n",
    "\n",
    "        return smb, char_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `_double_sort` method implements the classic Fama-French portfolio construction: split on size at the median, split on the characteristic into terciles, and form 2 x 3 = 6 portfolios. The `_build_long_short` then constructs the factor as the average return of the high-characteristic portfolios minus the average return of the low-characteristic portfolios, averaged across size groups. This design \u2014 averaging across the size dimension \u2014 ensures that each factor captures the *pure* characteristic effect, not a size effect in disguise.\n",
    "\n",
    "Now let's add the orchestration method that builds all six factors, plus the validation method that compares our output to Ken French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_all_factors(self):\n",
    "    \"\"\"Build all 6 factors: Mkt-RF, SMB, HML, RMW, CMA, MOM.\"\"\"\n",
    "    self.compute_characteristics()\n",
    "\n",
    "    factor_series = {}\n",
    "\n",
    "    # Market factor (value-weighted universe excess return)\n",
    "    common = self.monthly_returns.index.intersection(ff3.index)\n",
    "    rf = ff3.loc[common, \"RF\"]\n",
    "\n",
    "    # VW market return approximation\n",
    "    valid_mcap = self.chars[\"market_cap\"].dropna()\n",
    "    weights = valid_mcap / valid_mcap.sum()\n",
    "    mkt_tickers = valid_mcap.index.tolist()\n",
    "    mkt_ret = (self.monthly_returns[mkt_tickers] * weights).sum(axis=1)\n",
    "    factor_series[\"Mkt-RF\"] = mkt_ret.loc[common] - rf\n",
    "\n",
    "    # HML (book-to-market: high = value, low = growth)\n",
    "    pf_bm, valid_bm = self._double_sort(\"book_to_market\", n_groups=3)\n",
    "    pf_bm_rets = self._compute_vw_returns(pf_bm, valid_bm)\n",
    "    smb_bm, hml = self._build_long_short(pf_bm_rets)\n",
    "\n",
    "    self.quality_report[\"portfolio_counts\"][\"HML\"] = {\n",
    "        k: len(v) for k, v in pf_bm.items()\n",
    "    }\n",
    "\n",
    "    # RMW (profitability: high = robust, low = weak)\n",
    "    if \"profitability\" in self.chars.columns:\n",
    "        valid_prof = self.chars.dropna(subset=[\"market_cap\", \"profitability\"])\n",
    "        if len(valid_prof) >= 30:\n",
    "            pf_prof, valid_p = self._double_sort(\"profitability\", n_groups=3)\n",
    "            pf_prof_rets = self._compute_vw_returns(pf_prof, valid_p)\n",
    "            smb_prof, rmw = self._build_long_short(pf_prof_rets)\n",
    "            self.quality_report[\"portfolio_counts\"][\"RMW\"] = {\n",
    "                k: len(v) for k, v in pf_prof.items()\n",
    "            }\n",
    "        else:\n",
    "            rmw = pd.Series(0, index=self.monthly_returns.index)\n",
    "    else:\n",
    "        rmw = pd.Series(0, index=self.monthly_returns.index)\n",
    "\n",
    "    # CMA (investment: low growth = conservative, high growth = aggressive)\n",
    "    # Note: CMA = Conservative - Aggressive, so we INVERT: low - high\n",
    "    if \"investment\" in self.chars.columns:\n",
    "        valid_inv = self.chars.dropna(subset=[\"market_cap\", \"investment\"])\n",
    "        if len(valid_inv) >= 30:\n",
    "            pf_inv, valid_i = self._double_sort(\"investment\", n_groups=3)\n",
    "            pf_inv_rets = self._compute_vw_returns(pf_inv, valid_i)\n",
    "            smb_inv, inv_factor = self._build_long_short(pf_inv_rets)\n",
    "            cma = -inv_factor  # Invert: conservative minus aggressive\n",
    "            self.quality_report[\"portfolio_counts\"][\"CMA\"] = {\n",
    "                k: len(v) for k, v in pf_inv.items()\n",
    "            }\n",
    "        else:\n",
    "            cma = pd.Series(0, index=self.monthly_returns.index)\n",
    "    else:\n",
    "        cma = pd.Series(0, index=self.monthly_returns.index)\n",
    "\n",
    "    # SMB: average of SMB from all three sorts\n",
    "    smb_components = [smb_bm]\n",
    "    if \"profitability\" in self.chars.columns:\n",
    "        smb_components.append(smb_prof)\n",
    "    if \"investment\" in self.chars.columns:\n",
    "        smb_components.append(smb_inv)\n",
    "    smb = pd.concat(smb_components, axis=1).mean(axis=1)\n",
    "\n",
    "    # Momentum (12-1 month return)\n",
    "    mom_monthly = []\n",
    "    for date in self.monthly_returns.index:\n",
    "        mom_end = date - pd.DateOffset(months=1)\n",
    "        mom_start = date - pd.DateOffset(months=12)\n",
    "        mask = (self.prices.index >= mom_start) & \\\n",
    "               (self.prices.index <= mom_end)\n",
    "        if mask.sum() < 20:\n",
    "            continue\n",
    "        mom_prices = self.prices.loc[mask]\n",
    "        if len(mom_prices) < 2:\n",
    "            continue\n",
    "        momentum = (mom_prices.iloc[-1] / mom_prices.iloc[0]) - 1\n",
    "        momentum = momentum.dropna()\n",
    "\n",
    "        rets = self.monthly_returns.loc[date].dropna()\n",
    "        common_t = momentum.index.intersection(rets.index)\n",
    "        if len(common_t) < 30:\n",
    "            continue\n",
    "\n",
    "        mom_sorted = momentum[common_t].sort_values()\n",
    "        n = len(mom_sorted)\n",
    "        losers = mom_sorted.iloc[:int(n * 0.3)].index\n",
    "        winners = mom_sorted.iloc[int(n * 0.7):].index\n",
    "\n",
    "        # Value-weighted\n",
    "        if \"market_cap\" in self.chars.columns:\n",
    "            w_win = self.chars.loc[\n",
    "                self.chars.index.isin(winners), \"market_cap\"\n",
    "            ].dropna()\n",
    "            w_los = self.chars.loc[\n",
    "                self.chars.index.isin(losers), \"market_cap\"\n",
    "            ].dropna()\n",
    "            if len(w_win) > 0 and len(w_los) > 0:\n",
    "                w_win = w_win / w_win.sum()\n",
    "                w_los = w_los / w_los.sum()\n",
    "                win_ret = (rets[w_win.index] * w_win).sum()\n",
    "                los_ret = (rets[w_los.index] * w_los).sum()\n",
    "            else:\n",
    "                win_ret = rets[winners].mean()\n",
    "                los_ret = rets[losers].mean()\n",
    "        else:\n",
    "            win_ret = rets[winners].mean()\n",
    "            los_ret = rets[losers].mean()\n",
    "\n",
    "        mom_monthly.append({\"date\": date, \"MOM\": win_ret - los_ret})\n",
    "\n",
    "    mom_df = pd.DataFrame(mom_monthly).set_index(\"date\")[\"MOM\"]\n",
    "\n",
    "    # Combine all factors\n",
    "    self.factor_returns = pd.DataFrame({\n",
    "        \"Mkt-RF\": factor_series[\"Mkt-RF\"],\n",
    "        \"SMB\": smb,\n",
    "        \"HML\": hml,\n",
    "        \"RMW\": rmw,\n",
    "        \"CMA\": cma,\n",
    "        \"MOM\": mom_df,\n",
    "    })\n",
    "\n",
    "    return self.factor_returns\n",
    "\n",
    "FactorBuilder.build_all_factors = build_all_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the CMA inversion: `cma = -inv_factor`. The Fama-French convention is \"Conservative Minus Aggressive\" \u2014 low-investment firms minus high-investment firms. Our `_build_long_short` returns high minus low by default, so we flip the sign. This is the kind of subtle convention that causes silent bugs in production factor pipelines. If you get CMA's sign wrong, your factor loadings invert, and your risk decomposition tells a backwards story.\n",
    "\n",
    "Momentum is constructed differently from the fundamental-based factors: it uses a 12-1 month price window (skipping the most recent month to avoid short-term reversal contamination) and does not require any fundamental data. This makes it our **cleanest benchmark** \u2014 if momentum replicates well, we know the price-based pipeline works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(self, official_ff5, official_carhart):\n",
    "    \"\"\"Compare self-built factors against official data.\"\"\"\n",
    "    results = {}\n",
    "    factor_map = {\n",
    "        \"SMB\": (\"SMB\", official_ff5),\n",
    "        \"HML\": (\"HML\", official_ff5),\n",
    "        \"RMW\": (\"RMW\", official_ff5),\n",
    "        \"CMA\": (\"CMA\", official_ff5),\n",
    "        \"MOM\": (\"MOM\", official_carhart),\n",
    "    }\n",
    "\n",
    "    for self_name, (off_name, off_df) in factor_map.items():\n",
    "        if self_name not in self.factor_returns.columns:\n",
    "            continue\n",
    "        if off_name not in off_df.columns:\n",
    "            continue\n",
    "\n",
    "        self_s = self.factor_returns[self_name].dropna()\n",
    "        common = self_s.index.intersection(off_df.index)\n",
    "        if len(common) < 12:\n",
    "            continue\n",
    "\n",
    "        corr = self_s.loc[common].corr(off_df.loc[common, off_name])\n",
    "        te = (self_s.loc[common] - off_df.loc[common, off_name]).std() \\\n",
    "             * np.sqrt(12)\n",
    "\n",
    "        results[self_name] = {\n",
    "            \"correlation\": corr,\n",
    "            \"tracking_error_ann\": te,\n",
    "            \"n_months\": len(common),\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "FactorBuilder.validate = validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the class complete, let's run it end-to-end. The `build_all_factors` method orchestrates the full pipeline: compute characteristics from fundamentals, form double-sort portfolios for each factor, compute value-weighted returns, and construct long-short factor returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = FactorBuilder(monthly_returns, prices, fundamentals)\nfactor_returns = builder.build_all_factors()\n\nprint(f\"Factor returns shape: {factor_returns.shape}\")\nprint(f\"\\nFactor return statistics (monthly):\")\nprint(factor_returns.describe().round(6).to_string())\n\nvalidation = builder.validate(ff5, carhart)\n\nprint(\"\\nValidation Against Official Data:\")\nprint(f\"{'Factor':<8} {'Corr':>8} {'TE (ann)':>10} {'N months':>10}\")\nprint(\"-\" * 40)\nfor name, vals in validation.items():\n    print(f\"{name:<8} {vals['correlation']:>8.4f} \"\n          f\"{vals['tracking_error_ann']:>10.4f} {vals['n_months']:>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Six factor return series, each spanning roughly 131 months. Now the moment of truth: how well do our self-built factors match Ken French's official data? This validation step is not optional \u2014 at any serious quant shop, factor returns that have not been benchmarked against a known reference are not trusted, full stop.\n\nThe validation reveals a clear hierarchy. HML replicates best (r ~ 0.83), followed by MOM (r ~ 0.64), then RMW (r ~ 0.35), SMB (r ~ 0.33), and CMA (r ~ 0.29). Compare this to production benchmarks: Tidy Finance (Scheuch, Voigt & Weiss, 2023), using the full CRSP/Compustat universe with proper annual rebalancing, achieves correlations above 0.95 for *all* factors.\n\nBut the hierarchy itself is the insight. HML replicates well because value-growth dispersion exists within large-caps \u2014 some S&P 500 companies trade at 20x earnings (growth) while others trade at 8x (value). SMB replicates poorly (r ~ 0.33) because our \"small\" stocks have market caps above \\$10B. Ken French's small-cap bucket includes companies at \\$100M\u2013\\$2B. We are measuring large-versus-very-large, not small-versus-large.\n\nAn important caveat on the HML correlation: our implementation uses a **static sort** \u2014 we classify stocks once using ~2021\u20132025 fundamentals and apply those fixed assignments over the entire 2014\u20132024 price window. This introduces look-ahead bias and flatters the correlation relative to a proper annual-rebalancing approach. The high HML correlation is partly real (value/growth classifications are stable among large-cap S&P 500 stocks over a decade) and partly a methodological artifact.\n\nAlso note that MOM correlation is 0.64 here \u2014 lower than the 0.85 seen in the seminar. The difference is not a bug. The seminar used equal-weighted portfolios for momentum, while the `FactorBuilder` uses value-weighted portfolios (matching the Fama-French methodology). Value-weighting concentrates the portfolio in mega-cap stocks, whose momentum signals are noisier relative to the broad market. This 0.21 correlation gap from a single weighting choice illustrates how \"the momentum factor\" is not a unique object \u2014 construction methodology matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr = builder.quality_report\n",
    "print(f\"\\nQuality Report:\")\n",
    "print(f\"  Missing equity: {len(qr['missing_equity'])} tickers\")\n",
    "print(f\"  Missing income: {len(qr['missing_income'])} tickers\")\n",
    "print(f\"  Net Income fallback: {len(qr['net_income_fallback'])} tickers\")\n",
    "print(f\"  Portfolio counts:\")\n",
    "for factor_name, counts in qr[\"portfolio_counts\"].items():\n",
    "    print(f\"    {factor_name}: {counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality report tells us the cost of using free data. About 11 tickers are missing book equity entirely (negative equity or no balance sheet data \u2014 often financials with complex capital structures). Another 15 tickers fall back to Net Income when Operating Income is unavailable. This is standard for yfinance: banks and financial companies like JPM and GS use a different income statement structure where \"Operating Income\" is not a meaningful line item. In production, Compustat standardizes these across all firms. With free data, you approximate.\n",
    "\n",
    "The portfolio counts confirm that all six double-sort portfolios have at least 19 stocks each \u2014 adequate for value-weighted return computation. If any portfolio had fewer than 10 stocks, we would need to widen breakpoints or reduce the number of sorts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "for col in factor_returns.columns:\n",
    "    cum = (1 + factor_returns[col].dropna()).cumprod()\n",
    "    ax.plot(cum.index, cum, label=col, lw=1.5)\n",
    "ax.set(title=\"Self-Built Factor Cumulative Returns\",\n",
    "       xlabel=\"Date\", ylabel=\"Cumulative Return\")\n",
    "ax.legend(ncol=3)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(1.0, color=\"gray\", lw=0.5, ls=\"--\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cumulative return plot brings the validation numbers to life. The market factor (Mkt-RF) dominates with strong cumulative growth \u2014 reflecting the exceptional 2014\u20132024 equity bull market driven by tech. Momentum shows its characteristic pattern: steady accumulation punctuated by sharp drawdowns. SMB bounces around zero \u2014 no surprise, since our universe lacks true small-caps. HML and CMA are flat or slightly negative, consistent with the widely discussed \"death of value\" in the post-2007 era.\n",
    "\n",
    "If you are a portfolio manager at a systematic fund, this plot is your morning dashboard. The factor you are loading on dictates your P&L. And the fact that different factors have radically different return profiles over the *same* period is why factor allocation \u2014 choosing *which* factors to tilt toward \u2014 is a job unto itself.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverable 2: The Cross-Sectional Feature Matrix\n\nYour second mission: build the bridge between factor models and machine learning.\n\nIn the lecture, we showed you that factors are really just features with economic theory behind them. Now you build the feature engineering system that translates raw financial data into a clean, standardized panel ready for ML consumption. This is *exactly* the preprocessing pipeline that a data scientist at an asset management firm would build \u2014 the step that sits between raw fundamental/price data and the gradient boosting or neural network models of Week 4.\n\nGu, Kelly & Xiu (2020) \u2014 the paper that settled the debate about whether ML works for stock prediction \u2014 used 94 firm characteristics. We will build 7 (4 fundamental + 3 technical). The scale is different, but the methodology is identical: compute raw characteristics, winsorize cross-sectionally, standardize to z-scores, and output a clean panel. The *how* matters more than the *how many*, because every one of the 94 features in GKX follows the same pipeline you are about to build.\n\nA warning that will save you from a subtle trap: not all financial ratios are safe to z-score and feed to ML. P/E ratio \u2014 seemingly the most basic valuation metric \u2014 is pathological. It is undefined when earnings are zero, sign-flips for loss-making firms, and produces a non-monotonic cross-sectional distribution that breaks standardization. Earnings yield (E/P) is the correct feature because it is bounded and handles losses gracefully. This is why MSCI Barra models use E/P, not P/E, as a style factor.\n\n### Student Workspace\n\nBuild a `FeatureEngineer` class that:\n- Computes fundamental features: P/B, ROE, asset growth, earnings yield\n- Computes technical features: 12-1 month momentum, 1-month reversal, 60-day trailing volatility\n- Applies two-stage outlier control: [1st, 99th] percentile winsorization + z-score hard cap at \u00b13.0\n- Provides both z-scored and rank-transformed outputs\n- Reports missing data statistics\n- Outputs an ML-ready panel saved to Parquet\n\nTease: the standardization verification will reveal why a z-cap at \u00b13 is necessary even after winsorization \u2014 with only ~179 stocks per cross-section, percentile clipping is not enough to tame heavy-tailed financial ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the FeatureEngineer class\n",
    "# - __init__: accept prices, monthly_returns, fundamentals\n",
    "# - _compute_fundamental_chars: P/B, ROE, asset growth, earnings yield\n",
    "# - build: assemble the full panel with technical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement winsorization + z-scoring + rank transform\n",
    "# - _winsorize_and_zscore: two-stage outlier control\n",
    "# - _build_quality_report: missing data stats"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "With the class defined and the standardization logic in place, you are ready to run the pipeline. The `build` method will loop through every month in the sample, compute technical features (momentum, reversal, volatility), merge with static fundamentals, and apply winsorization + z-scoring cross-sectionally. Expect this to take 30\u201360 seconds for ~130 months.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run the pipeline\n",
    "# fe = FeatureEngineer(prices, monthly_returns, fundamentals)\n",
    "# feature_matrix = fe.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "After running the pipeline, you should have a panel with ~23,000 rows and 21 columns (7 raw + 7 z-scored + 7 rank-transformed). The final step is to extract the ML-ready subset (z-scored columns only), verify the standardization properties, and visualize the distributions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Verify standardization and export ML-ready matrix\n",
    "# ml_matrix = fe.get_ml_ready_matrix()\n",
    "# ml_matrix.to_parquet(\"feature_matrix_ml.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize feature distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## \u2501\u2501\u2501 SOLUTION: Deliverable 2 \u2501\u2501\u2501\n\nThe `FeatureEngineer` class follows the same pattern as `FactorBuilder`: compute raw characteristics, then process them cross-sectionally. The critical design choice is the two-stage outlier control. With only ~179 stocks per cross-section, the [1st, 99th] percentile winsorization clips only about 2 observations per tail \u2014 not enough to tame structurally skewed features like P/B, which can range from 1 to 200+. The hard z-cap at \u00b13.0 after standardization is the industry-standard safety net, used in production pipelines at Barra/MSCI and Axioma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineer:\n",
    "    \"\"\"Construct a standardized cross-sectional feature matrix.\n",
    "\n",
    "    Computes fundamental features (P/E, P/B, ROE, asset growth,\n",
    "    earnings yield) and technical features (momentum, reversal,\n",
    "    volatility), then applies winsorization and cross-sectional\n",
    "    standardization.\n",
    "\n",
    "    Attributes:\n",
    "        feature_matrix: Panel DataFrame (MultiIndex: date, ticker)\n",
    "            with standardized feature columns.\n",
    "        quality_report: Dict with missing data statistics.\n",
    "    \"\"\"\n",
    "\n",
    "    FUNDAMENTAL_FEATURES = [\n",
    "        \"pb_ratio\", \"roe\", \"asset_growth\", \"earnings_yield\",\n",
    "    ]\n",
    "    TECHNICAL_FEATURES = [\n",
    "        \"momentum\", \"reversal\", \"volatility\",\n",
    "    ]\n",
    "    ALL_FEATURES = FUNDAMENTAL_FEATURES + TECHNICAL_FEATURES\n",
    "\n",
    "    def __init__(self, prices, monthly_returns, fundamentals):\n",
    "        self.prices = prices\n",
    "        self.monthly_returns = monthly_returns\n",
    "        self.fundamentals = fundamentals\n",
    "        self.feature_matrix = None\n",
    "        self.quality_report = {}\n",
    "\n",
    "    def _compute_fundamental_chars(self):\n",
    "        \"\"\"Compute fundamental ratios from balance sheet and income.\"\"\"\n",
    "        bs = self.fundamentals[\"balance_sheet\"]\n",
    "        inc = self.fundamentals[\"income_stmt\"]\n",
    "        mcap = self.fundamentals[\"market_cap\"]\n",
    "\n",
    "        records = {}\n",
    "        for ticker in self.monthly_returns.columns:\n",
    "            row = {}\n",
    "\n",
    "            # Book equity and total assets\n",
    "            equity = np.nan\n",
    "            total_assets = np.nan\n",
    "            prev_assets = np.nan\n",
    "\n",
    "            if ticker in bs.index.get_level_values(\"ticker\"):\n",
    "                tk_bs = bs.loc[ticker].sort_index()\n",
    "                eq = tk_bs[\"Stockholders Equity\"].dropna()\n",
    "                if len(eq) > 0 and eq.iloc[-1] > 0:\n",
    "                    equity = eq.iloc[-1]\n",
    "                assets = tk_bs[\"Total Assets\"].dropna()\n",
    "                if len(assets) > 0:\n",
    "                    total_assets = assets.iloc[-1]\n",
    "                if len(assets) > 1:\n",
    "                    prev_assets = assets.iloc[-2]\n",
    "\n",
    "            # Income items\n",
    "            net_income = np.nan\n",
    "            revenue = np.nan\n",
    "            if ticker in inc.index.get_level_values(\"ticker\"):\n",
    "                tk_inc = inc.loc[ticker].sort_index()\n",
    "                ni = tk_inc.get(\"Net Income\", pd.Series(dtype=float)).dropna()\n",
    "                if len(ni) > 0:\n",
    "                    net_income = ni.iloc[-1]\n",
    "                rev = tk_inc.get(\"Total Revenue\", pd.Series(dtype=float)).dropna()\n",
    "                if len(rev) > 0:\n",
    "                    revenue = rev.iloc[-1]\n",
    "\n",
    "            mk = mcap.get(ticker, np.nan)\n",
    "\n",
    "            # P/B ratio\n",
    "            if not np.isnan(mk) and mk > 0 and not np.isnan(equity) \\\n",
    "               and equity > 0:\n",
    "                row[\"pb_ratio\"] = mk / equity\n",
    "\n",
    "            # ROE\n",
    "            if not np.isnan(equity) and equity > 0 \\\n",
    "               and not np.isnan(net_income):\n",
    "                row[\"roe\"] = net_income / equity\n",
    "\n",
    "            # Asset growth\n",
    "            if not np.isnan(total_assets) and not np.isnan(prev_assets) \\\n",
    "               and prev_assets > 0:\n",
    "                row[\"asset_growth\"] = (total_assets / prev_assets) - 1\n",
    "\n",
    "            # Earnings yield\n",
    "            if not np.isnan(mk) and mk > 0 and not np.isnan(net_income):\n",
    "                row[\"earnings_yield\"] = net_income / mk\n",
    "\n",
    "            records[ticker] = row\n",
    "\n",
    "        return pd.DataFrame(records).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we compute **earnings yield** (E/P) rather than P/E. P/E is the ratio every financial news outlet quotes, but it is a trap for ML practitioners. When a firm reports a loss, P/E becomes negative \u2014 but the cross-sectional ordering between a firm with P/E = -5 and P/E = +5 is meaningless (one is unprofitable, the other is cheap). Earnings yield handles this gracefully: a loss-making firm simply has negative E/P. The cross-sectional ordering is always monotonic in \"profitability per dollar of market cap.\" This is exactly why MSCI Barra uses E/P, not P/E, as a style factor.\n",
    "\n",
    "Now the `build` method, which assembles the full panel by computing technical features for each month and merging them with the static fundamental characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(self):\n",
    "    \"\"\"Build the full feature matrix panel.\"\"\"\n",
    "    fund_chars = self._compute_fundamental_chars()\n",
    "    daily_returns = self.prices.pct_change().dropna(how=\"all\")\n",
    "\n",
    "    panel_records = []\n",
    "    for date in self.monthly_returns.index:\n",
    "        # Momentum: 12-1 month return\n",
    "        mom_end = date - pd.DateOffset(months=1)\n",
    "        mom_start = date - pd.DateOffset(months=12)\n",
    "        mask = (self.prices.index >= mom_start) & \\\n",
    "               (self.prices.index <= mom_end)\n",
    "        if mask.sum() < 20:\n",
    "            continue\n",
    "        mom_prices = self.prices.loc[mask]\n",
    "        if len(mom_prices) < 2:\n",
    "            continue\n",
    "        momentum = (mom_prices.iloc[-1] / mom_prices.iloc[0]) - 1\n",
    "\n",
    "        # Short-term reversal (1-month return)\n",
    "        reversal = self.monthly_returns.loc[date]\n",
    "\n",
    "        # Trailing volatility (60-day)\n",
    "        vol_idx = self.prices.index.get_indexer([date], method=\"pad\")[0]\n",
    "        vol_start = max(0, vol_idx - 60)\n",
    "        recent = daily_returns.iloc[vol_start:vol_idx]\n",
    "        volatility = recent.std() * np.sqrt(252)\n",
    "\n",
    "        for ticker in self.monthly_returns.columns:\n",
    "            if pd.isna(self.monthly_returns.loc[date, ticker]):\n",
    "                continue\n",
    "\n",
    "            row = {\n",
    "                \"date\": date,\n",
    "                \"ticker\": ticker,\n",
    "                \"momentum\": momentum.get(ticker, np.nan),\n",
    "                \"reversal\": reversal.get(ticker, np.nan),\n",
    "                \"volatility\": volatility.get(ticker, np.nan),\n",
    "            }\n",
    "\n",
    "            # Add fundamental features\n",
    "            if ticker in fund_chars.index:\n",
    "                for col in self.FUNDAMENTAL_FEATURES:\n",
    "                    if col in fund_chars.columns:\n",
    "                        row[col] = fund_chars.loc[ticker, col]\n",
    "\n",
    "            panel_records.append(row)\n",
    "\n",
    "    panel = pd.DataFrame(panel_records)\n",
    "    panel = panel.set_index([\"date\", \"ticker\"])\n",
    "\n",
    "    # Winsorize and standardize\n",
    "    self.feature_matrix = self._winsorize_and_zscore(panel)\n",
    "\n",
    "    # Quality report\n",
    "    self._build_quality_report()\n",
    "\n",
    "    return self.feature_matrix\n",
    "\n",
    "FeatureEngineer.build = build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The technical features \u2014 momentum, reversal, and volatility \u2014 are computed fresh for each month, while the fundamental features are static (computed once from the most recent annual filings). This is the standard approach: fundamentals update at most quarterly, while price-based signals update continuously. In production at a firm like Two Sigma or Citadel, the feature pipeline would re-compute price-based features daily and fundamental features quarterly, with point-in-time discipline to avoid look-ahead bias.\n",
    "\n",
    "Now the winsorization and standardization machinery \u2014 the part that separates a clean feature matrix from one that will silently corrupt downstream ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _winsorize_and_zscore(self, panel):\n",
    "    \"\"\"Winsorize at [1,99] percentiles, then z-score per month.\"\"\"\n",
    "    feature_cols = [c for c in self.ALL_FEATURES if c in panel.columns]\n",
    "\n",
    "    def process_month(group):\n",
    "        \"\"\"Winsorize and z-score one month.\"\"\"\n",
    "        for col in feature_cols:\n",
    "            vals = group[col].dropna()\n",
    "            if len(vals) > 10:\n",
    "                lo = vals.quantile(0.01)\n",
    "                hi = vals.quantile(0.99)\n",
    "                group[col] = group[col].clip(lo, hi)\n",
    "\n",
    "                mean, std = group[col].mean(), group[col].std()\n",
    "                if std > 0:\n",
    "                    group[col + \"_z\"] = ((group[col] - mean) / std).clip(-3.0, 3.0)\n",
    "                else:\n",
    "                    group[col + \"_z\"] = 0.0\n",
    "\n",
    "                # Rank transform (0 to 1)\n",
    "                group[col + \"_rank\"] = group[col].rank(pct=True)\n",
    "\n",
    "        return group\n",
    "\n",
    "    result = panel.groupby(level=\"date\", group_keys=False).apply(\n",
    "        process_month\n",
    "    )\n",
    "    return result\n",
    "\n",
    "FeatureEngineer._winsorize_and_zscore = _winsorize_and_zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at that `.clip(-3.0, 3.0)` on the z-scores. With ~179 stocks per cross-section, the [1st, 99th] percentile winsorization clips only about 2 observations per tail. For structurally skewed features \u2014 like P/B, where a handful of tech firms can have ratios above 50 while most firms sit between 1 and 10 \u2014 this is insufficient. The z-cap at \u00b13 is the industry-standard second stage: after percentile clipping in raw space, hard-cap in z-space. Barra USE4, Axioma, and every serious quant pipeline uses this two-stage approach.\n",
    "\n",
    "The z-cap mechanically reduces the standard deviation of the heaviest-tailed features \u2014 you will see z-std values around 0.84\u20131.00 instead of the theoretical 1.00. This is the expected cost of tail truncation, not a bug. The small histogram spikes you will see at \u00b13 are the capped mass accumulating at the boundaries.\n",
    "\n",
    "Now the quality report and the ML-ready export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_quality_report(self):\n",
    "    \"\"\"Report missing data statistics.\"\"\"\n",
    "    fm = self.feature_matrix\n",
    "    dates = fm.index.get_level_values(\"date\").unique()\n",
    "\n",
    "    for col in self.ALL_FEATURES:\n",
    "        if col in fm.columns:\n",
    "            missing_pct = fm[col].isna().mean()\n",
    "            self.quality_report[col] = {\n",
    "                \"missing_pct\": missing_pct,\n",
    "                \"n_non_null\": fm[col].notna().sum(),\n",
    "            }\n",
    "\n",
    "    # Monthly missing rates\n",
    "    worst_month_missing = {}\n",
    "    for col in self.ALL_FEATURES:\n",
    "        if col in fm.columns:\n",
    "            monthly_missing = fm.groupby(level=\"date\")[col].apply(\n",
    "                lambda x: x.isna().mean()\n",
    "            )\n",
    "            worst_month_missing[col] = monthly_missing.max()\n",
    "    self.quality_report[\"worst_month_missing\"] = worst_month_missing\n",
    "\n",
    "def get_ml_ready_matrix(self):\n",
    "    \"\"\"Return a clean panel for ML consumption.\n",
    "\n",
    "    Drops rows with too many missing features and returns\n",
    "    only z-scored columns.\n",
    "    \"\"\"\n",
    "    z_cols = [c for c in self.feature_matrix.columns if c.endswith(\"_z\")]\n",
    "    clean = self.feature_matrix[z_cols].dropna(thresh=len(z_cols) - 2)\n",
    "    return clean\n",
    "\n",
    "FeatureEngineer._build_quality_report = _build_quality_report\n",
    "FeatureEngineer.get_ml_ready_matrix = get_ml_ready_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_ml_ready_matrix` method applies a final filter: drop any row with more than 2 missing z-score features. This ensures that Week 4's models receive a panel where at least 5 of 7 features are present for every stock-month observation. The threshold of 2 is conservative \u2014 in production, you might impute the remaining NaNs with cross-sectional medians or use tree-based models that handle missing values natively.\n",
    "\n",
    "Let's run the full pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe = FeatureEngineer(prices, monthly_returns, fundamentals)\nfeature_matrix = fe.build()\n\nprint(f\"Feature matrix shape: {feature_matrix.shape}\")\nprint(f\"Columns: {feature_matrix.columns.tolist()}\")\n\nprint(f\"\\nMissing Data Report:\")\nfor feat in fe.ALL_FEATURES:\n    if feat in fe.quality_report:\n        info = fe.quality_report[feat]\n        print(f\"  {feat}: {info['missing_pct']:.1%} missing \"\n              f\"({info['n_non_null']} non-null)\")\n\nprint(f\"\\nWorst monthly missing rates:\")\nfor feat, rate in fe.quality_report.get(\"worst_month_missing\", {}).items():\n    print(f\"  {feat}: {rate:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The panel has ~23,000 rows (approximately 179 tickers times 130 months) with raw features, z-scored features, and rank-transformed features for each of the 7 characteristics. Now let's check the quality report to see where the gaps are.\n\nThe pattern is clear: price-based features (momentum, reversal, volatility) have less than 1% missing data because they rely only on daily prices, which yfinance provides reliably. Fundamental features (pb_ratio, roe) have about 6% missing \u2014 those are the ~11 tickers with missing or negative book equity. Asset growth and earnings yield have slightly lower missing rates because they require less data (total assets is almost always available, and earnings yield uses Net Income which has broader coverage than Stockholders Equity).\n\nNow let's verify the standardization is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_cols = [c for c in feature_matrix.columns if c.endswith(\"_z\")]\n",
    "sample_dates = feature_matrix.index.get_level_values(\"date\").unique()[:12]\n",
    "\n",
    "z_means = []\n",
    "z_stds = []\n",
    "for date in sample_dates:\n",
    "    month = feature_matrix.loc[date]\n",
    "    for col in z_cols:\n",
    "        vals = month[col].dropna()\n",
    "        if len(vals) > 10:\n",
    "            z_means.append(vals.mean())\n",
    "            z_stds.append(vals.std())\n",
    "\n",
    "print(f\"\\nZ-score verification (first 12 months):\")\n",
    "print(f\"  Mean range: [{min(z_means):.4f}, {max(z_means):.4f}]\")\n",
    "print(f\"  Std range: [{min(z_stds):.4f}, {max(z_stds):.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z-score means are near zero (within \u00b10.04) and standard deviations are in the range [0.84, 1.00]. The means are not exactly zero because the z-cap at \u00b13 slightly shifts the distribution \u2014 when you clip the right tail more than the left (or vice versa), the mean nudges toward the less-clipped side. The standard deviations below 1.0 reflect the mechanical compression from the z-cap: heavy-tailed features like pb_ratio lose more variance to the cap than well-behaved features like momentum. This is the expected cost of the two-stage outlier approach and is acceptable.\n",
    "\n",
    "Let's export the ML-ready matrix and visualize the feature distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_matrix = fe.get_ml_ready_matrix()\n",
    "print(f\"\\nML-ready matrix shape: {ml_matrix.shape}\")\n",
    "print(f\"Columns: {ml_matrix.columns.tolist()}\")\n",
    "\n",
    "# Save for downstream use\n",
    "ml_matrix.to_parquet(\"feature_matrix_ml.parquet\")\n",
    "print(f\"Saved to: feature_matrix_ml.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ML-ready matrix has ~23,000 rows and 7 z-scored feature columns \u2014 this is the direct input to Week 4's gradient boosting and neural network models. The Parquet format preserves the MultiIndex (date, ticker), column types, and is efficient to read. Every model you build next week will start with `pd.read_parquet(\"feature_matrix_ml.parquet\")`.\n",
    "\n",
    "Let's visualize the feature distributions to confirm the standardization and outlier control are working as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(z_cols[:8]):\n",
    "    ax = axes[i]\n",
    "    vals = feature_matrix[col].dropna()\n",
    "    ax.hist(vals, bins=50, alpha=0.7, edgecolor=\"k\", linewidth=0.3)\n",
    "    ax.set_title(col.replace(\"_z\", \"\"), fontsize=10)\n",
    "    ax.axvline(0, color=\"red\", lw=1, ls=\"--\")\n",
    "\n",
    "plt.suptitle(\"Feature Distributions (Z-scored)\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the small spikes at \u00b13 in the heaviest-tailed features (pb_ratio, roe). Those are the observations that got capped by the z-score hard limit \u2014 the mass that would have been at z = 4, 5, or even 7+ without the cap is now piled up at the boundary. For well-behaved features like momentum and reversal, the distributions are smooth and roughly symmetric, with very little mass at the boundaries. This confirms that the two-stage outlier control is doing exactly what it should: taming the problematic features while leaving the well-behaved ones alone.\n",
    "\n",
    "The feature correlation structure is also worth examining. P/B and ROE show a strong positive correlation (~0.85) \u2014 firms with high price-to-book tend to have high return on equity, because the market pays a premium for profitable companies. P/B and book-to-market show a strong negative correlation (~-0.65), which is expected since they are near-reciprocals (P/B = 1 / B/M). Momentum and reversal show a negative cross-sectional correlation (~-0.22), consistent with the well-documented short-term reversal effect: last month's winners tend to partially reverse.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverable 3: The Factor Model Horse Race\n\nYour final mission: settle the question of which factor model best explains the cross-section of stock returns.\n\nCAPM says one factor (beta) is enough. Fama and French said three factors are better (add size and value). Then they said five are better still (add profitability and investment). Each model claims to explain more of the cross-section \u2014 but how much more, exactly? And more importantly for your Week 4 ambitions: how much *residual alpha* remains unexplained? That residual is the signal that ML models will try to capture.\n\nYou will run a head-to-head Fama-MacBeth comparison of all three nested models on your full 200-stock universe. The Fama-MacBeth procedure \u2014 cross-sectional regressions every month, then time-series averages of the slopes \u2014 is the gold standard test for whether a characteristic carries a risk premium. It is the test that any quant researcher at AQR or Two Sigma would use to evaluate a new signal before committing capital.\n\nTease: the horse race delivers a clear winner \u2014 but a modest one. Even the best factor model leaves ~85% of cross-sectional return variation unexplained. The average stock has about 5% monthly unexplained return. That is both the promise and the curse of cross-sectional ML: there is enormous residual to capture, but most of it is noise.\n\n### Student Workspace\n\nBuild the horse race:\n1. Estimate rolling 60-month market betas for all stocks\n2. Compute static fundamental characteristics (size, B/M, profitability, investment)\n3. Build Fama-MacBeth panels for three nested models: CAPM (beta only), FF3 (+ size, value), FF5 (+ profitability, investment)\n4. Run `linearmodels.FamaMacBeth` with Newey-West standard errors for each\n5. Compare R-squared progression, factor significance, and residual alpha across models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Estimate rolling 60-month market betas\n",
    "# For each month starting at month 60, regress each stock's\n",
    "# excess returns on Mkt-RF over the trailing 60 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute static characteristics\n",
    "# log_mcap, book-to-market, profitability, investment"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "With betas and characteristics in hand, the next step is assembling the Fama-MacBeth panel and running all three regressions. Remember that `linearmodels.FamaMacBeth` requires a MultiIndex of (entity, time) and that you should standardize characteristics cross-sectionally before regression to make gamma coefficients comparable across variables.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build the panel and run Fama-MacBeth for all three models\n",
    "# CAPM: [beta]\n",
    "# FF3: [beta, log_mcap, bm]\n",
    "# FF5: [beta, log_mcap, bm, profitability, investment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare R-squared, residual alpha, and visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## \u2501\u2501\u2501 SOLUTION: Deliverable 3 \u2501\u2501\u2501\n\nThe horse race begins with the slowest step: estimating rolling 60-month market betas for every stock in the universe. This is the time-series step of the Fama-MacBeth procedure \u2014 for each stock, we regress its excess returns on the market factor over a trailing window. The 60-month window is standard in academic finance (roughly matching a business cycle), though practitioners often use shorter windows (24\u201336 months) for more responsive beta estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_idx = monthly_returns.index.intersection(ff3.index)\n",
    "rf = ff3.loc[common_idx, \"RF\"]\n",
    "excess_returns = monthly_returns.loc[common_idx].sub(rf, axis=0)\n",
    "mkt_rf = ff3.loc[common_idx, \"Mkt-RF\"]\n",
    "\n",
    "# Rolling 60-month market betas\n",
    "window = 60\n",
    "beta_panel = {}\n",
    "\n",
    "for i in range(window, len(common_idx)):\n",
    "    date = common_idx[i]\n",
    "    mkt_window = mkt_rf.iloc[i - window:i]\n",
    "    betas = {}\n",
    "    for ticker in excess_returns.columns:\n",
    "        y = excess_returns[ticker].iloc[i - window:i].dropna()\n",
    "        if len(y) < 36:\n",
    "            continue\n",
    "        x = mkt_window.loc[y.index]\n",
    "        X = sm.add_constant(x)\n",
    "        model = sm.OLS(y, X).fit()\n",
    "        betas[ticker] = model.params[\"Mkt-RF\"]\n",
    "    beta_panel[date] = betas\n",
    "\n",
    "beta_df = pd.DataFrame(beta_panel).T\n",
    "print(f\"Rolling beta panel: {beta_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rolling beta panel has one row per month (starting at month 61 \u2014 we need 60 months of history before the first estimate) and one column per ticker. This consumes the first ~5 years of our 11-year sample, leaving about 71 months of usable cross-sections. In production, you would estimate betas daily for faster response to regime changes, but monthly is standard for academic tests.\n",
    "\n",
    "Now the static fundamental characteristics. These are the same characteristics we computed in Deliverable 1, but here they serve as Fama-MacBeth regressors rather than portfolio sort variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static characteristics\n",
    "bs = fundamentals[\"balance_sheet\"]\n",
    "inc = fundamentals[\"income_stmt\"]\n",
    "mcap = fundamentals[\"market_cap\"]\n",
    "\n",
    "fund_chars = {}\n",
    "for ticker in monthly_returns.columns:\n",
    "    chars = {}\n",
    "\n",
    "    # Log market cap (size)\n",
    "    if ticker in mcap.index and mcap[ticker] > 0:\n",
    "        chars[\"log_mcap\"] = np.log(mcap[ticker])\n",
    "\n",
    "    # Book-to-market (value)\n",
    "    if ticker in bs.index.get_level_values(\"ticker\"):\n",
    "        eq = bs.loc[ticker].sort_index()[\"Stockholders Equity\"].dropna()\n",
    "        if len(eq) > 0 and eq.iloc[-1] > 0:\n",
    "            if ticker in mcap.index and mcap[ticker] > 0:\n",
    "                chars[\"bm\"] = eq.iloc[-1] / mcap[ticker]\n",
    "\n",
    "    # Operating profitability\n",
    "    if ticker in inc.index.get_level_values(\"ticker\"):\n",
    "        tk_inc = inc.loc[ticker].sort_index()\n",
    "        oi = tk_inc.get(\"Operating Income\", pd.Series(dtype=float)).dropna()\n",
    "        if len(oi) == 0:\n",
    "            oi = tk_inc.get(\"Net Income\", pd.Series(dtype=float)).dropna()\n",
    "        if len(oi) > 0:\n",
    "            if ticker in bs.index.get_level_values(\"ticker\"):\n",
    "                eq2 = bs.loc[ticker].sort_index()[\"Stockholders Equity\"].dropna()\n",
    "                if len(eq2) > 0 and eq2.iloc[-1] > 0:\n",
    "                    chars[\"profitability\"] = oi.iloc[-1] / eq2.iloc[-1]\n",
    "\n",
    "    # Investment (asset growth)\n",
    "    if ticker in bs.index.get_level_values(\"ticker\"):\n",
    "        assets = bs.loc[ticker].sort_index()[\"Total Assets\"].dropna()\n",
    "        if len(assets) > 1 and assets.iloc[-2] > 0:\n",
    "            chars[\"investment\"] = (assets.iloc[-1] / assets.iloc[-2]) - 1\n",
    "\n",
    "    fund_chars[ticker] = chars\n",
    "\n",
    "fund_df = pd.DataFrame(fund_chars).T\n",
    "print(f\"Fundamental characteristics: {fund_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we assemble the panel that Fama-MacBeth will consume. The panel has one row per stock-month observation, with excess returns as the dependent variable and characteristics as independent variables. The key requirement for `linearmodels.FamaMacBeth` is a MultiIndex of (entity, time) \u2014 in our case, (ticker, date)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build panels for each model specification\npanel_records = []\n\nfor date in beta_df.index:\n    for ticker in monthly_returns.columns:\n        if date not in excess_returns.index:\n            continue\n        ret = excess_returns.loc[date, ticker]\n        if pd.isna(ret):\n            continue\n        beta_val = beta_df.loc[date].get(ticker, np.nan)\n        if pd.isna(beta_val):\n            continue\n\n        row = {\n            \"date\": date,\n            \"ticker\": ticker,\n            \"excess_ret\": ret,\n            \"beta\": beta_val,\n        }\n        if ticker in fund_df.index:\n            for col in fund_df.columns:\n                row[col] = fund_df.loc[ticker, col]\n        panel_records.append(row)\n\nfull_panel = pd.DataFrame(panel_records)\nfull_panel = full_panel.set_index([\"ticker\", \"date\"]).sort_index()\n\nall_chars = [\"beta\", \"log_mcap\", \"bm\", \"profitability\", \"investment\"]\n\ndef standardize_month(group):\n    \"\"\"Z-score standardize within each month.\"\"\"\n    for col in all_chars:\n        if col in group.columns:\n            vals = group[col]\n            mean, std = vals.mean(), vals.std()\n            if std > 0:\n                group[col] = (vals - mean) / std\n    return group\n\npanel_std = full_panel.groupby(level=\"date\", group_keys=False).apply(\n    standardize_month\n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running Fama-MacBeth, we standardize all characteristics cross-sectionally within each month. This ensures that the risk premium estimates (gamma coefficients) are comparable across characteristics \u2014 otherwise, the coefficient for log_mcap (which ranges from ~23 to ~28) would be mechanically smaller than the coefficient for book-to-market (which ranges from ~0.01 to ~0.80) simply due to scale differences.\n\nNow the main event: three nested Fama-MacBeth regressions. CAPM uses only beta. FF3 adds size (log market cap) and value (book-to-market). FF5 adds profitability and investment. For each model, we compute the average cross-sectional R-squared and the average absolute residual \u2014 the unexplained return that ML might capture.\n\nThe `linearmodels.FamaMacBeth` with `cov_type='kernel'` applies Newey-West standard errors, which correct for serial correlation in the time-series of cross-sectional slopes. Without this correction, t-statistics can be inflated \u2014 or deflated, depending on the autocorrelation structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three model specifications\n",
    "models = {\n",
    "    \"CAPM\": [\"beta\"],\n",
    "    \"FF3\": [\"beta\", \"log_mcap\", \"bm\"],\n",
    "    \"FF5\": [\"beta\", \"log_mcap\", \"bm\", \"profitability\", \"investment\"],\n",
    "}\n",
    "\n",
    "fm_results = {}\n",
    "\n",
    "for model_name, char_list in models.items():\n",
    "    clean = panel_std.dropna(subset=char_list + [\"excess_ret\"])\n",
    "    if len(clean) < 1000:\n",
    "        print(f\"  {model_name}: insufficient data ({len(clean)} obs)\")\n",
    "        continue\n",
    "\n",
    "    dep = clean[[\"excess_ret\"]]\n",
    "    indep = clean[char_list]\n",
    "\n",
    "    fm = LMFamaMacBeth(dep, indep).fit(cov_type=\"kernel\")\n",
    "\n",
    "    # Compute average cross-sectional R\u00b2\n",
    "    r2_list = []\n",
    "    dates = clean.index.get_level_values(\"date\").unique()\n",
    "    for date in dates:\n",
    "        md = clean.loc[clean.index.get_level_values(\"date\") == date]\n",
    "        if len(md) < 30:\n",
    "            continue\n",
    "        y = md[\"excess_ret\"].values\n",
    "        X = sm.add_constant(md[char_list].values)\n",
    "        model_ols = sm.OLS(y, X).fit()\n",
    "        r2_list.append(model_ols.rsquared)\n",
    "\n",
    "    avg_r2 = np.mean(r2_list)\n",
    "\n",
    "    # Residual alpha\n",
    "    resid_abs = []\n",
    "    for date in dates:\n",
    "        md = clean.loc[clean.index.get_level_values(\"date\") == date]\n",
    "        if len(md) < 30:\n",
    "            continue\n",
    "        y = md[\"excess_ret\"].values\n",
    "        X = sm.add_constant(md[char_list].values)\n",
    "        model_ols = sm.OLS(y, X).fit()\n",
    "        resid_abs.extend(np.abs(model_ols.resid))\n",
    "\n",
    "    avg_abs_resid = np.mean(resid_abs)\n",
    "\n",
    "    fm_results[model_name] = {\n",
    "        \"params\": fm.params,\n",
    "        \"tstats\": fm.tstats,\n",
    "        \"pvalues\": fm.pvalues,\n",
    "        \"avg_r2\": avg_r2,\n",
    "        \"avg_abs_residual\": avg_abs_resid,\n",
    "        \"n_obs\": len(clean),\n",
    "        \"n_months\": len(dates),\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{model_name} ({len(clean)} obs, {len(dates)} months):\")\n",
    "    print(f\"  Avg cross-sectional R\u00b2: {avg_r2:.4f}\")\n",
    "    print(f\"  Avg |residual|: {avg_abs_resid:.6f}\")\n",
    "    for char in char_list:\n",
    "        print(f\"  {char}: gamma={fm.params[char]:.6f}, \"\n",
    "              f\"NW t={fm.tstats[char]:.2f}, p={fm.pvalues[char]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results tell a clear story. R-squared increases monotonically: CAPM (~8.6%), FF3 (~13.9%), FF5 (~15.3%). Every factor you add improves the cross-sectional fit \u2014 but the improvements shrink: +5.4 percentage points from CAPM to FF3, only +1.4 from FF3 to FF5. This is diminishing returns in action: the first two factors (size and value) add more explanatory power than the next two (profitability and investment).\n",
    "\n",
    "Look at which factors are statistically significant in the FF5 specification. Beta carries a significant premium (t ~ 2.37, p ~ 0.018) \u2014 surprising if you have read the textbook literature, which documents the \"beta anomaly\" (flat or negative cross-sectional relationship between beta and returns). But our sample is 2014\u20132024, dominated by the AI/tech boom where high-beta stocks (NVDA, TSLA, AMD) earned extraordinary returns. This is the same sample-dependence lesson from the lecture: the \"right\" answer to \"is beta priced?\" depends on which decade you examine.\n",
    "\n",
    "Investment is also significant (t ~ 2.41) \u2014 firms with lower asset growth (\"conservative\" companies) earned higher risk-adjusted returns. Profitability is insignificant (t ~ -0.43), which is surprising given the literature. With CRSP data spanning 60+ years, profitability is one of the more robust factors (Novy-Marx, 2013). With our 71-month window and S&P 500 universe, there is not enough statistical power to detect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_values = {name: res[\"avg_r2\"] for name, res in fm_results.items()}\n",
    "residuals = {name: res[\"avg_abs_residual\"] for name, res in fm_results.items()}\n",
    "\n",
    "print(f\"\\nR\u00b2 Progression:\")\n",
    "for name in [\"CAPM\", \"FF3\", \"FF5\"]:\n",
    "    if name in r2_values:\n",
    "        print(f\"  {name}: R\u00b2 = {r2_values[name]:.4f}, \"\n",
    "              f\"|resid| = {residuals[name]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R-squared progression from 8.6% to 15.3% means that even the *best* factor model leaves about 85% of cross-sectional return variation unexplained. The average stock has roughly 4.9% monthly unexplained return (absolute residual) under FF5. To put that in perspective: if your ML model from Week 4 can predict even 1% of that 4.9% residual, you have a potentially tradeable signal. But the other 3.9% is noise that will fight you every step of the way.\n",
    "\n",
    "This is the fundamental tension of cross-sectional ML: the opportunity space is enormous (85%+ unexplained variance), but the signal-to-noise ratio is terrible. This is why Gu, Kelly & Xiu (2020) found that neural networks improve upon linear models \u2014 but only by a modest margin. The noise floor is high.\n",
    "\n",
    "Let's visualize the horse race."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# R\u00b2 comparison\n",
    "ax = axes[0]\n",
    "model_names = list(fm_results.keys())\n",
    "r2_vals = [fm_results[m][\"avg_r2\"] for m in model_names]\n",
    "bars = ax.bar(model_names, r2_vals, color=[\"#66BB6A\", \"#42A5F5\", \"#EF5350\"],\n",
    "              edgecolor=\"k\", linewidth=0.5)\n",
    "for bar, val in zip(bars, r2_vals):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.001,\n",
    "            f\"{val:.4f}\", ha=\"center\", fontsize=10)\n",
    "ax.set(title=\"Average Cross-Sectional R\u00b2\", ylabel=\"R\u00b2\")\n",
    "\n",
    "# Residual alpha\n",
    "ax = axes[1]\n",
    "resid_vals = [fm_results[m][\"avg_abs_residual\"] * 100 for m in model_names]\n",
    "bars = ax.bar(model_names, resid_vals, color=[\"#66BB6A\", \"#42A5F5\", \"#EF5350\"],\n",
    "              edgecolor=\"k\", linewidth=0.5)\n",
    "for bar, val in zip(bars, resid_vals):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.05,\n",
    "            f\"{val:.2f}%\", ha=\"center\", fontsize=10)\n",
    "ax.set(title=\"Average |Residual| (Monthly %)\", ylabel=\"Absolute Residual (%)\")\n",
    "\n",
    "# t-statistics for FF5 factors\n",
    "ax = axes[2]\n",
    "if \"FF5\" in fm_results:\n",
    "    ff5_chars = [\"beta\", \"log_mcap\", \"bm\", \"profitability\", \"investment\"]\n",
    "    ff5_tstats = [fm_results[\"FF5\"][\"tstats\"][c] for c in ff5_chars]\n",
    "    colors = [\"#2196F3\" if abs(t) >= 2.0 else \"#BDBDBD\" for t in ff5_tstats]\n",
    "    ax.barh(range(len(ff5_chars)), ff5_tstats, color=colors,\n",
    "            edgecolor=\"k\", linewidth=0.5)\n",
    "    ax.set_yticks(range(len(ff5_chars)))\n",
    "    ax.set_yticklabels(ff5_chars)\n",
    "    ax.axvline(2.0, color=\"red\", ls=\"--\", lw=1)\n",
    "    ax.axvline(-2.0, color=\"red\", ls=\"--\", lw=1)\n",
    "    ax.set(title=\"FF5 Fama-MacBeth t-Statistics\", xlabel=\"t-statistic\")\n",
    "\n",
    "plt.suptitle(\"Factor Model Horse Race: CAPM vs. FF3 vs. FF5\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three-panel visualization makes the horse race results immediately legible. The left panel shows the R-squared staircase \u2014 each model improves on the last, but with diminishing steps. The middle panel shows the average absolute residual barely budging: from 5.05% (CAPM) to 4.91% (FF5). Going from one factor to five reduced the unexplained return by only 0.14 percentage points per month. The right panel shows which factors clear the significance bar (red dashed lines at |t| = 2.0): beta and investment are significant, while size, value, and profitability are not \u2014 a result that would surprise many textbook authors but is consistent with the 2014\u20132024 sample period.\n\nIf you are a quant researcher at a fund, this chart is the starting point for a conversation: \"We can explain 15% of the cross-section with known factors. The remaining 85% is our opportunity. But most of it is noise. Week 4's ML models will try to separate the signal from the noise \u2014 using the feature matrix you just built in Deliverable 2 as their input.\"\n\n---\n\n## Summary of Discoveries\n\n- **Factor replication quality is factor-dependent, not just data-dependent.** HML (value) replicates well from S&P 500 data (r ~ 0.83) because value-growth dispersion exists within large-caps. SMB (size) replicates poorly (r ~ 0.33) because the entire S&P 500 is large-cap. The relative ordering of replication quality (HML > MOM > RMW > SMB > CMA) tells you which characteristics have meaningful dispersion in your universe and which require broader data.\n\n- **Construction methodology matters more than you think.** Switching from equal-weighted to value-weighted portfolios dropped MOM correlation from 0.85 to 0.64 \u2014 a 0.21 correlation gap from a single design choice. \"The momentum factor\" is not a unique object; it is an artifact of specific methodological decisions.\n\n- **Static sorts introduce look-ahead bias.** Our HML correlation of ~0.83 is flattered by classifying stocks once using ~2025 fundamentals and applying those assignments backward over the full 2014\u20132024 window. A proper annual-rebalancing implementation would likely produce lower correlations. Always understand the methodology behind any benchmark comparison.\n\n- **Feature engineering in finance requires domain knowledge.** P/E ratio \u2014 the most quoted valuation metric \u2014 is pathological for cross-sectional ML because it is undefined at E=0 and sign-flips for loss-makers. Earnings yield (E/P) is the correct feature. Winsorization at [1st, 99th] percentiles is insufficient for small universes; a z-cap at \u00b13 is the industry-standard safety net.\n\n- **Factor models explain only 8\u201315% of cross-sectional variation.** The horse race shows FF5 at 15.3% R-squared \u2014 a clear winner, but a modest one. The average stock has ~4.9% monthly unexplained return under the best model. This is the residual alpha \u2014 and it is exactly what Week 4's ML models will attempt to capture.\n\n- **Which factors are \"priced\" depends on the decade.** Beta is significantly priced in 2014\u20132024 (the AI/tech boom rewarded high-beta stocks), contradicting the classic \"beta anomaly\" documented with 1963\u20131990 data. Size and value are insignificant in our sample, despite being the star results in Fama and French (1992). Factor premia are sample-dependent \u2014 this is not a nuisance, it is the first-order problem.\n\n- **Production data closes most of these gaps.** With CRSP/Compustat (4,000+ stocks, point-in-time fundamentals, proper annual rebalancing), factor replication correlations exceed 0.95, cross-sectional tests span 60+ years, and factor premia estimates are far more stable. The gaps you saw today are a tour of what free data costs you \u2014 and why institutional data infrastructure is a multi-billion-dollar industry.\n\n- **The feature matrix you built is the direct input to Week 4.** Seven features, bounded within \u00b13, saved to Parquet. Next week's gradient boosting and neural network models will consume exactly this panel and ask: can ML extract signal from the 85% that linear factor models leave on the table?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}