{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar: Factor Models & Return Decomposition\n",
    "\n",
    "> *\"Of the 400+ published factors that 'predict' stock returns, roughly two-thirds fail to replicate. The factor zoo isn't a zoo \u2014 it's a graveyard with great marketing.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lecture showed you what factor models are and why they matter. Now you build them yourself \u2014 from raw data, with all the warts. You will construct Fama-French factors from scratch and discover exactly how far free data can (and cannot) take you. You will run the Fama-MacBeth procedure on live data and see that which factors are \"priced\" depends entirely on which decade you examine. You will decompose a portfolio's risk and find that the relationship between concentration and factor exposure is more subtle than textbooks suggest. And you will walk through the factor zoo, adding noise characteristics alongside real ones, to see firsthand how conventional significance thresholds produce false discoveries.\n",
    "\n",
    "Four exercises, each building on the lecture but pushing into territory the lecture did not cover. By the end, you will have hands-on evidence for why factor research is both powerful and dangerously fragile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Shared imports and data download \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# All data for all four exercises, downloaded once\n",
    "\n",
    "!pip install -q yfinance getfactormodels linearmodels statsmodels scipy tqdm\n",
    "\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import yfinance as yf\n",
    "from scipy import stats as scipy_stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.rcParams.update({\"figure.dpi\": 120, \"axes.grid\": True,\n",
    "                     \"grid.alpha\": 0.3, \"figure.figsize\": (10, 5)})\n",
    "\n",
    "# \u2500\u2500 200-ticker S&P 500 universe \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "TICKERS = [\n",
    "    \"AAPL\", \"MSFT\", \"NVDA\", \"AVGO\", \"AMD\", \"CSCO\", \"ADBE\", \"CRM\", \"INTC\", \"TXN\",\n",
    "    \"QCOM\", \"AMAT\", \"LRCX\", \"MU\", \"NOW\", \"INTU\", \"SNPS\", \"CDNS\", \"KLAC\", \"MCHP\",\n",
    "    \"META\", \"GOOG\", \"NFLX\", \"DIS\", \"CMCSA\", \"T\", \"VZ\", \"TMUS\", \"EA\", \"TTWO\",\n",
    "    \"AMZN\", \"TSLA\", \"HD\", \"MCD\", \"NKE\", \"LOW\", \"SBUX\", \"TJX\", \"BKNG\", \"ORLY\",\n",
    "    \"MAR\", \"YUM\", \"DHI\", \"CMG\", \"ROST\", \"LEN\", \"GPC\", \"BBY\", \"POOL\", \"GRMN\",\n",
    "    \"PG\", \"KO\", \"PEP\", \"COST\", \"WMT\", \"PM\", \"MO\", \"CL\", \"KMB\", \"GIS\",\n",
    "    \"SJM\", \"HSY\", \"KHC\", \"MNST\", \"STZ\", \"KR\", \"TSN\", \"SYY\", \"CHD\", \"MKC\",\n",
    "    \"XOM\", \"CVX\", \"COP\", \"SLB\", \"EOG\", \"MPC\", \"PSX\", \"VLO\", \"OXY\", \"HES\",\n",
    "    \"DVN\", \"FANG\", \"HAL\", \"BKR\", \"CTRA\", \"APA\", \"TRGP\", \"KMI\", \"WMB\", \"OKE\",\n",
    "    \"JPM\", \"BAC\", \"WFC\", \"GS\", \"MS\", \"BLK\", \"SCHW\", \"AXP\", \"USB\", \"PNC\",\n",
    "    \"TFC\", \"AIG\", \"MET\", \"PRU\", \"ALL\", \"CB\", \"MMC\", \"AON\", \"ICE\", \"CME\",\n",
    "    \"LLY\", \"UNH\", \"JNJ\", \"PFE\", \"ABT\", \"MRK\", \"TMO\", \"AMGN\", \"MDT\", \"ISRG\",\n",
    "    \"BMY\", \"GILD\", \"VRTX\", \"REGN\", \"ZTS\", \"SYK\", \"BSX\", \"BDX\", \"EW\", \"HCA\",\n",
    "    \"CAT\", \"GE\", \"HON\", \"UNP\", \"RTX\", \"BA\", \"DE\", \"LMT\", \"MMM\", \"GD\",\n",
    "    \"WM\", \"ITW\", \"EMR\", \"ETN\", \"FDX\", \"CSX\", \"NSC\", \"PCAR\", \"ROK\", \"FAST\",\n",
    "    \"LIN\", \"APD\", \"SHW\", \"ECL\", \"NEM\", \"FCX\", \"NUE\", \"VMC\", \"MLM\", \"DOW\",\n",
    "    \"PLD\", \"AMT\", \"CCI\", \"EQIX\", \"PSA\", \"SPG\", \"DLR\", \"O\", \"WELL\", \"ARE\",\n",
    "    \"NEE\", \"DUK\", \"SO\", \"D\", \"AEP\", \"SRE\", \"EXC\", \"XEL\", \"WEC\", \"ES\",\n",
    "]\n",
    "\n",
    "START, END = \"2014-01-01\", \"2024-12-31\"\n",
    "\n",
    "# \u2500\u2500 Download prices \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "print(\"Downloading equity prices...\")\n",
    "raw = yf.download(TICKERS, start=START, end=END, auto_adjust=True, progress=True)\n",
    "prices = raw[\"Close\"]\n",
    "completeness = prices.notna().mean()\n",
    "valid_tickers = completeness[completeness > 0.50].index.tolist()\n",
    "prices = prices[valid_tickers]\n",
    "prices.index = pd.to_datetime(prices.index).tz_localize(None)\n",
    "print(f\"Valid tickers: {len(valid_tickers)}\")\n",
    "\n",
    "# Monthly returns\n",
    "monthly_prices = prices.resample(\"ME\").last()\n",
    "monthly_returns = monthly_prices.pct_change().dropna(how=\"all\")\n",
    "\n",
    "# \u2500\u2500 Download official factor data \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "from getfactormodels import FamaFrenchFactors, CarhartFactors\n",
    "\n",
    "ff3_official = FamaFrenchFactors(model=\"3\", frequency=\"M\").to_pandas()\n",
    "if \"date\" in ff3_official.columns:\n",
    "    ff3_official[\"date\"] = pd.to_datetime(ff3_official[\"date\"])\n",
    "    ff3_official = ff3_official.set_index(\"date\")\n",
    "ff3_official.index = pd.to_datetime(ff3_official.index)\n",
    "\n",
    "ff5 = FamaFrenchFactors(model=\"5\", frequency=\"M\").to_pandas()\n",
    "if \"date\" in ff5.columns:\n",
    "    ff5[\"date\"] = pd.to_datetime(ff5[\"date\"])\n",
    "    ff5 = ff5.set_index(\"date\")\n",
    "ff5.index = pd.to_datetime(ff5.index)\n",
    "\n",
    "carhart = CarhartFactors(frequency=\"M\").to_pandas()\n",
    "if \"date\" in carhart.columns:\n",
    "    carhart[\"date\"] = pd.to_datetime(carhart[\"date\"])\n",
    "    carhart = carhart.set_index(\"date\")\n",
    "carhart.index = pd.to_datetime(carhart.index)\n",
    "\n",
    "# \u2500\u2500 Download fundamentals \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "print(\"\\nDownloading fundamentals...\")\n",
    "bs_records, inc_records = [], []\n",
    "sector_map, mcap_map = {}, {}\n",
    "\n",
    "for ticker_str in tqdm(valid_tickers, desc=\"Fundamentals\"):\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            tk = yf.Ticker(ticker_str)\n",
    "            bs = tk.balance_sheet\n",
    "            if bs is not None and not bs.empty:\n",
    "                for col_date in bs.columns:\n",
    "                    row = {}\n",
    "                    for field in [\"Stockholders Equity\", \"Total Assets\",\n",
    "                                  \"Ordinary Shares Number\"]:\n",
    "                        if field in bs.index:\n",
    "                            row[field] = bs.loc[field, col_date]\n",
    "                    if row:\n",
    "                        row[\"ticker\"] = ticker_str\n",
    "                        row[\"date\"] = pd.Timestamp(col_date).tz_localize(None)\n",
    "                        bs_records.append(row)\n",
    "            inc = tk.income_stmt\n",
    "            if inc is not None and not inc.empty:\n",
    "                for col_date in inc.columns:\n",
    "                    row = {\"ticker\": ticker_str,\n",
    "                           \"date\": pd.Timestamp(col_date).tz_localize(None)}\n",
    "                    for field in [\"Operating Income\", \"Net Income\",\n",
    "                                  \"Total Revenue\", \"Pretax Income\"]:\n",
    "                        if field in inc.index:\n",
    "                            row[field] = inc.loc[field, col_date]\n",
    "                    inc_records.append(row)\n",
    "            info = tk.info\n",
    "            sector_map[ticker_str] = info.get(\"sector\", \"Other\")\n",
    "            mcap_map[ticker_str] = info.get(\"marketCap\", np.nan)\n",
    "            break\n",
    "        except Exception:\n",
    "            if attempt < 2:\n",
    "                time.sleep(2 ** (attempt + 1))\n",
    "\n",
    "bs_df = pd.DataFrame(bs_records)\n",
    "if not bs_df.empty:\n",
    "    bs_df = bs_df.set_index([\"ticker\", \"date\"]).sort_index()\n",
    "inc_df = pd.DataFrame(inc_records)\n",
    "if not inc_df.empty:\n",
    "    inc_df = inc_df.set_index([\"ticker\", \"date\"]).sort_index()\n",
    "sectors = pd.Series(sector_map, name=\"sector\")\n",
    "mcap_current = pd.Series(mcap_map, name=\"market_cap\")\n",
    "\n",
    "print(f\"Balance sheet records: {len(bs_df)}\")\n",
    "print(f\"Income stmt records: {len(inc_df)}\")\n",
    "print(f\"Sectors: {sectors.nunique()} unique\")\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That cell downloads everything the entire seminar needs: 200 S&P 500 stocks with 11 years of daily prices, annual balance sheet and income statement data via yfinance, and official Fama-French factor returns from Ken French's data library. Every exercise below draws from this shared data \u2014 no further downloads required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Can You Replicate Fama-French?\n",
    "\n",
    "The lecture showed you the Fama-French factors as clean time series downloaded from Ken French's library. Neat, tidy, and ready to use. But where do those numbers actually come from? They come from sorting thousands of stocks into portfolios by size and book-to-market, computing value-weighted returns, and taking the long-short spread \u2014 every single month, using the full NYSE/AMEX/NASDAQ universe of 4,000+ stocks with CRSP/Compustat data going back to 1963.\n",
    "\n",
    "You have 200 S&P 500 stocks and yfinance. Can you replicate the canonical factors from this?\n",
    "\n",
    "**The question:** Construct SMB, HML, and momentum factors from raw data and compare your self-built versions against official Ken French returns. Where do they diverge \u2014 and more importantly, *why*?\n",
    "\n",
    "**Tasks:**\n",
    "1. Compute book-to-market ratios from balance sheet data and current market caps\n",
    "2. Perform a 2x3 double sort (median size, 30/70 B/M breakpoints) and form value-weighted portfolio returns\n",
    "3. Construct SMB and HML monthly return series from the six portfolios\n",
    "4. Construct a momentum factor using 12-1 month returns (top 30% vs. bottom 30%)\n",
    "5. Compute correlation and tracking error for each factor against official Ken French data\n",
    "6. Explain the divergence pattern: which factors replicate well, which do not, and why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Student workspace \u2014 Exercise 1\n# Step 1: Compute book-to-market for each ticker\n#   - Extract the most recent Stockholders Equity from bs_df\n#   - Divide by current market cap from mcap_current\n#   - Filter to tickers present in monthly_returns\n"
  },
  {
   "cell_type": "markdown",
   "source": "Once you have book-to-market ratios and market caps, the next step is to form portfolios and compute the long-short factor returns. Then validate against the official data.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2-3: Double-sort and compute factor returns\n",
    "#   - Sort on market_cap (median) and book_to_market (30/70)\n",
    "#   - For each month, compute value-weighted returns per portfolio\n",
    "#   - SMB = avg(Small portfolios) - avg(Big portfolios)\n",
    "#   - HML = avg(High B/M portfolios) - avg(Low B/M portfolios)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4-5: Momentum factor and validation\n",
    "#   - Momentum: 12-1 month price return, long top 30%, short bottom 30%\n",
    "#   - Compare each self-built factor to official Ken French data\n",
    "#   - Report correlation and annualized tracking error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### \u25b6 Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need book-to-market ratios. We take the most recent stockholders' equity per ticker, divide by the current market cap, and filter to tickers that also appear in our monthly return data. This is a static sort \u2014 we classify stocks once using the most recent fundamentals and apply those assignments over the full 11-year price window. That is a simplification: the proper Fama-French methodology rebalances portfolios annually each June using that year's fundamentals. Our approach introduces look-ahead bias \u2014 a stock classified as \"value\" using its 2025 book-to-market may have been \"growth\" in 2014. We will come back to what this means for the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_equity = bs_df[\"Stockholders Equity\"].dropna()\n",
    "book_equity = book_equity[book_equity > 0]\n",
    "\n",
    "bm_records = []\n",
    "for ticker in book_equity.index.get_level_values(\"ticker\").unique():\n",
    "    be = book_equity.loc[ticker].sort_index().iloc[-1]\n",
    "    if ticker in mcap_current.index and mcap_current[ticker] > 0:\n",
    "        bm_records.append({\n",
    "            \"ticker\": ticker,\n",
    "            \"book_equity\": be,\n",
    "            \"market_cap\": mcap_current[ticker],\n",
    "            \"book_to_market\": be / mcap_current[ticker],\n",
    "        })\n",
    "\n",
    "bm_df = pd.DataFrame(bm_records).set_index(\"ticker\")\n",
    "valid_tickers = set(bm_df.index) & set(monthly_returns.columns)\n",
    "bm_df = bm_df.loc[bm_df.index.isin(valid_tickers)]\n",
    "print(f\"Tickers with B/M: {len(bm_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 170 tickers have valid book-to-market ratios \u2014 the remainder are missing fundamental data (typically newer listings or data gaps in yfinance). Now comes the double sort: we split on median market cap and 30th/70th percentile book-to-market to form six portfolios. For each month, we compute value-weighted returns for each portfolio and construct SMB (small minus big) and HML (high B/M minus low B/M). Pay attention to the portfolio sizes \u2014 with only 200 stocks, each bucket holds roughly 30-35 stocks, compared to the hundreds or thousands in Ken French's full-universe sorts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_median = bm_df[\"market_cap\"].median()\n",
    "bm_30 = bm_df[\"book_to_market\"].quantile(0.30)\n",
    "bm_70 = bm_df[\"book_to_market\"].quantile(0.70)\n",
    "\n",
    "def assign_portfolio(row):\n",
    "    \"\"\"Assign to 2x3 portfolio.\"\"\"\n",
    "    size = \"S\" if row[\"market_cap\"] < size_median else \"B\"\n",
    "    if row[\"book_to_market\"] <= bm_30:\n",
    "        value = \"L\"\n",
    "    elif row[\"book_to_market\"] >= bm_70:\n",
    "        value = \"H\"\n",
    "    else:\n",
    "        value = \"M\"\n",
    "    return f\"{size}/{value}\"\n",
    "\n",
    "bm_df[\"portfolio\"] = bm_df.apply(assign_portfolio, axis=1)\n",
    "\n",
    "smb_monthly = []\n",
    "hml_monthly = []\n",
    "\n",
    "for date in monthly_returns.index:\n",
    "    rets = monthly_returns.loc[date].dropna()\n",
    "    available = set(rets.index) & set(bm_df.index)\n",
    "    if len(available) < 30:\n",
    "        continue\n",
    "\n",
    "    pf_rets = {}\n",
    "    for pf in [\"S/L\", \"S/M\", \"S/H\", \"B/L\", \"B/M\", \"B/H\"]:\n",
    "        tickers = bm_df[bm_df[\"portfolio\"] == pf].index\n",
    "        t_avail = [t for t in tickers if t in available]\n",
    "        if len(t_avail) == 0:\n",
    "            pf_rets[pf] = np.nan\n",
    "            continue\n",
    "        w = bm_df.loc[t_avail, \"market_cap\"]\n",
    "        w = w / w.sum()\n",
    "        pf_rets[pf] = (rets[t_avail] * w).sum()\n",
    "\n",
    "    if any(np.isnan(v) for v in pf_rets.values()):\n",
    "        continue\n",
    "\n",
    "    smb = (pf_rets[\"S/L\"] + pf_rets[\"S/M\"] + pf_rets[\"S/H\"]) / 3 \\\n",
    "        - (pf_rets[\"B/L\"] + pf_rets[\"B/M\"] + pf_rets[\"B/H\"]) / 3\n",
    "    hml = (pf_rets[\"S/H\"] + pf_rets[\"B/H\"]) / 2 \\\n",
    "        - (pf_rets[\"S/L\"] + pf_rets[\"B/L\"]) / 2\n",
    "\n",
    "    smb_monthly.append({\"date\": date, \"SMB_self\": smb})\n",
    "    hml_monthly.append({\"date\": date, \"HML_self\": hml})\n",
    "\n",
    "smb_self = pd.DataFrame(smb_monthly).set_index(\"date\")[\"SMB_self\"]\n",
    "hml_self = pd.DataFrame(hml_monthly).set_index(\"date\")[\"HML_self\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMB and HML are constructed. Now the momentum factor \u2014 this one uses only prices, no fundamentals, which means it should not suffer from the fundamental data limitations that affect SMB and HML. We compute 12-1 month returns (skip the most recent month to avoid microstructure contamination), then go long the top 30% and short the bottom 30%, equal-weighted for simplicity. Momentum is our calibration anchor: if this replicates poorly, the problem is in the return computation or portfolio methodology, not in the fundamental data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Momentum: 12-1 month return, top 30% vs bottom 30%\n",
    "mom_monthly = []\n",
    "\n",
    "for date in monthly_returns.index:\n",
    "    mom_end = date - pd.DateOffset(months=1)\n",
    "    mom_start = date - pd.DateOffset(months=12)\n",
    "    mask = (prices.index >= mom_start) & (prices.index <= mom_end)\n",
    "    if mask.sum() < 20:\n",
    "        continue\n",
    "    mom_prices = prices.loc[mask]\n",
    "    if len(mom_prices) < 2:\n",
    "        continue\n",
    "    momentum = (mom_prices.iloc[-1] / mom_prices.iloc[0]) - 1\n",
    "    momentum = momentum.dropna()\n",
    "\n",
    "    rets = monthly_returns.loc[date].dropna()\n",
    "    common = momentum.index.intersection(rets.index)\n",
    "    if len(common) < 30:\n",
    "        continue\n",
    "\n",
    "    mom_sorted = momentum[common].sort_values()\n",
    "    n = len(mom_sorted)\n",
    "    losers = mom_sorted.iloc[:int(n * 0.3)].index\n",
    "    winners = mom_sorted.iloc[int(n * 0.7):].index\n",
    "\n",
    "    # Equal-weighted for simplicity\n",
    "    mom_ret = rets[winners].mean() - rets[losers].mean()\n",
    "    mom_monthly.append({\"date\": date, \"MOM_self\": mom_ret})\n",
    "\n",
    "mom_self = pd.DataFrame(mom_monthly).set_index(\"date\")[\"MOM_self\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three self-built factors ready. Time for the moment of truth: how well do they match the official Ken French data? We align dates and compute correlation and annualized tracking error for each factor. Tracking error is the standard deviation of the difference between our factor and the official factor, annualized by multiplying by $\\sqrt{12}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align and compare\n",
    "results = {}\n",
    "\n",
    "# SMB\n",
    "common_smb = smb_self.index.intersection(ff3_official.index)\n",
    "smb_corr = smb_self.loc[common_smb].corr(ff3_official.loc[common_smb, \"SMB\"])\n",
    "smb_te = (smb_self.loc[common_smb] - ff3_official.loc[common_smb, \"SMB\"]).std() * np.sqrt(12)\n",
    "results[\"SMB\"] = {\"corr\": smb_corr, \"te\": smb_te, \"n\": len(common_smb)}\n",
    "\n",
    "# HML\n",
    "common_hml = hml_self.index.intersection(ff3_official.index)\n",
    "hml_corr = hml_self.loc[common_hml].corr(ff3_official.loc[common_hml, \"HML\"])\n",
    "hml_te = (hml_self.loc[common_hml] - ff3_official.loc[common_hml, \"HML\"]).std() * np.sqrt(12)\n",
    "results[\"HML\"] = {\"corr\": hml_corr, \"te\": hml_te, \"n\": len(common_hml)}\n",
    "\n",
    "# Momentum\n",
    "common_mom = mom_self.index.intersection(carhart.index)\n",
    "mom_corr = mom_self.loc[common_mom].corr(carhart.loc[common_mom, \"MOM\"])\n",
    "mom_te = (mom_self.loc[common_mom] - carhart.loc[common_mom, \"MOM\"]).std() * np.sqrt(12)\n",
    "results[\"MOM\"] = {\"corr\": mom_corr, \"te\": mom_te, \"n\": len(common_mom)}\n",
    "\n",
    "print(\"\\nFactor Replication Results:\")\n",
    "print(f\"{'Factor':<8} {'Corr':>8} {'TE (ann)':>10} {'N months':>10}\")\n",
    "print(\"-\" * 40)\n",
    "for name, vals in results.items():\n",
    "    print(f\"{name:<8} {vals['corr']:>8.4f} {vals['te']:>10.4f} {vals['n']:>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at those correlations. Momentum replicates well (r around 0.85) \u2014 our code works, the methodology is sound. But SMB barely registers (r around 0.19). HML shows a surprisingly high correlation (r around 0.82), though this comes with a caveat we will address below. The cumulative return comparison makes this even more vivid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "for ax, (name, self_s, off_s, off_col) in zip(axes, [\n",
    "    (\"SMB\", smb_self, ff3_official, \"SMB\"),\n",
    "    (\"HML\", hml_self, ff3_official, \"HML\"),\n",
    "    (\"MOM\", mom_self, carhart, \"MOM\"),\n",
    "]):\n",
    "    common = self_s.index.intersection(off_s.index)\n",
    "    cum_self = (1 + self_s.loc[common]).cumprod()\n",
    "    cum_off = (1 + off_s.loc[common, off_col]).cumprod()\n",
    "    ax.plot(cum_self.index, cum_self, label=\"Self-built\", lw=1.5)\n",
    "    ax.plot(cum_off.index, cum_off, label=\"Ken French\", lw=1.5, ls=\"--\")\n",
    "    r = self_s.loc[common].corr(off_s.loc[common, off_col])\n",
    "    ax.set_title(f\"{name} (r={r:.2f})\")\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Self-Built vs. Official Factor Returns\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is going on with SMB?** Our \"small\" stocks are anything below the S&P 500 median market cap \u2014 roughly $150 billion. In Ken French's universe, true small-caps have market caps of $100 million to $2 billion. We are measuring \"large versus very large,\" not \"small versus large.\" No amount of methodological refinement fixes this. The size effect requires genuine small-caps, which the S&P 500 does not contain.\n",
    "\n",
    "**Why is HML so high?** Two reasons, one genuine and one methodological. The genuine reason: value-growth dispersion exists within large-caps. Banks and energy companies trade at meaningfully different book-to-market ratios than tech companies, and that dispersion generates a value spread even within the S&P 500. The methodological reason: our static sort uses the most recent fundamentals (circa 2021-2025) and applies those fixed classifications backward over the full 11-year window. This introduces look-ahead bias \u2014 a stock labeled \"value\" today may have been \"growth\" in 2014. It also avoids the noise of annual rebalancing. A proper annual-rebalancing implementation limited to the fundamental window (~36-48 months of factor returns) would likely produce lower correlations, closer to the 0.20-0.60 range that the expectations analysis predicted for HML. The qualitative insight \u2014 HML replicates better than SMB \u2014 is robust to methodology; the absolute correlation of 0.82 overstates what a clean implementation would produce.\n",
    "\n",
    "**The calibration anchor logic works.** Momentum (price-only, no fundamentals, no look-ahead bias) replicates well. That confirms our return computation, portfolio formation, and alignment with Ken French dates are all correct. When momentum is right but SMB is wrong, the problem is in the universe, not the code. For reference, Tidy Finance (Scheuch, Voigt & Weiss, 2023), using the full CRSP/Compustat universe with proper annual rebalancing, achieves R-squared of ~0.99 for SMB and ~0.96 for HML. The gap between our free-data results and their institutional-data results is the price you pay for not having a CRSP subscription \u2014 roughly $25,000 per year for academic access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Which Factors Carry a Risk Premium?\n",
    "\n",
    "The lecture demonstrated the Fama-MacBeth procedure as a clean two-step machine: estimate betas, run cross-sectional regressions, average the slopes. But that procedure was applied to three official factors over the full 1963-1990 period where the \"beta anomaly\" \u2014 beta being flat or insignificant \u2014 is a well-documented result.\n",
    "\n",
    "What happens when you run the same test on 2014-2024 data? The textbook says beta should be insignificant and momentum should be strong. Your data is about to disagree.\n",
    "\n",
    "**The question:** Run a Fama-MacBeth regression using five stock characteristics \u2014 market beta, size, book-to-market, operating profitability, and momentum. Which carry statistically significant risk premia? Do the results match the textbook predictions?\n",
    "\n",
    "**Tasks:**\n",
    "1. Estimate rolling 60-month market betas for each stock\n",
    "2. Build a cross-sectional characteristics panel (beta, log market cap, B/M, profitability, momentum)\n",
    "3. Cross-sectionally standardize all characteristics each month\n",
    "4. Run Fama-MacBeth with `linearmodels.FamaMacBeth` using Newey-West standard errors\n",
    "5. Identify which factors are significant at the 5% level and compare against textbook expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student workspace \u2014 Exercise 2\n",
    "# Step 1: Estimate rolling 60-month market betas for each stock\n",
    "#   - Use excess returns (stock return minus RF)\n",
    "#   - OLS of excess stock return on Mkt-RF over rolling 60-month windows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With betas estimated, now assemble the full panel of characteristics and run the cross-sectional test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2-3: Build panel with all 5 characteristics, standardize per month\n",
    "#   - Static fundamentals: log_mcap, bm, profitability from balance sheet/income stmt\n",
    "#   - Time-varying: momentum (12-1 month price return)\n",
    "#   - Z-score standardize cross-sectionally each month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4-5: Run Fama-MacBeth and interpret\n",
    "#   - Use linearmodels.FamaMacBeth with cov_type='kernel' for Newey-West\n",
    "#   - Which gammas are significant? Which aren't?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### \u25b6 Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first Fama-MacBeth step: estimate each stock's market beta from a rolling 60-month window of excess returns on the market factor. This consumes the first 60 months of data (2014-2019), leaving roughly 71 months of cross-sectional observations. That is not a lot \u2014 a quant at AQR would use 40+ years of CRSP data. But it is enough to see whether the textbook results hold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Market betas from rolling 60-month windows\n",
    "common_idx = monthly_returns.index.intersection(ff3_official.index)\n",
    "excess_returns = monthly_returns.loc[common_idx].sub(ff3_official.loc[common_idx, \"RF\"], axis=0)\n",
    "mkt_rf = ff3_official.loc[common_idx, \"Mkt-RF\"]\n",
    "\n",
    "beta_panel = {}\n",
    "window = 60\n",
    "\n",
    "for i in range(window, len(common_idx)):\n",
    "    date = common_idx[i]\n",
    "    mkt_window = mkt_rf.iloc[i - window:i]\n",
    "    betas = {}\n",
    "    for ticker in excess_returns.columns:\n",
    "        y = excess_returns[ticker].iloc[i - window:i].dropna()\n",
    "        if len(y) < 36:\n",
    "            continue\n",
    "        x = mkt_window.loc[y.index]\n",
    "        X = sm.add_constant(x)\n",
    "        model = sm.OLS(y, X).fit()\n",
    "        betas[ticker] = model.params[\"Mkt-RF\"]\n",
    "    beta_panel[date] = betas\n",
    "\n",
    "beta_df = pd.DataFrame(beta_panel).T\n",
    "print(f\"Rolling beta panel: {beta_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the full characteristics panel. For each month and stock, we need five numbers: the rolling beta we just estimated, plus log market cap, book-to-market, operating profitability, and 12-1 month momentum. The fundamental characteristics are static (computed once from the most recent data), while beta and momentum vary by month. This panel feeds directly into the Fama-MacBeth cross-sectional regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static fundamental characteristics\n",
    "fund_chars = {}\n",
    "for ticker in monthly_returns.columns:\n",
    "    chars = {}\n",
    "    # Log market cap\n",
    "    if ticker in mcap_current.index and mcap_current[ticker] > 0:\n",
    "        chars[\"log_mcap\"] = np.log(mcap_current[ticker])\n",
    "\n",
    "    # Book-to-market\n",
    "    if ticker in bs_df.index.get_level_values(\"ticker\"):\n",
    "        tk_bs = bs_df.loc[ticker].sort_index()\n",
    "        eq = tk_bs[\"Stockholders Equity\"].dropna()\n",
    "        if len(eq) > 0 and eq.iloc[-1] > 0:\n",
    "            if ticker in mcap_current.index and mcap_current[ticker] > 0:\n",
    "                chars[\"bm\"] = eq.iloc[-1] / mcap_current[ticker]\n",
    "\n",
    "    # Profitability (operating income / book equity)\n",
    "    if ticker in inc_df.index.get_level_values(\"ticker\"):\n",
    "        tk_inc = inc_df.loc[ticker].sort_index()\n",
    "        oi = tk_inc.get(\"Operating Income\", pd.Series(dtype=float)).dropna()\n",
    "        if len(oi) == 0:\n",
    "            oi = tk_inc.get(\"Net Income\", pd.Series(dtype=float)).dropna()\n",
    "        if len(oi) > 0 and \"bm\" in chars:\n",
    "            # Use equity from above\n",
    "            eq_val = bs_df.loc[ticker].sort_index()[\"Stockholders Equity\"].dropna()\n",
    "            if len(eq_val) > 0 and eq_val.iloc[-1] > 0:\n",
    "                chars[\"profitability\"] = oi.iloc[-1] / eq_val.iloc[-1]\n",
    "\n",
    "    fund_chars[ticker] = chars\n",
    "\n",
    "fund_df = pd.DataFrame(fund_chars).T\n",
    "print(f\"Fundamental chars: {fund_df.shape}\")\n",
    "print(f\"  log_mcap non-null: {fund_df['log_mcap'].notna().sum()}\")\n",
    "print(f\"  bm non-null: {fund_df['bm'].notna().sum()}\")\n",
    "print(f\"  profitability non-null: {fund_df['profitability'].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we assemble the panel \u2014 one row per stock-month, with excess returns and all five characteristics. Momentum is computed fresh for each month (12-1 month price return), while fundamentals are static. This gives us around 12,000-13,000 stock-month observations spanning roughly 71 months after the rolling-beta estimation period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build panel: for each month, characteristics + returns\n",
    "panel_records = []\n",
    "\n",
    "for date in beta_df.index:\n",
    "    # Momentum: 12-1 month return\n",
    "    mom_end = date - pd.DateOffset(months=1)\n",
    "    mom_start = date - pd.DateOffset(months=12)\n",
    "    mask = (prices.index >= mom_start) & (prices.index <= mom_end)\n",
    "    if mask.sum() < 20:\n",
    "        continue\n",
    "    mom_prices = prices.loc[mask]\n",
    "    if len(mom_prices) < 2:\n",
    "        continue\n",
    "    momentum = (mom_prices.iloc[-1] / mom_prices.iloc[0]) - 1\n",
    "\n",
    "    for ticker in monthly_returns.columns:\n",
    "        if date not in excess_returns.index:\n",
    "            continue\n",
    "        ret = excess_returns.loc[date, ticker]\n",
    "        if pd.isna(ret):\n",
    "            continue\n",
    "\n",
    "        beta_val = beta_df.loc[date].get(ticker, np.nan)\n",
    "        if pd.isna(beta_val):\n",
    "            continue\n",
    "\n",
    "        row = {\n",
    "            \"date\": date,\n",
    "            \"ticker\": ticker,\n",
    "            \"excess_ret\": ret,\n",
    "            \"beta\": beta_val,\n",
    "            \"momentum\": momentum.get(ticker, np.nan),\n",
    "        }\n",
    "\n",
    "        if ticker in fund_df.index:\n",
    "            for col in fund_df.columns:\n",
    "                row[col] = fund_df.loc[ticker, col]\n",
    "\n",
    "        panel_records.append(row)\n",
    "\n",
    "panel = pd.DataFrame(panel_records)\n",
    "panel = panel.set_index([\"ticker\", \"date\"]).sort_index()\n",
    "print(f\"\\nPanel shape: {panel.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running Fama-MacBeth, we standardize all characteristics cross-sectionally each month. This is critical: without standardization, the gammas for log market cap (values around 25) and book-to-market (values around 0.2) would be on completely different scales, making comparison meaningless. Z-scoring within each month ensures every gamma is in units of \"monthly return per standard deviation of the characteristic.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize characteristics cross-sectionally each month\n",
    "char_cols = [\"beta\", \"log_mcap\", \"bm\", \"profitability\", \"momentum\"]\n",
    "\n",
    "def standardize_month(group):\n",
    "    \"\"\"Z-score standardize within each month.\"\"\"\n",
    "    for col in char_cols:\n",
    "        if col in group.columns:\n",
    "            vals = group[col]\n",
    "            mean, std = vals.mean(), vals.std()\n",
    "            if std > 0:\n",
    "                group[col] = (vals - mean) / std\n",
    "    return group\n",
    "\n",
    "panel_std = panel.groupby(level=\"date\", group_keys=False).apply(standardize_month)\n",
    "panel_clean = panel_std.dropna(subset=char_cols + [\"excess_ret\"])\n",
    "print(f\"Clean panel: {panel_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the main event. We feed the standardized panel into `linearmodels.FamaMacBeth` with Newey-West standard errors (`cov_type='kernel'`). Newey-West corrects for autocorrelation in the time series of cross-sectional slopes \u2014 which can either shrink or inflate the t-statistics relative to naive OLS, depending on the autocorrelation structure. The textbook prediction: beta should be insignificant (the \"beta anomaly\"), momentum should be significant. Let us see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linearmodels import FamaMacBeth as LMFamaMacBeth\n",
    "\n",
    "dep = panel_clean[[\"excess_ret\"]]\n",
    "indep = panel_clean[char_cols]\n",
    "\n",
    "fm = LMFamaMacBeth(dep, indep).fit(cov_type=\"kernel\")\n",
    "\n",
    "print(\"\\nFama-MacBeth Risk Premia:\")\n",
    "print(f\"{'Factor':<16} {'Gamma':>10} {'NW t-stat':>10} {'p-value':>10}\")\n",
    "print(\"-\" * 50)\n",
    "for factor in char_cols:\n",
    "    g = fm.params[factor]\n",
    "    t = fm.tstats[factor]\n",
    "    p = fm.pvalues[factor]\n",
    "    sig = \"*\" if p < 0.05 else \"\"\n",
    "    print(f\"{factor:<16} {g:>10.6f} {t:>10.2f} {p:>10.4f} {sig}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read those results carefully. The textbook says beta should be insignificant \u2014 the \"beta anomaly\" documented by Fama and French (1992) using 1963-1990 CRSP data. But our 2014-2024 data gives us a significant beta premium (t around 2.46, p around 0.014). Meanwhile, momentum \u2014 which the literature considers one of the most robust cross-sectional predictors \u2014 is insignificant (t around 0.81).\n",
    "\n",
    "This is not a bug. It is the central lesson: factor premia are sample-dependent. The 2014-2024 period was dominated by an AI/tech boom where high-beta stocks (NVDA with 43% annualized alpha, TSLA, AMD) earned dramatically more. That regime rewarded high-beta exposure. Meanwhile, the COVID-era market disruption (crash-and-recovery in 2020, rate shocks in 2022) broke the momentum signal \u2014 stocks that had been winners suddenly became losers and vice versa, erratically.\n",
    "\n",
    "The visualization makes the pattern even clearer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "factors = char_cols\n",
    "gammas = [fm.params[f] for f in factors]\n",
    "tstats = [fm.tstats[f] for f in factors]\n",
    "colors = [\"#2196F3\" if abs(t) >= 2.0 else \"#BDBDBD\" for t in tstats]\n",
    "\n",
    "bars = ax.bar(range(len(factors)), tstats, color=colors, edgecolor=\"k\",\n",
    "              linewidth=0.5)\n",
    "ax.axhline(2.0, color=\"red\", ls=\"--\", lw=1, label=\"t = 2.0\")\n",
    "ax.axhline(-2.0, color=\"red\", ls=\"--\", lw=1)\n",
    "ax.set_xticks(range(len(factors)))\n",
    "ax.set_xticklabels(factors, rotation=30, ha=\"right\")\n",
    "ax.set(title=\"Fama-MacBeth t-Statistics \\u2014 Which Factors Are Priced?\",\n",
    "       ylabel=\"Newey-West t-statistic\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only one bar clears the significance threshold, and it is the one factor that textbooks say should not be significant. Size is marginally insignificant (t around 1.70), book-to-market is negative and insignificant, profitability is essentially zero. If you ran this same test on Fama and French's original 1963-1990 sample, you would get the opposite result: beta insignificant, size and value significant.\n",
    "\n",
    "\"Which factors are priced?\" has no stable answer across time. The ground truth keeps shifting \u2014 which is precisely why asset pricing remains an active field 60 years after Sharpe's CAPM. For the ML practitioner, the takeaway is practical: never trust a factor premium estimated from a single sample period, no matter how long. Always test across regimes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Decompose Your Portfolio's Risk\n",
    "\n",
    "The lecture introduced Barra-style risk models and the idea of decomposing portfolio returns into factor and idiosyncratic components. The conventional wisdom \u2014 and what many textbooks will tell you \u2014 is straightforward: concentrated portfolios should have higher factor risk because their stocks share common exposures, while diversified portfolios should have more idiosyncratic risk.\n",
    "\n",
    "This is a good hypothesis. Let us test it and see if the data cooperates.\n",
    "\n",
    "**The question:** Take two portfolios \u2014 one diversified across 10 sectors, one concentrated in tech \u2014 and decompose each using a Fama-French 5-factor regression. Does concentration always mean more factor risk?\n",
    "\n",
    "**Tasks:**\n",
    "1. Construct a diversified 20-stock portfolio (2 stocks from each of 10 sectors)\n",
    "2. Construct a concentrated 20-stock tech-only portfolio\n",
    "3. Run FF5 time-series regressions for each portfolio's excess returns\n",
    "4. Compute the factor risk share (% of variance explained by factors) and specific risk share\n",
    "5. Compare factor loadings between the two portfolios and explain the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student workspace \u2014 Exercise 3\n",
    "# Step 1-2: Define the two portfolios\n",
    "#   - Diversified: 2 stocks per sector, 10 sectors, 20 total\n",
    "#   - Concentrated: 20 tech stocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3-5: Run FF5 regressions and compare risk decomposition\n",
    "#   - Equal-weighted portfolio returns minus RF\n",
    "#   - OLS on Mkt-RF, SMB, HML, RMW, CMA\n",
    "#   - Factor risk = var(fitted), Specific risk = var(residual)\n",
    "#   - Compare factor shares and loadings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### \u25b6 Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by building the two portfolios. The diversified portfolio picks 2 stocks from each sector available in our universe, capping at 20 total. The concentrated portfolio takes up to 20 technology stocks. The sector assignments come from yfinance's `.info['sector']` field \u2014 not perfect (some stocks may have reclassified over the 11-year window), but a reasonable approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diversified portfolio: 2 stocks from each sector\n",
    "diversified_tickers = []\n",
    "for sector_name in sorted(sectors.unique()):\n",
    "    tickers_in_sector = sectors[sectors == sector_name].index.tolist()\n",
    "    available = [t for t in tickers_in_sector if t in monthly_returns.columns]\n",
    "    diversified_tickers.extend(available[:2])\n",
    "diversified_tickers = diversified_tickers[:20]\n",
    "\n",
    "# Concentrated portfolio: all technology stocks\n",
    "tech_tickers = sectors[sectors == \"Technology\"].index.tolist()\n",
    "tech_available = [t for t in tech_tickers if t in monthly_returns.columns]\n",
    "concentrated_tickers = tech_available[:20]\n",
    "\n",
    "print(f\"Diversified portfolio: {len(diversified_tickers)} stocks \"\n",
    "      f\"across {len(set(sectors[t] for t in diversified_tickers if t in sectors.index))} sectors\")\n",
    "print(f\"Concentrated portfolio: {len(concentrated_tickers)} tech stocks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run FF5 time-series regressions for each portfolio. We compute equal-weighted portfolio returns, subtract the risk-free rate, and regress on the five Fama-French factors. The fitted values represent the systematic (factor) component of returns; the residuals represent the idiosyncratic component. The ratio of factor variance to total variance is the factor risk share \u2014 the number that tells risk managers how much of the portfolio's risk comes from broad factor exposures versus stock-specific bets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equal-weighted portfolio returns\n",
    "common_idx_ff5 = monthly_returns.index.intersection(ff5.index)\n",
    "rf = ff5.loc[common_idx_ff5, \"RF\"]\n",
    "factors_ff5 = ff5.loc[common_idx_ff5, [\"Mkt-RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\"]]\n",
    "\n",
    "def decompose_portfolio(tickers, label):\n",
    "    \"\"\"Run FF5 regression and compute risk decomposition.\"\"\"\n",
    "    port_ret = monthly_returns[tickers].loc[common_idx_ff5].mean(axis=1)\n",
    "    port_excess = port_ret - rf\n",
    "\n",
    "    X = sm.add_constant(factors_ff5)\n",
    "    model = sm.OLS(port_excess, X).fit()\n",
    "\n",
    "    total_var = port_excess.var()\n",
    "    factor_var = model.fittedvalues.var()\n",
    "    specific_var = model.resid.var()\n",
    "    factor_share = factor_var / total_var\n",
    "\n",
    "    print(f\"\\n{label}:\")\n",
    "    print(f\"  R\\u00b2: {model.rsquared:.4f}\")\n",
    "    print(f\"  Factor risk share: {factor_share:.2%}\")\n",
    "    print(f\"  Specific risk share: {1 - factor_share:.2%}\")\n",
    "    print(f\"  Factor loadings:\")\n",
    "    for f_name in factors_ff5.columns:\n",
    "        print(f\"    {f_name}: {model.params[f_name]:.4f} \"\n",
    "              f\"(t={model.tvalues[f_name]:.2f})\")\n",
    "\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"r2\": model.rsquared,\n",
    "        \"factor_share\": factor_share,\n",
    "        \"specific_share\": 1 - factor_share,\n",
    "        \"total_var\": total_var,\n",
    "        \"loadings\": {f: model.params[f] for f in factors_ff5.columns},\n",
    "    }\n",
    "\n",
    "div_result = decompose_portfolio(diversified_tickers, \"Diversified (20 stocks)\")\n",
    "conc_result = decompose_portfolio(concentrated_tickers, \"Concentrated (Tech)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at those numbers. The diversified portfolio has a factor risk share around 89% \u2014 nearly all of its risk comes from common factor exposures. The concentrated tech portfolio? About 81%. The direction is reversed from what most textbooks would predict: the diversified portfolio has *higher* factor risk share than the concentrated one.\n",
    "\n",
    "This is counterintuitive but genuine. The diversified portfolio \u2014 2 stocks from each of 10 sectors \u2014 tracks the broad market cleanly. Its Mkt-RF loading is close to 1.0, and there is very little idiosyncratic risk left because the stock-specific noise averages out across sectors. The tech portfolio, by contrast, is full of stocks that are individually volatile and heterogeneous: NVDA, AAPL, and MSFT are all \"tech\" but their return drivers differ substantially. That heterogeneity shows up as idiosyncratic risk that the five Fama-French factors cannot explain.\n",
    "\n",
    "The visualization makes this concrete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Pie charts\n",
    "for ax, result in zip(axes[:2], [div_result, conc_result]):\n",
    "    sizes = [result[\"factor_share\"], result[\"specific_share\"]]\n",
    "    labels = [\"Factor Risk\", \"Specific Risk\"]\n",
    "    colors = [\"#2196F3\", \"#FF9800\"]\n",
    "    ax.pie(sizes, labels=labels, colors=colors, autopct=\"%1.1f%%\",\n",
    "           startangle=90, textprops={\"fontsize\": 10})\n",
    "    ax.set_title(f\"{result['label']}\\n(R\\u00b2={result['r2']:.3f})\")\n",
    "\n",
    "# Factor loadings comparison\n",
    "ax = axes[2]\n",
    "factor_names = list(factors_ff5.columns)\n",
    "x = np.arange(len(factor_names))\n",
    "width = 0.35\n",
    "div_loads = [div_result[\"loadings\"][f] for f in factor_names]\n",
    "conc_loads = [conc_result[\"loadings\"][f] for f in factor_names]\n",
    "ax.bar(x - width / 2, div_loads, width, label=\"Diversified\", color=\"#2196F3\")\n",
    "ax.bar(x + width / 2, conc_loads, width, label=\"Concentrated\", color=\"#FF9800\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(factor_names, rotation=30, ha=\"right\")\n",
    "ax.set(title=\"Factor Loadings Comparison\", ylabel=\"Loading\")\n",
    "ax.legend()\n",
    "ax.axhline(0, color=\"gray\", lw=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loadings comparison in the right panel tells the story. Both portfolios load heavily on Mkt-RF (the market factor dominates everything in equities). But notice CMA: the diversified portfolio has a positive loading (tilted toward conservative firms), while the tech portfolio has a negative loading (aggressive investment). This makes economic sense \u2014 tech firms invest heavily, so they load negatively on the Conservative Minus Aggressive factor.\n",
    "\n",
    "The key takeaway: concentration does not always mean more factor risk. It depends on whether the concentrated sector's stocks co-move with standard factors or with each other in idiosyncratic ways. Tech stocks are individually volatile, drive the market in distinctive ways, and have return components that the five Fama-French factors do not capture. A risk manager at a multi-strategy fund would not be surprised by this result \u2014 they know that \"sector concentration\" and \"factor concentration\" are different things. The pie chart does not lie, but it does demand interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: The Factor Zoo Safari\n",
    "\n",
    "Harvey, Liu, and Zhu (2016) analyzed 296 published factors and estimated that 53% were false discoveries. The standard significance threshold of t > 2.0 \u2014 the one you learned in your first statistics course \u2014 does not protect you when you have tested hundreds of hypotheses. With 296 tests, you expect about 15 to clear the t = 2.0 bar by pure chance.\n",
    "\n",
    "We are going to reproduce this problem in miniature. You will test 10 stock characteristics \u2014 6 real ones (momentum, value, profitability, asset growth, reversal, earnings yield) and 4 that are pure random noise. Without correction, some of the noise will look \"significant.\" The question is: how many real signals survive once you correct for the testing you have done?\n",
    "\n",
    "**The question:** Test 10 characteristics via univariate Fama-MacBeth regressions. Apply Bonferroni, Benjamini-Hochberg, and the Harvey-Liu-Zhu threshold (t > 3.0). How many \"significant\" factors survive each correction \u2014 and does the noise factor slip through?\n",
    "\n",
    "**Tasks:**\n",
    "1. Compute 6 real characteristics (momentum, B/M, profitability, asset growth, reversal, earnings yield)\n",
    "2. Generate 4 noise characteristics from random normal draws\n",
    "3. Run univariate Fama-MacBeth for each characteristic, cross-sectionally standardized\n",
    "4. Apply Bonferroni correction (p < 0.05/10), Benjamini-Hochberg at FDR = 5%, and the HLZ threshold (t > 3.0)\n",
    "5. Count how many factors survive each threshold, and whether noise factors slip through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student workspace \u2014 Exercise 4\n",
    "# Step 1-2: Build 10 characteristics (6 real + 4 noise)\n",
    "#   - Real: momentum, book_to_market, profitability, asset_growth, reversal, earnings_yield\n",
    "#   - Noise: np.random.normal() for each stock-month (seed=42 for reproducibility)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the characteristics computed and the panel built, now test them and apply corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Univariate Fama-MacBeth for each characteristic\n",
    "#   - Cross-sectionally z-score standardize each month\n",
    "#   - For each characteristic: OLS of excess_ret on the single char, each month\n",
    "#   - Average the slopes, compute t-stat = mean(gamma) / (std(gamma) / sqrt(T))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4-5: Apply multiple testing corrections\n",
    "#   - Bonferroni: p_adj = p_raw * n_tests, reject if p_adj < 0.05\n",
    "#   - BH: rank p-values, reject if p_i <= (rank_i / n_tests) * FDR\n",
    "#   - HLZ: reject if |t| > 3.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### \u25b6 Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we compute the six real characteristics from fundamentals and prices. Four of them (book-to-market, profitability, asset growth, earnings yield) are static \u2014 computed once from the most recent fundamental data. Two (momentum and reversal) are computed fresh each month from the price series. Then we add four noise characteristics: pure random normal draws with no relationship to returns. If our significance tests are working correctly, the noise should not survive correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build 10 characteristics: 6 real + 4 noise\n",
    "np.random.seed(42)\n",
    "\n",
    "fund_chars_zoo = {}\n",
    "for ticker in monthly_returns.columns:\n",
    "    chars = {}\n",
    "\n",
    "    # 1. Book-to-market\n",
    "    if ticker in bs_df.index.get_level_values(\"ticker\"):\n",
    "        tk_bs = bs_df.loc[ticker].sort_index()\n",
    "        eq = tk_bs[\"Stockholders Equity\"].dropna()\n",
    "        if len(eq) > 0 and eq.iloc[-1] > 0:\n",
    "            if ticker in mcap_current.index and mcap_current[ticker] > 0:\n",
    "                chars[\"book_to_market\"] = eq.iloc[-1] / mcap_current[ticker]\n",
    "\n",
    "    # 2. Operating profitability\n",
    "    if ticker in inc_df.index.get_level_values(\"ticker\"):\n",
    "        tk_inc = inc_df.loc[ticker].sort_index()\n",
    "        oi = tk_inc.get(\"Operating Income\", pd.Series(dtype=float)).dropna()\n",
    "        if len(oi) == 0:\n",
    "            oi = tk_inc.get(\"Net Income\", pd.Series(dtype=float)).dropna()\n",
    "        if len(oi) > 0 and ticker in bs_df.index.get_level_values(\"ticker\"):\n",
    "            tk_bs2 = bs_df.loc[ticker].sort_index()\n",
    "            eq2 = tk_bs2[\"Stockholders Equity\"].dropna()\n",
    "            if len(eq2) > 0 and eq2.iloc[-1] > 0:\n",
    "                chars[\"profitability\"] = oi.iloc[-1] / eq2.iloc[-1]\n",
    "\n",
    "    # 3. Asset growth\n",
    "    if ticker in bs_df.index.get_level_values(\"ticker\"):\n",
    "        tk_bs3 = bs_df.loc[ticker].sort_index()\n",
    "        assets = tk_bs3[\"Total Assets\"].dropna()\n",
    "        if len(assets) > 1 and assets.iloc[-2] > 0:\n",
    "            chars[\"asset_growth\"] = (assets.iloc[-1] / assets.iloc[-2]) - 1\n",
    "\n",
    "    # 4. Earnings yield\n",
    "    if ticker in inc_df.index.get_level_values(\"ticker\"):\n",
    "        ni = inc_df.loc[ticker].sort_index().get(\n",
    "            \"Net Income\", pd.Series(dtype=float)\n",
    "        ).dropna()\n",
    "        if len(ni) > 0 and ticker in mcap_current.index and mcap_current[ticker] > 0:\n",
    "            chars[\"earnings_yield\"] = ni.iloc[-1] / mcap_current[ticker]\n",
    "\n",
    "    fund_chars_zoo[ticker] = chars\n",
    "\n",
    "fund_df_zoo = pd.DataFrame(fund_chars_zoo).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the full panel, adding time-varying characteristics (momentum, reversal) and the four noise columns. Each noise column gets a fresh independent random normal draw for every stock-month observation. With around 130 months and 180+ stocks, the panel has roughly 23,000 observations. Some of these noise draws will, by chance, correlate with returns in a given month \u2014 that is exactly the problem we are trying to expose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_records_zoo = []\n",
    "\n",
    "for date in common_idx:\n",
    "    # 5. Momentum (12-1 month)\n",
    "    mom_end = date - pd.DateOffset(months=1)\n",
    "    mom_start = date - pd.DateOffset(months=12)\n",
    "    mask = (prices.index >= mom_start) & (prices.index <= mom_end)\n",
    "    if mask.sum() < 20:\n",
    "        continue\n",
    "    mom_prices = prices.loc[mask]\n",
    "    if len(mom_prices) < 2:\n",
    "        continue\n",
    "    momentum = (mom_prices.iloc[-1] / mom_prices.iloc[0]) - 1\n",
    "\n",
    "    # 6. Short-term reversal (1-month return)\n",
    "    reversal = monthly_returns.loc[date] if date in monthly_returns.index else None\n",
    "    if reversal is None:\n",
    "        continue\n",
    "\n",
    "    for ticker in monthly_returns.columns:\n",
    "        ret = excess_returns.loc[date, ticker] if date in excess_returns.index else np.nan\n",
    "        if pd.isna(ret):\n",
    "            continue\n",
    "\n",
    "        row = {\n",
    "            \"date\": date,\n",
    "            \"ticker\": ticker,\n",
    "            \"excess_ret\": ret,\n",
    "            \"momentum\": momentum.get(ticker, np.nan),\n",
    "            \"reversal\": reversal.get(ticker, np.nan),\n",
    "        }\n",
    "\n",
    "        # Add fundamental chars\n",
    "        if ticker in fund_df_zoo.index:\n",
    "            for col in fund_df_zoo.columns:\n",
    "                row[col] = fund_df_zoo.loc[ticker, col]\n",
    "\n",
    "        # 7-10. Noise characteristics\n",
    "        row[\"noise_1\"] = np.random.normal()\n",
    "        row[\"noise_2\"] = np.random.normal()\n",
    "        row[\"noise_3\"] = np.random.normal()\n",
    "        row[\"noise_4\"] = np.random.normal()\n",
    "\n",
    "        panel_records_zoo.append(row)\n",
    "\n",
    "panel_zoo = pd.DataFrame(panel_records_zoo)\n",
    "panel_zoo = panel_zoo.set_index([\"ticker\", \"date\"]).sort_index()\n",
    "\n",
    "all_chars = [\"momentum\", \"book_to_market\", \"profitability\",\n",
    "             \"asset_growth\", \"reversal\", \"earnings_yield\",\n",
    "             \"noise_1\", \"noise_2\", \"noise_3\", \"noise_4\"]\n",
    "print(f\"Panel shape: {panel_zoo.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run univariate Fama-MacBeth for each of the 10 characteristics. \"Univariate\" means we test each characteristic in isolation \u2014 one cross-sectional regression per characteristic per month, then average the slopes. This is the simplest form of the test, and it is the approach most vulnerable to false discoveries because confounding factors are not controlled for. We standardize cross-sectionally before testing so the gammas are comparable across characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize cross-sectionally\n",
    "def standardize_month_zoo(group):\n",
    "    \"\"\"Z-score standardize within each month.\"\"\"\n",
    "    for col in all_chars:\n",
    "        if col in group.columns:\n",
    "            vals = group[col]\n",
    "            mean, std = vals.mean(), vals.std()\n",
    "            if std > 0:\n",
    "                group[col] = (vals - mean) / std\n",
    "    return group\n",
    "\n",
    "panel_std_zoo = panel_zoo.groupby(level=\"date\", group_keys=False).apply(standardize_month_zoo)\n",
    "\n",
    "# Univariate Fama-MacBeth for each characteristic\n",
    "fm_results = {}\n",
    "\n",
    "for char in all_chars:\n",
    "    sub = panel_std_zoo[[\"excess_ret\", char]].dropna()\n",
    "    if len(sub) < 1000:\n",
    "        continue\n",
    "\n",
    "    # Manual Fama-MacBeth\n",
    "    gammas = []\n",
    "    dates = sub.index.get_level_values(\"date\").unique()\n",
    "    for date in dates:\n",
    "        month_data = sub.loc[sub.index.get_level_values(\"date\") == date]\n",
    "        if len(month_data) < 30:\n",
    "            continue\n",
    "        y = month_data[\"excess_ret\"].values\n",
    "        x = month_data[char].values\n",
    "        X = sm.add_constant(x)\n",
    "        model = sm.OLS(y, X).fit()\n",
    "        gammas.append(model.params[1])\n",
    "\n",
    "    gammas = np.array(gammas)\n",
    "    mean_gamma = gammas.mean()\n",
    "    se = gammas.std() / np.sqrt(len(gammas))\n",
    "    t_stat = mean_gamma / se if se > 0 else 0\n",
    "    p_value = 2 * (1 - scipy_stats.t.cdf(abs(t_stat), df=len(gammas) - 1))\n",
    "\n",
    "    fm_results[char] = {\n",
    "        \"gamma\": mean_gamma,\n",
    "        \"t_stat\": t_stat,\n",
    "        \"p_value\": p_value,\n",
    "        \"n_months\": len(gammas),\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(fm_results).T\n",
    "results_df = results_df.sort_values(\"p_value\")\n",
    "print(\"\\nUnivariate Fama-MacBeth Results (sorted by p-value):\")\n",
    "print(results_df.round(4).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the raw results before any correction. Several characteristics clear the traditional p < 0.05 threshold \u2014 but so does one of the noise factors (noise_1, with a t-statistic around 2.69). That is a textbook false discovery: pure random noise that happened to correlate with returns just enough to look \"significant\" by conventional standards.\n",
    "\n",
    "Also note: reversal shows an extremely high t-statistic (around 36). This is not a real discovery either \u2014 reversal is computed from last month's return, and the dependent variable is this month's return. When a stock drops sharply, it tends to bounce back (mean reversion), creating a near-mechanical relationship between the \"signal\" and the outcome. In production, a quant researcher would immediately flag this and investigate whether the signal is genuinely predictive or mechanically induced.\n",
    "\n",
    "Now let us apply the corrections and watch the zoo shrink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tests = len(results_df)\n",
    "\n",
    "# Bonferroni correction\n",
    "results_df[\"bonferroni_p\"] = (results_df[\"p_value\"] * n_tests).clip(upper=1.0)\n",
    "results_df[\"bonf_sig\"] = results_df[\"bonferroni_p\"] < 0.05\n",
    "\n",
    "# Benjamini-Hochberg\n",
    "results_sorted = results_df.sort_values(\"p_value\")\n",
    "ranks = np.arange(1, n_tests + 1)\n",
    "bh_threshold = ranks / n_tests * 0.05\n",
    "results_sorted[\"bh_threshold\"] = bh_threshold\n",
    "results_sorted[\"bh_sig\"] = results_sorted[\"p_value\"] <= bh_threshold\n",
    "\n",
    "# Harvey-Liu-Zhu threshold (t > 3.0)\n",
    "results_df[\"hlz_sig\"] = results_df[\"t_stat\"].abs() > 3.0\n",
    "\n",
    "naive_sig = (results_df[\"p_value\"] < 0.05).sum()\n",
    "bonf_sig = results_df[\"bonf_sig\"].sum()\n",
    "bh_sig = results_sorted[\"bh_sig\"].sum()\n",
    "hlz_sig = results_df[\"hlz_sig\"].sum()\n",
    "\n",
    "# Check noise factor survival\n",
    "noise_chars = [\"noise_1\", \"noise_2\", \"noise_3\", \"noise_4\"]\n",
    "noise_naive = sum(1 for c in noise_chars\n",
    "                  if c in results_df.index and results_df.loc[c, \"p_value\"] < 0.05)\n",
    "noise_bonf = sum(1 for c in noise_chars\n",
    "                 if c in results_df.index and results_df.loc[c, \"bonf_sig\"])\n",
    "\n",
    "print(f\"\\nMultiple Testing Correction Summary:\")\n",
    "print(f\"  Naive (p < 0.05):         {naive_sig} / {n_tests} significant\")\n",
    "print(f\"  Bonferroni (p < 0.005):   {bonf_sig} / {n_tests} significant\")\n",
    "print(f\"  Benjamini-Hochberg (FDR): {bh_sig} / {n_tests} significant\")\n",
    "print(f\"  HLZ (|t| > 3.0):         {hlz_sig} / {n_tests} significant\")\n",
    "print(f\"  Noise factors naive sig:  {noise_naive} / {len(noise_chars)}\")\n",
    "print(f\"  Noise factors Bonf sig:   {noise_bonf} / {len(noise_chars)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers tell the story. Without correction, 4 of 10 characteristics look significant \u2014 including one noise factor. After Bonferroni correction, only 2 survive, and the noise factor is correctly filtered out. That is a 50% drop from naive to corrected.\n",
    "\n",
    "A subtlety worth noting: Benjamini-Hochberg at FDR = 5% retains all 4 naive significant factors, including the noise factor. BH controls the expected *proportion* of false discoveries, not individual false positives. With only 10 tests, a noise factor at p around 0.008 can survive BH even though it is pure noise. This illustrates why BH alone is insufficient for the factor zoo \u2014 when the number of tests is in the hundreds, a 5% false discovery *rate* means dozens of fake factors get through. Bonferroni and the HLZ threshold (t > 3.0) are more conservative and correctly reject noise here.\n",
    "\n",
    "The visualization puts all four thresholds side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# t-statistics bar chart\n",
    "ax = axes[0]\n",
    "chars_sorted = results_df.sort_values(\"t_stat\", ascending=False)\n",
    "colors = [\"#2196F3\" if c not in noise_chars else \"#FF5252\"\n",
    "          for c in chars_sorted.index]\n",
    "ax.barh(range(len(chars_sorted)), chars_sorted[\"t_stat\"], color=colors,\n",
    "        edgecolor=\"k\", linewidth=0.5)\n",
    "ax.axvline(2.0, color=\"green\", ls=\"--\", lw=1.5, label=\"t = 2.0\")\n",
    "ax.axvline(-2.0, color=\"green\", ls=\"--\", lw=1.5)\n",
    "ax.axvline(3.0, color=\"red\", ls=\"--\", lw=1.5, label=\"t = 3.0 (HLZ)\")\n",
    "ax.axvline(-3.0, color=\"red\", ls=\"--\", lw=1.5)\n",
    "ax.set_yticks(range(len(chars_sorted)))\n",
    "ax.set_yticklabels(chars_sorted.index, fontsize=9)\n",
    "ax.set(title=\"Fama-MacBeth t-Statistics by Characteristic\",\n",
    "       xlabel=\"t-statistic\")\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# Survival count comparison\n",
    "ax = axes[1]\n",
    "methods = [\"Naive\\n(t>2.0)\", \"Bonferroni\", \"BH\\n(FDR=5%)\", \"HLZ\\n(t>3.0)\"]\n",
    "counts = [naive_sig, bonf_sig, bh_sig, hlz_sig]\n",
    "bars = ax.bar(methods, counts, color=[\"#66BB6A\", \"#42A5F5\",\n",
    "              \"#FFA726\", \"#EF5350\"], edgecolor=\"k\", linewidth=0.5)\n",
    "ax.set(title=\"How Many Factors Survive Correction?\",\n",
    "       ylabel=\"# Significant Factors\")\n",
    "ax.axhline(n_tests, color=\"gray\", ls=\":\", alpha=0.5,\n",
    "           label=f\"Total tested: {n_tests}\")\n",
    "for bar, count in zip(bars, counts):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.2,\n",
    "            str(count), ha=\"center\", fontsize=12, fontweight=\"bold\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left panel shows the raw t-statistics color-coded by real (blue) versus noise (red). The noise factor that cleared the naive threshold sticks out \u2014 it sits just above the green t = 2.0 line but falls well below the red t = 3.0 HLZ threshold. The right panel shows the survival count across correction methods: the drop from naive to Bonferroni is a 50% reduction.\n",
    "\n",
    "Scale this up to the actual factor zoo: Harvey, Liu, and Zhu (2016) found that 53% of 296 published factors were likely false discoveries. With 296 tests at the conventional 5% threshold, you expect about 15 false positives by chance alone. At t > 3.0, most of those wash out \u2014 which is why they recommended raising the bar from 2.0 to 3.0 for any new factor claim. And even surviving the threshold is necessary but not sufficient. Our reversal factor survived Bonferroni with t = 36.58, but that extreme statistic is a red flag, not a badge of honor \u2014 it signals a near-mechanical overlap between the signal and the outcome that would need to be investigated before declaring it a real discovery.\n",
    "\n",
    "The lesson for ML practitioners building cross-sectional models: every feature you add is a hypothesis you are testing. If you compute 100 features and select the top 10 by p-value, you have not found 10 signals \u2014 you have found the 10 that got lucky in your sample. Multiple testing correction is not a statistical nicety. It is the difference between a real strategy and an expensive way to learn about overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Four exercises, four concrete discoveries:\n",
    "\n",
    "- **Factor replication is universe-dependent, not just code-dependent.** SMB replication fails (r around 0.19) not because of a bug but because the S&P 500 contains no real small-caps. HML replicates better (r around 0.82), but that number is flattered by a static-sort methodology with look-ahead bias. Momentum (r around 0.85) serves as the calibration anchor \u2014 when the price-only factor works but the fundamental-based factor does not, the problem is in the data, not the code.\n",
    "\n",
    "- **Factor premia are sample-dependent.** Beta is significantly priced in 2014-2024 (the textbook says it should not be), while momentum is insignificant (the textbook says it should be). The same test on 1963-1990 data gives the opposite answer. There is no stable set of \"priced factors\" \u2014 the answer depends on the decade.\n",
    "\n",
    "- **Concentration does not always mean more factor risk.** A diversified 20-stock cross-sector portfolio showed higher factor risk share (89%) than a concentrated tech portfolio (81%). Tech stocks have high idiosyncratic volatility that standard factors cannot capture. Always decompose before assuming.\n",
    "\n",
    "- **Multiple testing correction is not optional.** A noise factor cleared p < 0.01 before correction \u2014 a textbook false discovery. Bonferroni cut the number of significant factors by 50%. Scale this to 296 published factors, and you see why Harvey, Liu, and Zhu called for t > 3.0.\n",
    "\n",
    "In the homework, you will build a complete `FactorBuilder` class that constructs all six factors at scale, engineer a cross-sectional feature matrix ready for ML consumption, and run the factor model horse race to quantify how much return variation remains unexplained \u2014 the 85% that Week 4's gradient boosting and neural networks will attempt to capture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}